= High Availability and {productname}
include::entities.adoc[]


== Overview

In this guide you will use KVM to setup {productname} on two nodes with {sleha}.
You will also add a third server to provide iSCSI shared storage for the nodes.

[NOTE]
.What you Should Already Know
====
This guide assumes basic {sleha} and {sle} installation knowledge.
For an overview and more information on {sleha} see: 

https://www.suse.com/documentation/sle-ha-12/[{sleha} {sles-version} {sp-version} Documentation]
====

.Storage-based Fencing
NOTE: The STONITH(Shoot the other node in the head) Block Devices(SBD) will be set up on the shared storage server based on {sle}{nbsp}{sp-version} running an iSCSI target. The following sections will guide you through the setup of this device.

[#iscsi-hardware-requirements]



== Preparation and Requirements


[#prepare-two-nodes]
=== Prepare two Nodes

Begin by setting up two servers on KVM with the following software installed:

* SUSE® Linux Enterprise Server 12 SP3 (with all available online updates)

* SUSE Linux Enterprise High Availability Extension 12 SP3 (with all available online updates)

During the {sle} {sp-version} installation select {sleha} to be installed. 

[#node-server-requirements]
.KVM Node Server Disk and RAM Requirements

* 100{nbsp}GB (Btrfs - filesystem)

* For this guide each node requires at least 4096{nbsp}GB of RAM.

Time Synchronization::
During the {sle} installation setup NTP. Cluster nodes must synchronize to an NTP server outside the cluster. For more information, see the Administration Guide for SUSE Linux Enterprise Server 12 SP3:
* http://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html

[IMPORTANT]
.Calculating RAM and Storage Requirements
====
In a production setting your storage needs and memory requirements will be much higher. These numbers depend entirely on your environment. See the <PRODUCTNAME> documentation for recommendations on storage, memory and number of CPUs.
====




[#prepare-the-shared-storage-node]
=== Prepare the iSCSI Shared Storage Node

Continue by setting up a storage server on KVM with the following software installed:

* SUSE® Linux Enterprise Server 12 SP3 (with all available online updates)

* SUSE Linux Enterprise High Availability Extension 12 SP3 (with all available online updates)

During the {sle} {sp-version} installation select {sleha} to be installed. 

[#node-server-requirements]
.KVM Storage Server Disk Requirements

* 500{nbsp}GB (Btrfs - filesystem)

* For this guide the storage server requires at least 4096{nbsp}GB of RAM.

Time Synchronization::
During the {sle} installation setup NTP. Cluster nodes must synchronize to an NTP server outside the cluster. For more information, see the Administration Guide for SUSE Linux Enterprise Server 12 SP3:
* http://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html



[#setup-the-iscsi-storage-server]
== Create the iSCSI Block Devices on the Storage Server

In this procedure you will setup the iSCSI block devices on the storage server.
From this point on we will call this server the *storage node*


[#procedure-setup-the-storage-server]
.Procedure: Configure the iSCSI Block Devices on the Storage Server
****

. Once the storage server has been prepared as explained in: <<#prepare-the-shared-storage-node>>, open a terminal as *{ruser}* and prepare the following image files for use as the block device files:
+

. Prepare the following block device files:
+

[.small]
....
dd if=/dev/zero of=c01-sbd.img bs=1M seek=100 count=0 <1>

dd if=/dev/zero of=c01-spacewalk.img bs=1M seek=102400 count=0 <2>

dd if=/dev/zero of=c01-srv.img bs=1M seek=10240 count=0 <3>

dd if=/dev/zero of=c01-pgsql.img bs=1M seek=51200 count=0 <4>

dd if=/dev/zero of=c01-cacherhn.img bs=1M seek=26200 count=0 <5>

dd if=/dev/zero of=c01-others.img bs=1M seek=52200 count=0 <6>
....
+

[#storage-server-requirements]
.Storage Server Block Devices:

<1> *c01-sbd.img* SBD storage device 100{nbsp}MB 
<2> *c01-spacewalk.img* /var/spacewalk 100{nbsp}GB (A production setup will be greater than 500{nbsp}GB+)
<3> *c01-srv.img* /srv 10{nbsp}GB
<4> *c01-pgsql.img* /var/lib/pgsql 50{nbsp}GB
<5> *c01-cacherhn.img* /var/cache 25{nbsp}GB
<6> *c01-others.img* other 100{nbsp}GB for all other required file systems
****

[#setup-the-block-device-images-using-yast]
=== Setup the Block Device Files with {yast}


.Procedure: Setup the Block Devices
****
. Run {yast} to setup the new block device images on the storage server:
+

....
yast2 iscsi-lio-server
....
+

image:lio-server-service.png[]

. From the *Service* screen select the service to start kbd:[(x) When Booting].
+

image:lio-server-global.png[]

. On the *Global* screen select kbd:[(x) No Discovery Authentication]
+

. On the *Target* screen select kbd:[Add] to add a new iSCSI Target.
+

image:lio-server-target.png[]

. On the *Add iSCSI Target* screen uncheck kbd:[x Use Login Authentication]
****


== Configure the Two Nodes

In this procedure you will finalize setup of the two {sle} nodes.
From this point on we will call these two servers  *node1* and *node2*.

[#procedure-setup-the-two-nodes]
.Procedure: Post Installation Setup of the Two Nodes

. Once the two nodes have been prepared as explained in: <<#prepare-two-nodes>>, open a terminal as *{ruser}* and configure watchdog.

. Create the file [filename]``/etc/modules-load.d/watchdog.conf``.

. Add the following snippet to the file:
+

----
softdog
----
+
. Restart the service:
+
----
systemctl restart systemd-modules-load
----

. Create the file [filename]``/etc/corosync/corosync.conf`` and add the following content:
+
----
totem {
     crypto_hash:    none
     rrp_mode:       none
     join:   60
     max_messages:   20
     cluster_name:   c01 
     vsftype:        none
     secauth:        on
     crypto_cipher:  none
     consensus:      6000
     interface {
           bindnetaddr:    192.168.150.0 <1>
           mcastaddr:      239.255.1.1
           ringnumber:     0
           mcastport:      5405
           ttl:    1
           }
     token:  5000
     version:        2
     transport:      udp
     token_retransmits_before_loss_const:    10
     ip_version:     ipv4
     clear_node_high_bit:    yes
}
logging {
      to_logfile:     no
      logger_subsys {
           debug:  off
           subsys: QUORUM
      }
      to_syslog:      yes
      debug:  off
      timestamp:      on
      to_stderr:      no
      fileline:       off
      syslog_facility:        daemon
}
quorum {
      expected_votes: 2
      two_node:       1
      provider:       corosync_votequorum
}
----
+

<1> The *bindnetaddr* should be set to your network.

. On *node1* generate a *corosync key*. 
To generate a key enter:
+

----
corosync-keygen
---- 
+

The file is generated and is placed in: [filename]``/etc/corosync/authkey``.

. Now as *root* copy the key using *scp* from *node1* to *node2* and place it in: [filename]``/etc/corosync/authkey``.
+
----
scp /etc/corosync/authkey node2@node2:/etc/corosync/authkey <1>
----

<1> In the *scp* snippet *node2* is the user, and the servers short host name was added to the `/etc/hosts` file on node1 with its IP address:
+
./etc/hosts on node1
----
192.168.188.2 node2
----

