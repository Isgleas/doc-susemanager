<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en">
<info>
<title>SUSE Manager Documentation</title>
<date>2019-02-20</date>
</info>
<part xml:id="book.mgr.architecture">
<title>SUSE Manager Architecture</title>
<chapter xml:id="_component_legend">
<title>Component Legend</title>
<simpara>These diagram components will be used in the following sections explaining the architecture of SUSE Manager. Components in SUSE Manager can communicate in three ways:</simpara>
<itemizedlist>
<listitem>
<simpara>One way</simpara>
</listitem>
<listitem>
<simpara>Two way</simpara>
</listitem>
<listitem>
<simpara>Scheduled (time based)</simpara>
</listitem>
</itemizedlist>
<section xml:id="_types_of_components">
<title>Types of Components</title>
<formalpara>
<title>One Way</title>
<para>Components that communicate in only one direction are represented by:</para>
</formalpara>
<figure>
<title>One way communication between components</title>
<mediaobject>
<imageobject>
<imagedata fileref="dia-one-way-component.png"/>
</imageobject>
<textobject><phrase>dia one way component</phrase></textobject>
</mediaobject>
</figure>
<formalpara>
<title>Two Way</title>
<para>Components that communicate in both directions are represented by:</para>
</formalpara>
<figure>
<title>Two way communication between components</title>
<mediaobject>
<imageobject>
<imagedata fileref="dia-two-way-component.png"/>
</imageobject>
<textobject><phrase>dia two way component</phrase></textobject>
</mediaobject>
</figure>
<formalpara>
<title>Database Connections</title>
<para>A component that reads and writes to the database communicates in both directions are represented by:</para>
</formalpara>
<figure>
<title>Two way communication between a component and the database(read and write)</title>
<mediaobject>
<imageobject>
<imagedata fileref="dia-database-communication.png"/>
</imageobject>
<textobject><phrase>dia database communication</phrase></textobject>
</mediaobject>
</figure>
<formalpara>
<title>Scheduled (Time based)</title>
<para>Components that run on a schedule are represented by:</para>
</formalpara>
<figure>
<title>Component that runs on a schedule</title>
<mediaobject>
<imageobject>
<imagedata fileref="dia-component-schedule.png"/>
</imageobject>
<textobject><phrase>dia component schedule</phrase></textobject>
</mediaobject>
</figure>
</section>
</chapter>
<chapter xml:id="chapt.arch.salt">
<title>Salt Architecture</title>
<section xml:id="_salt_stack_overview">
<title>Salt Stack Overview</title>
<simpara>Some description&#8230;&#8203;</simpara>
<formalpara>
<title>Salt Stack Diagram</title>
<para><inlinemediaobject>
<imageobject>
<imagedata fileref="dia-salt-stack.png"/>
</imageobject>
<textobject><phrase>dia salt stack</phrase></textobject>
</inlinemediaobject></para>
</formalpara>
</section>
<section xml:id="_core_salt_components">
<title>Core Salt Components</title>
<simpara>Comming soon&#8230;&#8203;</simpara>
<figure>
<title>Salt Core</title>
<mediaobject>
<imageobject>
<imagedata fileref="dia-salt-stack-core-block.png"/>
</imageobject>
<textobject><phrase>dia salt stack core block</phrase></textobject>
</mediaobject>
</figure>
</section>
<section xml:id="salt-contact-methods-overview">
<title>Salt Contact Methods</title>
<section xml:id="_choosing_a_contact_method_for_salt">
<title>Choosing a Contact Method for Salt</title>
<simpara>SUSE Manager provides several methods for communication between client and server.
All commands that the SUSE Manager server sends to its clients will be routed through one of these contact methods.</simpara>
<simpara>The contact method you select for Salt will depend on your network infrastructure.
The following sections provide a starting point for selecting a method which best suits your network environment.</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="arch.contact.method.salt.pull">Salt Pull</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="arch.contact.methods.saltssh.push">Salt SSH Push</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="arch.contact.method.salt.ssh.push.tunnel">Salt SSH Push and Tunnel</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="arch.contact.method.salt.pull">
<title>Salt Pull</title>

</section>
<section xml:id="arch.contact.methods.saltssh.push">
<title>Salt SSH Push</title>
<simpara>Salt SSH Push is intended to be used in environments where your Salt clients cannot reach the SUSE Manager server directly to regularly checking in and, for example, fetch package updates.</simpara>
<note>
<title>Push via SSH</title>
<simpara>This feature is not related to Push via SSH for the traditional clients.
For Push via SSH, see xref:bp.contact.methods.ssh.push[Salt SSH Push].</simpara>
</note>
<section xml:id="_overview">
<title>Overview</title>
<figure>
<title>Push via Salt SSH Contact Method</title>
<mediaobject>
<imageobject>
<imagedata fileref="salt-ssh-contact-taigon.png" width="80%"/>
</imageobject>
<textobject><phrase>salt ssh contact taigon</phrase></textobject>
</mediaobject>
</figure>
<simpara>Salt provides &#8220;Salt SSH&#8221;
 (<literal role="command">salt-ssh</literal>), a feature to manage clients from a server.
It works without installing Salt related software on clients.
Using Salt SSH there is no need to have minions connected to the Salt master.
Using this as a SUSE Manager connect method, this feature provides similar functionality for Salt clients as the traditional Push via SSH feature for traditional clients.</simpara>
<simpara>This feature allows:</simpara>
<itemizedlist>
<listitem>
<simpara>Managing Salt entitled systems with the Push via SSH contact method using Salt SSH.</simpara>
</listitem>
<listitem>
<simpara>Bootstrapping such systems.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_requirements">
<title>Requirements</title>
<itemizedlist>
<listitem>
<simpara>SSH daemon must be running on the remote system and reachable by the <literal role="systemitem">salt-api</literal> daemon (typically running on the SUSE Manager server).</simpara>
</listitem>
<listitem>
<simpara>Python must be available on the remote system (Python must be supported by the installed Salt). Currently: python 2.6.</simpara>
</listitem>
</itemizedlist>
<note>
<title>Unsupported Systems</title>
<simpara>Red Hat Enterprise Linux
and CentOS versions &#8656; 5 are not supported because they do not have Python 2.6 by default.</simpara>
</note>
</section>
<section xml:id="_bootstrapping">
<title>Bootstrapping</title>
<simpara>To bootstrap a Salt SSH system, proceed as follows:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Open the <menuchoice><guimenu>Bootstrap Minions</guimenu> <guimenuitem>] dialog in the Web UI (menu:Systems[Bootstrapping</guimenuitem></menuchoice> ).</simpara>
</listitem>
<listitem>
<simpara>Fill out the required fields. Select an <menuchoice><guimenu>Activation Key</guimenu> <guimenuitem>] with the menu:Push via SSH[</guimenuitem></menuchoice> contact method configured. For more information about activation keys, see:
xref:ref.webui.systems.activ-keys.</simpara>
</listitem>
<listitem>
<simpara>Check the <guimenu>Manage system completely via SSH</guimenu> option.</simpara>
</listitem>
<listitem>
<simpara>Confirm with clicking the <guimenu>Bootstrap</guimenu> button.</simpara>
</listitem>
</orderedlist>
<simpara>Now the system will be bootstrapped and registered in SUSE Manager.
If done successfully, it will appear in the <guimenu>Systems</guimenu> list.</simpara>
</section>
<section xml:id="_configuration">
<title>Configuration</title>
<simpara>There are two kinds of parameters for Push via Salt SSH:</simpara>
<itemizedlist>
<listitem>
<simpara>Bootstrap-time parameters - configured in the <guimenu>Bootstrapping</guimenu> page:</simpara>
<itemizedlist>
<listitem>
<simpara>Host</simpara>
</listitem>
<listitem>
<simpara>Activation key</simpara>
</listitem>
<listitem>
<simpara>Password - used only for bootstrapping, not saved anywhere; all future SSH sessions are authorized via a key/certificate pair</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Persistent parameters - configured SUSE Manager-wide:</simpara>
<itemizedlist>
<listitem>
<simpara>sudo user - same as in bp.contact.methods.ssh.push.sudo.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="_action_execution">
<title>Action Execution</title>
<simpara>The Push via Salt SSH feature uses a taskomatic job to execute scheduled actions using <literal role="command">salt-ssh</literal>.
The taskomatic job periodically checks for scheduled actions and executes them.
While on traditional clients with SSH push configured only <literal role="command">rhn_check</literal> is executed via SSH, the Salt SSH push job executes a complete <literal role="command">salt-ssh</literal> call based on the scheduled action.</simpara>
</section>
<section xml:id="_known_limitation">
<title>Known Limitation</title>
<itemizedlist>
<listitem>
<simpara>OpenSCAP auditing is not available on Salt SSH minions.</simpara>
</listitem>
<listitem>
<simpara>Beacons do not work with Salt SSH.</simpara>
<itemizedlist>
<listitem>
<simpara>Installing a package on a system using <literal role="command">zypper</literal> will not invoke the package refresh.</simpara>
</listitem>
<listitem>
<simpara>Virtual Host functions (for example, a host to guests) will not work if the virtual host system is Salt SSH-based.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="_for_more_information">
<title>For More Information</title>
<simpara>For more information, see</simpara>
<itemizedlist>
<listitem>
<simpara><link xl:href="https://wiki.microfocus.com/index.php/SUSE_Manager/SaltSSHServerPush">https://wiki.microfocus.com/index.php/SUSE_Manager/SaltSSHServerPush</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="https://docs.saltstack.com/en/latest/topics/ssh/">https://docs.saltstack.com/en/latest/topics/ssh/</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="arch.contact.method.salt.ssh.push.tunnel">
<title>Salt SSH Push &amp; Tunnel</title>

</section>
<section xml:id="_boostrapping_ui">
<title>Boostrapping UI</title>

</section>
<section xml:id="_onboarding_and_registration">
<title>Onboarding and Registration</title>

</section>
<section xml:id="arch.salt.data.productname">
<title>Salt Data and SUSE Manager</title>
<section xml:id="_suse_manager_salt_global_static_data">
<title>SUSE Manager Salt Global Static Data</title>
<simpara>Global static data <emphasis role="strong">should not</emphasis> be customized or edited by SUSE Manager users. This data is generated by the server.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Salt Global Static Data</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Directory</entry>
<entry align="left" valign="top">Function</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>/usr/share/susemanager/salt/</simpara></entry>
<entry align="left" valign="top"><simpara>custom modules, states, grains</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/usr/share/susemanager/pillar_data/</simpara></entry>
<entry align="left" valign="top"><simpara>global pillar data</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/usr/share/susemanager/formulas/</simpara></entry>
<entry align="left" valign="top"><simpara>formulas</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="_suse_manager_generated_data_per_minion">
<title>SUSE Manager Generated Data Per Minion</title>
<simpara>Generated data for minions <emphasis role="strong">should not</emphasis> be customized or edited by SUSE Manager users. This data is generated by the server.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Salt Generated Data Per Minion</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Directory</entry>
<entry align="left" valign="top">Function</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>/srv/susemanager/pillar_data/</simpara></entry>
<entry align="left" valign="top"><simpara>custom modules, states, grains</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/usr/share/susemanager/pillar_data/</simpara></entry>
<entry align="left" valign="top"><simpara>global pillar data</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/srv/susemanager/formulas_data/</simpara></entry>
<entry align="left" valign="top"><simpara>formulas</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="_custom_salt_data">
<title>Custom Salt Data</title>
<simpara>The following directories are reserved for use by users <emphasis role="strong">and should be</emphasis> customized and edited by SUSE Manager users. The custom salt data place here will be calculated and combined with the content generated listed above when running a highstate.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Salt Generated Data Per Minion</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Directory</entry>
<entry align="left" valign="top">Function</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>/srv/salt/</simpara></entry>
<entry align="left" valign="top"><simpara>user defined custom modules, states, grains</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/srv/pillar/</simpara></entry>
<entry align="left" valign="top"><simpara>user defined global pillar data</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/srv/formula_metadata</simpara></entry>
<entry align="left" valign="top"><simpara>user defined formulas</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
</chapter>
<chapter xml:id="chapt.arch.traditional">
<title>Traditional Architecture</title>
<section xml:id="_the_traditional_stack">
<title>The Traditional Stack</title>
<formalpara>
<title>The Traditional Stack</title>
<para><inlinemediaobject>
<imageobject>
<imagedata fileref="dia-traditional-stack.png"/>
</imageobject>
<textobject><phrase>dia traditional stack</phrase></textobject>
</inlinemediaobject></para>
</formalpara>
</section>
<section xml:id="bp.contact.methods.rhnsd">
<title>Traditional Contact Method (rhnsd)</title>
<section xml:id="_the_default_contact_method">
<title>The Default Contact Method</title>
<simpara>The SUSE Manager <emphasis role="strong">rhnsd</emphasis> daemon runs on client systems and periodically connects with SUSE Manager to check for new updates and notifications.
The daemon, which runs in the background, is started by <emphasis role="strong">rhnsd.service</emphasis>.
By default, it will check every 4 hours for new actions, therefore it may take some time for your clients to begin updating after actions have been scheduled for them.</simpara>
<simpara>To check for updates, <emphasis role="strong">rhnsd</emphasis> runs the external <emphasis role="strong">mgr_check</emphasis> program located in <literal>/usr/sbin/</literal>.
This is a small application that establishes the network connection to SUSE Manager.
The SUSE Manager daemon does not listen on any network ports or talk to the network directly.
All network activity is done via the <emphasis role="strong">mgr_check</emphasis> utility.</simpara>
<warning>
<title>Auto accepting (EULAs)</title>
<simpara>When new packages or updates are installed on the client using SUSE Manager, any end user licence agreements (EULAs) are automatically accepted.
To review a package EULA, open the package detail page in the Web UI.</simpara>
</warning>
<simpara>This figure provides an overview of the default <emphasis role="strong">rhnsd</emphasis> process path.
All items left of the <emphasis role="strong">Python XMLRPC server</emphasis> block represent processes running on an SUSE Manager client.</simpara>
<figure>
<title>rhnsd Contact Method</title>
<mediaobject>
<imageobject>
<imagedata fileref="dia-rhnsd-taigon.png" width="80%"/>
</imageobject>
<textobject><phrase>dia rhnsd taigon</phrase></textobject>
</mediaobject>
</figure>
<section xml:id="_configuring_suse_manager_rhnsd_daemon">
<title>Configuring SUSE Manager rhnsd Daemon</title>
<simpara>The SUSE Manager daemon can be configured by editing the file on the client:</simpara>
<screen>/etc/sysconfig/rhn/rhnsd</screen>
<simpara>This is the configuration file the rhnsd initialization script uses.
An important parameter for the daemon is its check-in frequency.
The default interval time is four hours (240 minutes). If you modify the configuration file, you must as root restart the daemon with:</simpara>
<screen>systemctl rhnsd restart</screen>
<important>
<title>Minimum Allowed Check-in Parameter</title>
<simpara>The minimum allowed time interval is one hour (60 minutes). If you set the interval below one hour, it will change back to the default of 4 hours (240 minutes).</simpara>
</important>
</section>
<section xml:id="_viewing_rhnsd_daemon_status">
<title>Viewing rhnsd Daemon Status</title>
<simpara>As the root you may view the status of rhnsd by typing the command:</simpara>
<screen>systemctl status rhnsd</screen>
</section>
</section>
</section>
<section xml:id="bp.contact.methods.osad">
<title>Traditional Contact Method (osad)</title>
<simpara>OSAD is an alternative contact method between SUSE Manager and its clients.
By default, SUSE Manager uses <literal role="systemitem">rhnsd</literal>, which contacts the server every four hours to execute scheduled actions.
OSAD allows registered client systems to execute scheduled actions immediately.</simpara>
<simpara>OSAD has several distinct components:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal role="systemitem">osa-dispatcher</literal> service runs on the server, and uses database checks  to determine if clients need to be pinged, or if actions need to be executed.</simpara>
</listitem>
<listitem>
<simpara>The <literal role="systemitem">osad</literal> service runs on the client. It responds to pings from <literal role="systemitem">osa-dispatcher</literal> and runs <literal role="command">mgr_check</literal> to execute actions when directed to do so.</simpara>
</listitem>
<listitem>
<simpara>The <literal role="systemitem">jabberd</literal> service is a daemon that uses the <literal role="systemitem">XMPP</literal> protocol for communication between the client and the server.
The <literal role="systemitem">jabberd</literal> service also handles authentication.</simpara>
</listitem>
<listitem>
<simpara>The <literal role="command">mgr_check</literal> tool runs on the client to execute actions.
It is triggered by communication from the <literal role="systemitem">osa-dispatcher</literal> service.</simpara>
</listitem>
</itemizedlist>
<simpara>The <literal role="systemitem">osa-dispatcher</literal> periodically runs a query to check when clients last showed network activity.
If it finds a client that has not shown activity recently, it will use <literal role="systemitem">jabberd</literal> to ping all <literal role="systemitem">osad</literal> instances running on all clients registered with your SUSE Manager server.
The <literal role="systemitem">osad</literal> instances respond to the ping using <literal role="systemitem">jabberd</literal>, which is running in the background on the server.
When the <literal role="systemitem">osa-dispatcher</literal> receives the response, it marks the client as online.
If the <literal role="systemitem">osa-dispatcher</literal> fails to receive a response within a certain period of time, it marks the client as offline.</simpara>
<simpara>When you schedule actions on an OSAD-enabled system, the task will be carried out  immediately.
The <literal role="systemitem">osa-dispatcher</literal> periodically checks clients for actions that need to be executed.
If an outstanding action is found, it uses <literal role="systemitem">jabberd</literal> to execute <literal role="command">mgr_check</literal> on the client, which will then execute the action.</simpara>
<figure>
<title>osad Contact Method</title>
<mediaobject>
<imageobject>
<imagedata fileref="dia-osad.png"/>
</imageobject>
<textobject><phrase>dia osad</phrase></textobject>
</mediaobject>
</figure>
<section xml:id="_enabling_and_configuring_osad">
<title>Enabling and Configuring OSAD</title>
<simpara>This section covers enabling the <literal role="systemitem">osa-dispatcher</literal> and <literal role="systemitem">osad</literal> services, and performing initial setup.</simpara>
<simpara>OSAD clients use the fully qualified domain name (FQDN) of the server to communicate with the <literal role="systemitem">osa-dispatcher</literal> service.</simpara>
<simpara>SSL is required for <literal role="systemitem">osad</literal> communication.
If SSL certificates are not available, the daemon on your client systems will fail to connect.
Make sure your firewall rules are set to allow the required ports.
For more information, see xref:tab.install.ports.server[Server Ports].</simpara>
<orderedlist numeration="arabic">
<title>Procedure: Enabling OSAD</title>
<listitem>
<simpara>On your SUSE Manager server, as the root user, start the <literal role="systemitem">osa-dispatcher</literal> service:</simpara>
<screen>systemctl start osa-dispatcher</screen>
</listitem>
<listitem>
<simpara>On each client machine, install the <literal role="systemitem">osad</literal> package from the <literal role="systemitem">Tools</literal> child channel.
The <literal role="systemitem">osad</literal> package should be installed on clients only.
If you install the <literal role="systemitem">osad</literal> package on your SUSE Manager Server, it will conflict with the <literal role="systemitem">osa-dispatcher</literal> package.</simpara>
</listitem>
<listitem>
<simpara>On the client systems, as the root user, start the <literal role="systemitem">osad</literal> service:</simpara>
<screen>systemctl start osad</screen>
<simpara>Because <literal role="systemitem">osad</literal> and <literal role="systemitem">osa-dispatcher</literal> are run as services, you can use standard commands to manage them, including <literal role="command">stop</literal>, <literal role="command">restart</literal>, and <literal role="command">status</literal>.</simpara>
</listitem>
</orderedlist>
<formalpara>
<title>Configuration and Log Files</title>
<para>Each OSAD component is configured by local configuration files.
We recommend you keep the default configuration parameters for all OSAD components.</para>
</formalpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Component</entry>
<entry align="left" valign="top">Location</entry>
<entry align="left" valign="top">Path to Configuration File</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal role="systemitem">osa-dispatcher</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Server</simpara></entry>
<entry align="left" valign="top"><simpara><literal role="path">/etc/rhn/rhn.conf</literal> Section: <literal role="systemitem">OSA configuration</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal role="systemitem">osad</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Client</simpara></entry>
<entry align="left" valign="top"><simpara><literal role="path">/etc/sysconfig/rhn/osad.conf</literal> <literal role="path">/etc/syseconfig/rhn/up2date</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal role="systemitem">osad</literal>  log file</simpara></entry>
<entry align="left" valign="top"><simpara>Client</simpara></entry>
<entry align="left" valign="top"><simpara><literal role="path">/var/log/osad</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal role="systemitem">jabberd</literal> log file</simpara></entry>
<entry align="left" valign="top"><simpara>Both</simpara></entry>
<entry align="left" valign="top"><simpara><literal role="path">/var/log/messages</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<formalpara>
<title>Troubleshooting OSAD</title>
<para>If your OSAD clients cannot connect to the server, or if the <literal role="systemitem">jabberd</literal> service takes a lot of time responding to port 5552, it could be because you have exceeded the open file count.</para>
</formalpara>
<simpara>Every client needs one always-open TCP connection to the server, which consumes a single file handler.
If the number of file handlers currently open exceeds the maximum number of files that <literal role="systemitem">jabberd</literal> is allowed to use, <literal role="systemitem">jabberd</literal> will queue the requests, and refuse connections.</simpara>
<simpara>To resolve this issue, you can increase the file limits for <literal role="systemitem">jabberd</literal> by editing the <literal role="path">/etc/security/limits.conf</literal> configuration file and adding these lines:</simpara>
<screen>jabbersoftnofile5100
jabberhardnofile6000</screen>
<simpara>Calculate the limits required for your environment by adding 100 to the number of clients for the soft limit, and 1000 to the current number of clients for the soft limit.
In the example above, we have assumed 500 current clients, so the soft limit is 5100, and the hard limit is 6000.</simpara>
<simpara>You will also need to update the <literal role="systemitem">max_fds</literal> parameter in the <literal role="path">/etc/jabberd/c2s.xml</literal> file with your chosen hard limit:</simpara>
<screen>&lt;max_fds&gt;6000&lt;/max_fds&gt;</screen>
</section>
</section>
<section xml:id="_traditional_ssh_push">
<title>Traditional SSH Push</title>

</section>
<section xml:id="_traditional_ssh_pull">
<title>Traditional SSH Pull</title>

</section>
</chapter>
<chapter xml:id="chapt.arch.repositories">
<title>Repositories</title>
<section xml:id="_suse_manager_repositories_overview">
<title>SUSE Manager Repositories Overview</title>

</section>
<section xml:id="_pool_repositories">
<title>Pool Repositories</title>

</section>
<section xml:id="_devel_repositories">
<title>Devel Repositories</title>

</section>
<section xml:id="_tool_repositories">
<title>Tool Repositories</title>

</section>
<section xml:id="_maintenance_repositories">
<title>Maintenance Repositories</title>

</section>
<section xml:id="_repository_synchronization">
<title>Repository Synchronization</title>

</section>
</chapter>
<chapter xml:id="chapt.arch.component.index">
<title>Component Index</title>
<section xml:id="arch.component.apache">
<title>Apache</title>
<formalpara>
<title>Functions</title>
<para>Apache is a primary component of SUSE Manager. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Handles HTTP(S) communication</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Serves Static Files</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">HTTP gateway to: Apache Tomcat, the Python XMLRPC server and Cobbler</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for Apache are located in:</emphasis></para>
</formalpara>
<screen>/var/log/apache2/error_log</screen>
</section>
<section xml:id="arch.component.apache.tomcat">
<title>Apache Tomcat</title>
<formalpara>
<title>Functions</title>
<para>Apache Tomcat is a primary component of SUSE Manager. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Contains servlet (Java) applications</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">The most important servlet is the RHN servlet:</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Handles the majority of the Web UI</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Public XMLRPC API</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for Apache Tomcat are located in:</emphasis></para>
</formalpara>
<screen>/var/log/rhn/rhn_web_ui.log
/var/log/rhn/rhn_web_api.log
/var/log/tomcat/catalina.out
/var/log/tomcat/catalina.*.log</screen>
</section>
<section xml:id="arch.component.python.xmlrpc.server">
<title>Python XMLRPC Server</title>
<formalpara>
<title>Functions</title>
<para>The Python XMLRPC Server is a primary component of SUSE Manager. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Provides the private XMLRPC API</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Used primarily by client tools (mgr_check)</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for the Python XMLRPC Server are located in:</emphasis></para>
</formalpara>
<screen>/var/log/apache2/error_log
/var/log/rhn/rhn_server_xmlrpc.log</screen>
</section>
<section xml:id="arch.component.taskomatic">
<title>Taskomatic</title>
<formalpara>
<title>Functions</title>
<para>Taskomatic is a primary component of SUSE Manager. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Taskomatic handles most background jobs</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Patch applicability status refresh</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Server side scheduling</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">SSH push</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Cobbler database sync</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Repository synchronization and repository metadata generation</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">CVE audit pre-computation</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Cleanup Jobs</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Log Files</title>
<para><emphasis>Log files for taskomatic are located in:</emphasis></para>
</formalpara>
<screen>/var/log/rhn/rhn_taskomatic_daemon.log
/var/log/rhn/reposync/*</screen>
</section>
<section xml:id="arch.component.database">
<title>Database</title>
<formalpara>
<title>Functions</title>
<para>The database is a primary component of SUSE Manager. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Primarily stores application data</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Functions as a data exchange area between components</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for the database are located in:</emphasis></para>
</formalpara>
<screen>/var/lib/pgsql/data/pg_log/*</screen>
</section>
<section xml:id="arch.component.mgr.sync">
<title>mgr-sync</title>
<formalpara>
<title>Functions</title>
<para><emphasis role="strong">mgr-sync</emphasis> is a command line tool for SUSE Manager.
It performs the following function.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">mgr-sync is a command line tool that synchronizes with SUSE Customer Center(SCC) and retrieves data and package repositories.</emphasis></simpara>
</listitem>
</itemizedlist>
<important>
<title>mgr-sync and Open Source Distributions</title>
<simpara><emphasis>This tool is designed for use with a support subscription or trial account with SUSE Customer Center. It is not required for open source distributions(OpenSUSE Leap , CentOS, Ubuntu, etc.).</emphasis></simpara>
</important>
<formalpara>
<title>mgr-sync --help</title>
<para><emphasis>The following options are available for the <emphasis role="strong">mgr-sync</emphasis> command:</emphasis></para>
</formalpara>
<screen>mgr-sync --help
usage: mgr-sync [-h] [--version] [-v] [-s] [-d {1,2,3}]
                {list,add,refresh,delete} ...

Synchronize SUSE Manager repositories.

optional arguments:
  -h, --help            show this help message and exit
  --version             Print mgr-sync version
  -v, --verbose         Be verbose
  -s, --store-credentials
                        Store credentials to the local dot file.
  -d {1,2,3}, --debug {1,2,3}
                        Log additional debug information depending on DEBUG

Subcommands:
  {list,add,refresh,delete}
    list                List channels, SCC organization credentials or
                        products
    add                 add channels, SCC organization credentials or products
    refresh             Refresh product, channel and subscription
    delete              Delete SCC organization credentials</screen>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for the mgr-sync tool are located in:</emphasis></para>
</formalpara>
<screen>/var/log/rhn/mgr-sync.log
/var/log/rhn/rhn_web_api.log</screen>
</section>
<section xml:id="arch.component.spacewalk.repo.sync">
<title>spacewalk-repo-sync</title>
<simpara>Test</simpara>
<formalpara>
<title>Functions</title>
<para>spacewalk-repo-sync is a command line tool for SUSE Manager. It performs the following functions.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Copies a repo’s metadata to the database</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Copies a repo’s RPM files to the filesystem</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>mgr-sync --help</title>
<para><emphasis>The following options are available for the <emphasis role="strong">spacewalk-repo-sync</emphasis> tool:</emphasis></para>
</formalpara>
<screen>spacewalk-repo-sync --help
Usage: spacewalk-repo-sync [options]

Options:
  -h, --help            show this help message and exit
  -l, --list            List the custom channels with the assosiated
                        repositories.
  -s, --show-packages   List all packages in a specified channel.
  -u URL, --url=URL     The url of the repository. Can be used multiple times.
  -c CHANNEL_LABEL, --channel=CHANNEL_LABEL
                        The label of the channel to sync packages to. Can be
                        used multiple times.
  -p PARENT_LABEL, --parent-channel=PARENT_LABEL
                        Synchronize the parent channel and all its child
                        channels.
  -d, --dry-run         Test run. No sync takes place.
  --latest              Sync latest packages only. Use carefully - you might
                        need to fix some dependencies on your own.
  -g CONFIG, --config=CONFIG
                        Configuration file
  -t REPO_TYPE, --type=REPO_TYPE
                        Force type of repository ("yum", "uln" and "deb" are
                        supported)
  -f, --fail            If a package import fails, fail the entire operation
  -n, --non-interactive
                        Do not ask anything, use default answers
  -i FILTERS, --include=FILTERS
                        Comma or space separated list of included packages or
                        package groups.
  -e FILTERS, --exclude=FILTERS
                        Comma or space separated list of excluded packages or
                        package groups.
  --email               e-mail a report of what was synced/imported
  --traceback-mail=TRACEBACK_MAIL
                        alternative email address(es) for sync output (--email
                        option)
  --no-errata           Do not sync errata
  --no-packages         Do not sync packages
  --sync-kickstart      Sync kickstartable tree
  --force-all-errata    Process metadata of all errata, not only missing.
  --batch-size=BATCH_SIZE
                        max. batch size for package import (debug only)
  -Y, --deep-verify     Do not use cached package checksums
  -v, --verbose         Verbose output. Possible to accumulate: -vvv</screen>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for the <emphasis role="strong">spacewalk-repo-sync</emphasis> tool are located in:</emphasis></para>
</formalpara>
<screen>/var/log/rhn/reposync/*</screen>
</section>
<section xml:id="arch.component.osa.dispatcher">
<title>osa-dispatcher</title>
<formalpara>
<title>Functions</title>
<para><emphasis role="strong">osa-dispatcher</emphasis> is a component of SUSE Manager. It performs the following function in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Monitors database for actions, informing osad clients when they need to run them</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>osa-dispatcher --help</title>
<para><emphasis>The following options are available for the <emphasis role="strong">osa-dispatcher</emphasis>:</emphasis></para>
</formalpara>
<screen>osa-dispatcher --help
Usage: osa-dispatcher [options]

Options:
  -v, --verbose        Increase verbosity
  -N, --nodetach       Suppress backgrounding and detachment of the process
  --pid-file=PID_FILE  Write to this PID file
  --logfile=LOGFILE    Write log information to this file
  -h, --help           show this help message and exit</screen>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for the <emphasis role="strong">osa-dispatcher</emphasis> are located in:</emphasis></para>
</formalpara>
<screen>/var/log/rhn/osa_dispatcher.log</screen>
</section>
<section xml:id="arch.component.jabberd">
<title>jabberd</title>
<formalpara>
<title>Functions</title>
<para>jabberd is a component of SUSE Manager. It performs the following function in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Implements the Jabber (XMPP) protocol that osa-dispatcher uses</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for jabberd are located in:</emphasis></para>
</formalpara>
<screen>/var/log/messages</screen>
</section>
<section xml:id="arch.component.mgr.check">
<title>mgr_check</title>
<formalpara>
<title>Functions</title>
<para><emphasis role="strong">mgr_check</emphasis> is a primary component of SUSE Manager. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Client-side command line tool for legacy clients that checks for actions on the server and executes them</emphasis></simpara>
</listitem>
</itemizedlist>
<note>
<title>mgr_check and rhn_check</title>
<simpara><emphasis role="strong">mgr_check</emphasis> is symlinked to <emphasis role="strong">rhn_check</emphasis> in <literal>/usr/sbin/</literal>.
Both <emphasis>mgr_check</emphasis> and <emphasis>rhn_check</emphasis> can be used for checking for actions on the server.</simpara>
<screen>lrwxrwxrwx 1 root root           9 Sep  9 09:05 mgr_check -&gt; rhn_check*</screen>
</note>
<formalpara>
<title>mgr_check --help</title>
<para><emphasis>The following options are available for the <emphasis role="strong">rhn_check</emphasis> on your legacy clients:</emphasis></para>
</formalpara>
<screen>mgr_check --help
Usage: rhn_check [options]

Options:
  -v, --verbose         Show additional output. Repeat for more detail.
  --proxy=PROXY         Specify an http proxy to use
  --proxyUser=PROXYUSER
                        Specify a username to use with an authenticated http
                        proxy
  --proxyPassword=PROXYPASSWORD
                        Specify a password to use with an authenticated http
                        proxy
  --version             show program's version number and exit
  -h, --help            show this help message and exit</screen>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for the <emphasis role="strong">mgr_check</emphasis> are located on your legacy clients in:</emphasis></para>
</formalpara>
<screen>/var/log/up2date</screen>
</section>
<section xml:id="arch.component.zypp.plugin.spacewalk">
<title>zypp-plugin-spacewalk</title>
<formalpara>
<title>Functions</title>
<para><emphasis role="strong">zypp-plugin-spacewalk</emphasis> is a component of SUSE Manager. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Client-side add-on to zypper for legacy clients</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Exposes SUSE Manager channels as zypper repositories</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">The plugin is not required on salt-minions</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for the <emphasis role="strong">zypp-plugin-spacewalk</emphasis> are located on your legacy clients in:</emphasis></para>
</formalpara>
<screen>/var/log/zypper.log
/var/log/zypp/*</screen>
</section>
<section xml:id="arch.component.rhnsd">
<title>rhnsd</title>
<formalpara>
<title>Functions</title>
<para><emphasis role="strong">rhnsd</emphasis> is a primary component of SUSE Manager. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Client-side daemon for legacy clients</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Periodically calls mgr_check(symlinked to rhn_check)</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Randomizes check time not to overload the server</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>rhnsd --help</title>
<para>The following options are available for use with rhnsd on your legacy clients:</para>
</formalpara>
<screen>rhnsd --help
Usage: rhnsd [OPTION...]
Spacewalk Services Daemon

  -f, --foreground           Run in foreground
  -i, --interval=MINS        Connect to Spacewalk every MINS minutes
  -?, --help                 Give this help list
      --usage                Give a short usage message
  -V, --version              Print program version

Mandatory or optional arguments to long options are also mandatory or optional
for any corresponding short options.</screen>
</section>
<section xml:id="arch.component.osad">
<title>osad</title>
<formalpara>
<title>Functions</title>
<para><emphasis role="strong">osad</emphasis> is a primary component of SUSE Manager. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Client-side daemon for legacy clients</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Calls mgr_check(rhn_check) when notified by Jabber</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>osad --help</title>
<para>The following options are available for use with <emphasis role="strong">osad</emphasis> on your legacy clients:</para>
</formalpara>
<screen>osad --help
Usage: osad [options]

Options:
  -v, --verbose         Increase verbosity
  -N, --nodetach        Suppress backgrounding and detachment of the process
  --pid-file=PID_FILE   Write to this PID file
  --logfile=LOGFILE     Write log information to this file
  --cfg=CFG             Use this configuration file for defaults
  --jabber-server=JABBER_SERVER
                        Primary jabber server to connect to
  -h, --help            show this help message and exit</screen>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for <emphasis role="strong">osad</emphasis> are located in:</emphasis></para>
</formalpara>
<screen>/var/log/osad</screen>
</section>
<section xml:id="arch.component.salt.master">
<title>salt-master</title>
<formalpara>
<title>Functions</title>
<para>The <emphasis role="strong">salt-master</emphasis> is a primary component of SUSE Manager. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Core process of Salt on the server side</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Provides communication with salt minions</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Handles Salt jobs, publishes to the Salt event Bus</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Handles minion responses</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Manages states, highstates, pillar information, etc</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>salt-master --help</title>
<para>The following options are available for the <emphasis role="strong">salt-master</emphasis>. The following list is not comprehensive, for more information see: <link xl:href="http://docs.saltstack.com">The Saltstack Docs</link></para>
</formalpara>
<simpara><emphasis role="strong">Options:</emphasis></simpara>
<screen>salt-master --help
Usage: salt-master [options]

The Salt Master, used to control the Salt Minions

Options:
  --version             show program's version number and exit
  -V, --versions-report
                        Show program's dependencies version number and exit.
  -h, --help            show this help message and exit
  --saltfile=SALTFILE   Specify the path to a Saltfile. If not passed, one
                        will be searched for in the current working directory.
  -c CONFIG_DIR, --config-dir=CONFIG_DIR
                        Pass in an alternative configuration directory.
                        Default: '/etc/salt'.
  -u USER, --user=USER  Specify user to run salt-master.
  -d, --daemon          Run the salt-master as a daemon.
  --pid-file=PIDFILE    Specify the location of the pidfile. Default:
                        '/var/run/salt-master.pid'.

  Logging Options:
    Logging options which override any settings defined on the
    configuration files.

    -l LOG_LEVEL, --log-level=LOG_LEVEL
                        Console logging log level. One of u'all', u'garbage',
                        u'trace', u'debug', u'profile', u'info', u'warning',
                        u'error', u'critical', u'quiet'. Default: 'warning'.
    --log-file=LOG_FILE
                        Log file path. Default: '/var/log/salt/master'.
    --log-file-level=LOG_LEVEL_LOGFILE
                        Logfile logging log level. One of u'all', u'garbage',
                        u'trace', u'debug', u'profile', u'info', u'warning',
                        u'error', u'critical', u'quiet'. Default: 'warning'.

You can find additional help about salt-master issuing "man salt-master" or on
http://docs.saltstack.com</screen>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for <emphasis role="strong">salt-master</emphasis> are located in:</emphasis></para>
</formalpara>
<screen>/var/log/salt/master</screen>
</section>
<section xml:id="arch.component.salt.api">
<title>salt-api</title>
<formalpara>
<title>Functions</title>
<para>The <emphasis role="strong">salt-api</emphasis> is a primary component of SUSE Manager. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Internal API communicates the Java side of SUSE Manager with the salt-master</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Provides HTTPS and websocket interfaces with the salt-master</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Handles the SSH connections to minions (SSH Push)</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>salt-api --help</title>
<para>The following options are available for the <emphasis role="strong">salt-api</emphasis>. The following list is not comprehensive, for more information see: <link xl:href="http://docs.saltstack.com">The Saltstack Docs</link></para>
</formalpara>
<simpara><emphasis role="strong">Options:</emphasis></simpara>
<screen>salt-api --help
Usage: salt-api [options]

The Salt API system manages network API connectors for the Salt Master

Options:
  --version             show program's version number and exit
  -V, --versions-report
                        Show program's dependencies version number and exit.
  -h, --help            show this help message and exit
  -c CONFIG_DIR, --config-dir=CONFIG_DIR
                        Pass in an alternative configuration directory.
                        Default: '/etc/salt'.
  -d, --daemon          Run the salt-api as a daemon.
  --pid-file=PIDFILE    Specify the location of the pidfile. Default:
                        '/var/run/salt-api.pid'.

  Logging Options:
    Logging options which override any settings defined on the
    configuration files.

    -l LOG_LEVEL, --log-level=LOG_LEVEL
                        Console logging log level. One of u'all', u'garbage',
                        u'trace', u'debug', u'profile', u'info', u'warning',
                        u'error', u'critical', u'quiet'. Default: 'warning'.
    --log-file=API_LOGFILE
                        Log file path. Default: '/var/log/salt/api'.
    --log-file-level=LOG_LEVEL_LOGFILE
                        Logfile logging log level. One of u'all', u'garbage',
                        u'trace', u'debug', u'profile', u'info', u'warning',
                        u'error', u'critical', u'quiet'. Default: 'warning'.

You can find additional help about salt-api issuing "man salt-api" or on
http://docs.saltstack.com</screen>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for <emphasis role="strong">salt-api</emphasis> are located in:</emphasis></para>
</formalpara>
<screen>/var/log/salt/master
/var/log/salt/api</screen>
</section>
<section xml:id="arch.component.salt.minion">
<title>salt-minion</title>
<formalpara>
<title>Functions</title>
<para>The <emphasis role="strong">salt-minion</emphasis> is a primary component of SUSE Manager. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Client-side main process for Salt clients (only pull method)</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Communicates the client with salt-master via Salt event bus (ZeroMQ)</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Executes the actions received from the Salt master on the client (minion)</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>salt-minion --help</title>
<para>The following options are available for the <emphasis role="strong">salt-minion</emphasis>. The following list is not comprehensive, for more information see: <link xl:href="http://docs.saltstack.com">The Saltstack Docs</link></para>
</formalpara>
<simpara><emphasis role="strong">Options:</emphasis></simpara>
<screen>salt-minion --help
Usage: salt-minion [options]

The Salt Minion, receives commands from a remote Salt Master

Options:
  --version             show program's version number and exit
  -V, --versions-report
                        Show program's dependencies version number and exit.
  -h, --help            show this help message and exit
  --saltfile=SALTFILE   Specify the path to a Saltfile. If not passed, one
                        will be searched for in the current working directory.
  -c CONFIG_DIR, --config-dir=CONFIG_DIR
                        Pass in an alternative configuration directory.
                        Default: '/etc/salt'.
  -u USER, --user=USER  Specify user to run salt-minion.
  -d, --daemon          Run the salt-minion as a daemon.
  --pid-file=PIDFILE    Specify the location of the pidfile. Default:
                        '/var/run/salt-minion.pid'.

  Logging Options:
    Logging options which override any settings defined on the
    configuration files.

    -l LOG_LEVEL, --log-level=LOG_LEVEL
                        Console logging log level. One of u'all', u'garbage',
                        u'trace', u'debug', u'profile', u'info', u'warning',
                        u'error', u'critical', u'quiet'. Default: 'warning'.
    --log-file=LOG_FILE
                        Log file path. Default: '/var/log/salt/minion'.
    --log-file-level=LOG_LEVEL_LOGFILE
                        Logfile logging log level. One of u'all', u'garbage',
                        u'trace', u'debug', u'profile', u'info', u'warning',
                        u'error', u'critical', u'quiet'. Default: 'warning'.

You can find additional help about salt-minion issuing "man salt-minion" or on
http://docs.saltstack.com</screen>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for <emphasis role="strong">salt-minion</emphasis> are located in:</emphasis></para>
</formalpara>
<screen>/var/log/salt/minion</screen>
</section>
<section xml:id="arch.component.salt.broker">
<title>salt-broker</title>
<formalpara>
<title>Functions</title>
<para>The <emphasis role="strong">salt-broker</emphasis> is a component of the SUSE Manager proxy. It performs the following functions in the stack.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Used only in the SUSE Manager Proxy for minions using pull method</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Forwards the ZeroMQ Salt channels from SUSE Manager server to the proxied minions</emphasis></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Log Files</title>
<para><emphasis>Logs for <emphasis role="strong">salt-broker</emphasis> are located in:</emphasis></para>
</formalpara>
<screen>/var/log/salt/broker</screen>
</section>
</chapter>
</part>
<part xml:id="book.mgr.install.guide">
<title>SUSE Manager Installation Guide</title>
<chapter xml:id="chapt.install.intro">
<title>Introduction</title>
<section xml:id="installation-overview">
<title>Overview</title>
<section xml:id="installation-about">
<title>About this book</title>
<simpara>This guide steps you through installing SUSE Manager.</simpara>
<simpara>The Requirements chapter covers the base system and networking requirements to be able to run SUSE Manager.</simpara>
<simpara>The First Install chapter covers a simple installation method for your first installation, or for use as a proof of concept.
In this chapter, you will instantiate a KVM virtual machine running JeOS 12.
This will serve as a sandbox for your SUSE Manager server.</simpara>
<simpara>The IBM zSystems chapter covers installing SUSE Manager on an IBM zSystems mainframe.
This method is recommended for z/VM administrators trained on zSystems operating protocols.</simpara>
</section>
</section>
<section xml:id="component-server">
<title>Server</title>
<variablelist>
<varlistentry>
<term>TODO</term>
<listitem>
<simpara>What is SUSE Manager Server?</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="component-proxy">
<title>Proxy</title>
<variablelist>
<varlistentry>
<term>TODO</term>
<listitem>
<simpara>What is SUSE Manager Proxy?</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="component-clients">
<title>Clients</title>
<variablelist>
<varlistentry>
<term>TODO</term>
<listitem>
<simpara>What are SUSE Manager Clients?</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="component-salt">
<title>Salt</title>
<variablelist>
<varlistentry>
<term>TODO</term>
<listitem>
<simpara>What is Salt?</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="component-db">
<title>Database</title>
<variablelist>
<varlistentry>
<term>TODO</term>
<listitem>
<simpara>What is a SUSE Manager database?</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</chapter>
<chapter xml:id="chapt.install.requirements">
<title>Requirements</title>
<section xml:id="installation-general-requirements.adoc">
<title>Prerequisites for Installation</title>
<simpara>Before you begin your installation, ensure you have fulfilled these prerequisites:</simpara>
<itemizedlist>
<listitem>
<simpara>Current SUSE Customer Center organization credentials</simpara>
</listitem>
<listitem>
<simpara>Access to installation media for your chosen operating system</simpara>
</listitem>
<listitem>
<simpara>Your environment meets the hardware and networking requirements</simpara>
</listitem>
<listitem>
<simpara>You understand the supported client operating systems</simpara>
</listitem>
</itemizedlist>
<simpara>This section contains more information on each of these prerequisites.</simpara>
<note>
<simpara>SUSE Manager 3.2 is based on SLES 12 SP3 as the host operating system.</simpara>
</note>
<section xml:id="quickstart.sect.prereq.scc">
<title>Obtaining your SUSE Customer Center Credentials</title>
<simpara>You will need to create an account with SUSE Customer Center before you install SUSE Linux Enterprise Server and SUSE Manager.
To obtain your SUSE Customer Center credentials:</simpara>
<orderedlist xml:id="creating.scc.account.mgr" numeration="arabic">
<title>Procedure: Obtaining Your SCC Organization Credentials</title>
<listitem>
<simpara>Open a browser and direct it to <link xl:href="https://scc.suse.com/login">https://scc.suse.com/login</link>.</simpara>
</listitem>
<listitem>
<simpara>If you have not done so, create an account now.</simpara>
</listitem>
<listitem>
<simpara>Log in to your new SCC account.</simpara>
</listitem>
<listitem>
<simpara>Under the <guibutton>Management tools</guibutton> widget select <guibutton>Manage Users</guibutton>.</simpara>
</listitem>
<listitem>
<simpara>Click the <guibutton>Organization Credentials</guibutton> tab.</simpara>
</listitem>
<listitem>
<simpara>Record your login information for use during SUSE Manager setup.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="quickstart.sect.prereq.installmedia">
<title>Obtaining Installation Media</title>
<simpara>This book describes installation methods for both JeOS and SUSE Linux Enterprise Server.
The JeOS image provides the quickest installation and setup, and is suitable for a test or proof of concept installation.
Alternatively, SUSE Linux Enterprise Server provides a more robust installation, which requires a larger initial download.
Choose your preferred operating system based on the type of environment you want to install, and the amount of bandwidth and time you have available.</simpara>
<simpara>You can find installation images for JeOS and SLES in your SUSE Customer Center account.
Log in, then navigate to the URL for your chosen operating system:</simpara>
<itemizedlist>
<listitem>
<simpara><link xl:href="https://www.suse.com/products/server/jeos/">JeOS - Media Download</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="https://www.suse.com/products/server/download/">SUSE Linux Enterprise Server - Media Download</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="install-hardware-requirements">
<title>Hardware Requirements</title>
<simpara>This table outlines hardware and software requirements on x86_64 and IBM Power PC architecture.
For installation on z Systems, see:</simpara>
<itemizedlist>
<listitem>
<simpara>xref:advanced_topics_suma3_zsystems.adoc#at-zsystems[{productname} and {zseries}]</simpara>
</listitem>
</itemizedlist>
<table frame="all" rowsep="1" colsep="1">
<title>Hardware Requirements for x86_64 Architecture</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Hardware</entry>
<entry align="left" valign="top">Recommended</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>CPU</simpara></entry>
<entry align="left" valign="top"><simpara>Minimum 4 dedicated 64-bit CPU cores</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RAM:</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis>Test Server</emphasis> Minimum 8&#160;GB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara><emphasis>Base Installation</emphasis> Minimum 16&#160;GB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara><emphasis>Production Server</emphasis> Minimum 32&#160;GB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Disk Space:</simpara></entry>
<entry align="left" valign="top"><simpara><literal role="path">/</literal> <emphasis>(root) The default JeOS root partition size of 24 GB is sufficient for this guide</emphasis></simpara></entry>
</row>
<row>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara><literal role="path">/var/lib/pgsql</literal> Minimum 50&#160;GB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara><literal role="path">/var/spacewalk</literal> Minimum 50&#160;GB per SUSE product and 250&#160;GB per Red Hat product</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title>Hardware Requirements for IBM POWER8 or POWER9 Architecture</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Hardware</entry>
<entry align="left" valign="top">Recommended</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>CPU</simpara></entry>
<entry align="left" valign="top"><simpara>Minimum 4 dedicated cores</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RAM:</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis>Test Server</emphasis> Minimum 8&#160;GB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara><emphasis>Base Installation</emphasis> Minimum 16&#160;GB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara><emphasis>Production Server</emphasis> Minimum 32&#160;GB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Disk Space:</simpara></entry>
<entry align="left" valign="top"><simpara><literal role="path">/</literal> Minimum 100&#160;GB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara><literal role="path">/var/lib/pgsql</literal> Minimum 50&#160;GB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara><literal role="path">/var/spacewalk</literal> Minimum 50&#160;GB per SUSE product and 250&#160;GB per Red Hat product</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="installation-network-requirements">
<title>Network Requirements</title>
<simpara>This section details the networking and port requirements for SUSE Manager.</simpara>
<variablelist>
<varlistentry>
<term>Fully Qualified Domain Name (FQDN)</term>
<listitem>
<simpara>The SUSE Manager server must resolve its FQDN correctly or cookies will not work properly on the WebUI.</simpara>
<simpara>For more information about configuring the hostname and DNS, see <link xl:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_basicnet_yast.html#sec_basicnet_yast_change_host">SUSE Linux Enterprise Server Documentation - Configuring Host Name and DNS</link></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Hostname and IP Address</term>
<listitem>
<simpara>To ensure that the SUSE Manager domain name can be resolved by its clients, both server and client machines must be connected to a working DNS server.</simpara>
<simpara>For more information about setting up a DNS server, see <link xl:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_dns.html">SUSE Linux Enterprise Server Documentation - The Domain Name System</link></simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Using a Proxy When Installing from SUSE Linux Enterprise Media</term>
<listitem>
<simpara>If you are on an internal network and do not have access to SUSE Customer Center, you can set up and use a proxy during installation.</simpara>
<simpara>For more information about configuring a proxy for access to SUSE Customer Center during a SUSE Linux Enterprise installation, see <link xl:href="https://www.suse.com/documentation/sled-12/singlehtml/book_sle_deployment/book_sle_deployment.html#sec.i.yast2.start.parameters.proxy">SUSE Linux Enterprise Server Documentation -  Using a Proxy During Installation</link></simpara>
</listitem>
</varlistentry>
</variablelist>
<important>
<title>Naming Your Server</title>
<simpara>The hostname of SUSE Manager must not contain uppercase letters as this may cause <emphasis>jabberd</emphasis> to fail.
Choose the hostname of your SUSE Manager server carefully.
Although changing the server name is possible, it is a complex process and unsupported.</simpara>
</important>
<simpara>In a production environment, SUSE Manager server and its clients should always use a firewall.
This table gives an overview of required ports, to be used when you are setting up your firewall rules.</simpara>
<table xml:id="tab.install.ports.server1" frame="all" rowsep="1" colsep="1">
<title>Required Server Ports</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Port</entry>
<entry align="left" valign="top">Protocol</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>22</simpara></entry>
<entry align="left" valign="top"><simpara>TCP</simpara></entry>
<entry align="left" valign="top"><simpara>SSH</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>67</simpara></entry>
<entry align="left" valign="top"><simpara>UDP</simpara></entry>
<entry align="left" valign="top"><simpara>DHCP</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>69</simpara></entry>
<entry align="left" valign="top"><simpara>UDP</simpara></entry>
<entry align="left" valign="top"><simpara>TFTP, used to support PXE services</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>80</simpara></entry>
<entry align="left" valign="top"><simpara>TCP</simpara></entry>
<entry align="left" valign="top"><simpara>HTTP, used in some bootstrap cases</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>123</simpara></entry>
<entry align="left" valign="top"><simpara>UDP</simpara></entry>
<entry align="left" valign="top"><simpara>NTP time service</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>443</simpara></entry>
<entry align="left" valign="top"><simpara>TCP</simpara></entry>
<entry align="left" valign="top"><simpara>HTTPS, used for Web UI, client, Proxy server, and API traffic</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>4505</simpara></entry>
<entry align="left" valign="top"><simpara>TCP</simpara></entry>
<entry align="left" valign="top"><simpara>Salt, used by the Salt-master to accept communication requests from minions</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>4506</simpara></entry>
<entry align="left" valign="top"><simpara>TCP</simpara></entry>
<entry align="left" valign="top"><simpara>Salt, used by the Salt-master to accept communication requests from minions</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>5222</simpara></entry>
<entry align="left" valign="top"><simpara>TCP</simpara></entry>
<entry align="left" valign="top"><simpara>XMPP client, used for communications with the <literal role="systemitem">osad</literal> daemon on traditional client systems</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>5269</simpara></entry>
<entry align="left" valign="top"><simpara>TCP</simpara></entry>
<entry align="left" valign="top"><simpara>XMPP server, used for pushing actions to SUSE Manager Proxy</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>For more information on disconnected setup and port configuration, see:</simpara>
<itemizedlist>
<listitem>
<simpara>xref:bp_chap_choosing_dist_scheme.adoc#bp-dist-scheme[Disconnected Setup]</simpara>
</listitem>
<listitem>
<simpara>xref:advanced_topics_ports.adoc#at-ports[Firewall Ports]</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="installation-client-requirements">
<title>Supported Client Systems</title>
<simpara>Supported operating systems for traditional and Salt clients are listed in this table.</simpara>
<table xml:id="mgr.supported.clients" frame="all" rowsep="1" colsep="1">
<title>Supported Client Systems</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Operating Systems</entry>
<entry align="left" valign="top">Architecture</entry>
<entry align="left" valign="top">Traditional Clients</entry>
<entry align="left" valign="top">Salt Clients</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>SUSE Linux Enterprise 11 SP4</simpara></entry>
<entry align="left" valign="top"><simpara>x86, x86_64, Itanium, IBM POWER, z Systems</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>SUSE Linux Enterprise 12 SP3, 12 SP4</simpara></entry>
<entry align="left" valign="top"><simpara>x86_64, IBM POWER (IBM Power PC), z Systems, ARM</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>SUSE Linux Enterprise 15</simpara></entry>
<entry align="left" valign="top"><simpara>x86_64, IBM POWER (IBM Power PC), z Systems, ARM</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis>Latest minor release Red Hat Enterprise Linux Server 6</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>x86, x86_64</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis>Latest minor release Red Hat Enterprise Linux Server 7</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>x86_64</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Open Enterprise Server 2015, 2015 SP1, 2018</simpara></entry>
<entry align="left" valign="top"><simpara>x86_64</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<title>Supported Versions and SP Levels</title>
<simpara>Client operating system versions and SP levels must be under general support (normal or LTSS) to be supported with SUSE Manager.
For details on supported product versions, see <link xl:href="https://www.suse.com/lifecycle">https://www.suse.com/lifecycle</link>.</simpara>
</note>
</section>
</chapter>
<chapter xml:id="chapt.install.first.install">
<title>First Install</title>
<section xml:id="install-vm">
<title>Installing the virtual machine environment</title>
<section xml:id="quickstart.sect.kvm.settings">
<title>Virtual Machine Manager (virt-manager) Settings</title>
<simpara>This chapter provides the required (KVM) settings for installation of SUSE Linux Enterprise Just Enough Operating System (JeOS) 12 as the base for SUSE Manager.
A kernel virtual machine (KVM) combined with Virtual Machine Manager (<emphasis>virt-manager</emphasis>) will be used as a sandbox for your first installation.</simpara>
<tip>
<title>SUSEVirtualization Guide</title>
<simpara>For more information on virtualization, see: <link xl:href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html">SUSE Linux Enterprise Virtualization Guide</link></simpara>
</tip>
<simpara>Enter the following settings when creating a new virtual machine using <emphasis role="strong">virt-manager</emphasis>.
In the following table replace <emphasis>version</emphasis> with the actual product version string.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top" namest="col_1" nameend="col_2">KVM Settings</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Installation Method</simpara></entry>
<entry align="left" valign="top"><simpara>Import Existing Disk Image</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>OS:</simpara></entry>
<entry align="left" valign="top"><simpara>Linux</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Version:</simpara></entry>
<entry align="left" valign="top"><simpara>SLES_version_-JeOS-for-kvm-and-xen.x86_64-GM.qcow2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Memory:</simpara></entry>
<entry align="left" valign="top"><simpara>4096 MB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>CPU&#8217;s:</simpara></entry>
<entry align="left" valign="top"><simpara>2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Storage Format:</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis>.qcow2</emphasis> 24 GB (Default) JeOS Root Partition</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Virtual Disks:</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>VirtIO Disk 2</simpara></entry>
<entry align="left" valign="top"><simpara>101 GB for <literal role="path">/var/spacewalk</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>VirtIO Disk 3</simpara></entry>
<entry align="left" valign="top"><simpara>50 GB for <literal role="path">/var/lib/pgsql</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>VirtIO Disk 4</simpara></entry>
<entry align="left" valign="top"><simpara>4 GB for swap</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Name:</simpara></entry>
<entry align="left" valign="top"><simpara>test-setup</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Network</simpara></entry>
<entry align="left" valign="top"><simpara>Bridge <emphasis>br0</emphasis></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<tip>
<title>SUSE Virtualization Guide</title>
<simpara>For more information on virtualization, see: <link xl:href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html">SUSE Linux Enterprise Virtualization Guide</link></simpara>
</tip>
</section>
<section xml:id="jeos.kvm.settings">
<title>JeOS KVM Settings</title>
<simpara>Create three additional virtual disks required for the SUSE Manager storage partitions.</simpara>
<orderedlist numeration="arabic">
<title>Procedure: Creating the Required Partitions with KVM</title>
<listitem>
<simpara>Create a new virtual machine using the downloaded JeOS KVM image and select <guimenu>Import existing disk image</guimenu> .</simpara>
</listitem>
<listitem>
<simpara>Configure RAM and number of CPUs (At least 4 GB RAM and 2 CPUs).</simpara>
</listitem>
<listitem>
<simpara>Name your KVM machine and select the <guimenu>Customize configuration before install</guimenu> check box.</simpara>
</listitem>
<listitem>
<simpara>Select the <guibutton>Add Hardware</guibutton> button and create three new virtual disks with the following specifications.
These disks will be partitioned and mounted in <xref linkend="proc.jeos.susemgr.prep"/>.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">VirtIO Storage Disks</entry>
<entry align="left" valign="top">Name</entry>
<entry align="left" valign="top">Sizing</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>VirtIO Disk 2</simpara></entry>
<entry align="left" valign="top"><simpara>spacewalk</simpara></entry>
<entry align="left" valign="top"><simpara>101&#160;GB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>VirtIO Disk 3</simpara></entry>
<entry align="left" valign="top"><simpara>pgsql</simpara></entry>
<entry align="left" valign="top"><simpara>50&#160;GB</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>VirtIO Disk 4</simpara></entry>
<entry align="left" valign="top"><simpara>swap</simpara></entry>
<entry align="left" valign="top"><simpara>4&#160;GB</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
<listitem>
<simpara>Click <guimenu>Begin Installation</guimenu> and your new VM will boot from the JeOS image.</simpara>
</listitem>
</orderedlist>
<simpara>Proceed through the basic JeOS installation prompts until you reach the command line.</simpara>
<tip>
<title>Root Password</title>
<simpara>During the basic installation prompts you are asked to enter the root password.
Select a strong password and then in the next message box <guibutton>Confirm root Password</guibutton>.</simpara>
</tip>
</section>
<section xml:id="jeos.susemgr.prep">
<title>Preparing JeOS for SUSE Manager</title>
<orderedlist xml:id="proc.jeos.susemgr.prep" numeration="arabic">
<title>Procedure: Preparing JeOS for SUSE Manager Installation</title>
<listitem>
<simpara>Register with SCC:</simpara>
<screen>SUSEConnect -e&lt;EMAIL_ADDRESS&gt; -r&lt;SUSE_MANAGER_CODE&gt;</screen>
</listitem>
<listitem>
<simpara>Add SUSE Manager repositories:</simpara>
<screen>SUSEConnect -p SUSE-Manager-Server/&lt;productnumber&gt;/x86_64 -r&lt;SUSE_MANAGER_CODE&gt;</screen>
</listitem>
<listitem>
<simpara>Install <phrase role="package">yast2-storage</phrase> with all required dependencies (approx. 40 packages, 30 MB when installed).
This basic administration package is required for preparing storage partitions:</simpara>
<screen>zypper in -t package yast2-storage</screen>
</listitem>
<listitem>
<simpara>Partition and mount the virtual disks at the following locations using YaST Partitioner (<literal role="command">yast2 disk</literal>).</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">VirtIO Storage Disks</entry>
<entry align="left" valign="top">Name</entry>
<entry align="left" valign="top">Storage Size</entry>
<entry align="left" valign="top">File System Type</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>VirtIO Disk 2</simpara></entry>
<entry align="left" valign="top"><simpara><literal role="path">/var/spacewalk</literal></simpara></entry>
<entry align="left" valign="top"><simpara>101&#160;GB</simpara></entry>
<entry align="left" valign="top"><simpara>XFS</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>VirtIO Disk 3</simpara></entry>
<entry align="left" valign="top"><simpara><literal role="path">/var/lib/pgsql</literal></simpara></entry>
<entry align="left" valign="top"><simpara>50&#160;GB</simpara></entry>
<entry align="left" valign="top"><simpara>XFS</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>VirtIO Disk 4</simpara></entry>
<entry align="left" valign="top"><simpara><literal role="path">swap</literal></simpara></entry>
<entry align="left" valign="top"><simpara>4&#160;GB</simpara></entry>
<entry align="left" valign="top"><simpara>swap</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
<listitem>
<simpara>If you are still using an older version than SUSE Manager&#160;3.2 check <literal role="path">/etc/fstab</literal> for correctness as follows (<emphasis>updated tools shipped with recent SPs will no longer require human intervention.</emphasis>):
Remove or comment out this mount point entry for <literal role="path">/var/lib/pgsql/</literal> in the <literal role="path">/etc/fstab</literal> file:</simpara>
<screen>/var/lib/pgsql btrfs subvol=@/var/lib/pgsql 0 0</screen>
<warning>
<title>Remove <literal role="path">pgsql</literal> from the fstab Configuration File</title>
<simpara>If you do not remove the <literal role="path">/var/lib/pgsql/</literal> line from fstab the first time you shut down the server you will lose your database because of duplicated entries in the fstab file.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Exit the partitioner and install the SUSE Manager pattern:</simpara>
<screen>zypper in -t pattern suma_server</screen>
</listitem>
</orderedlist>
<simpara>For proceeding with SUSE Manager setup, see <link xl:href="quickstart3_chap_sumasetup_with_yast.xml#suma.setup.with.yast">SUSE Manager Setup</link>.</simpara>
</section>
</section>
<section xml:id="install-server">
<title>Installing SUSE Manager Server on x86</title>
<simpara>This chapter provides the required KVM settings for installation of SUSE Linux Enterprise Server media as the base for SUSE Manager.
A kernel virtual machine KVM combined with Virtual Machine Manager (<literal role="command">virt-manager</literal>) will be used as a sandbox for this installation.</simpara>
<section xml:id="_sles_kvm_requirements">
<title>SLES KVM Requirements</title>
<simpara>Enter the following settings when creating a new virtual machine using <literal role="command">virt-manager</literal> (replace <literal role="replaceable">version</literal> with the actual version string):</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">KVM Settings for SLES</entry>
<entry align="left" valign="top">Installation Method:</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Local install media (ISO image or CDROM)</simpara></entry>
<entry align="left" valign="top"><simpara>OS:</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Linux</simpara></entry>
<entry align="left" valign="top"><simpara>Version:</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>SLE-[replaceable]</literal>version<literal>-Server-x86_64-GM-DVD1.iso</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Memory:</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>4096&#160;MB</simpara></entry>
<entry align="left" valign="top"><simpara>CPUs:</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>2</simpara></entry>
<entry align="left" valign="top"><simpara>Storage Format:</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>ISO 3&#160;GB</simpara></entry>
<entry align="left" valign="top"><simpara>Disk Space:</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>234&#160;GB split between 4&#160;GB swap and 130&#160;GB mounted at <literal role="path">/var/spacewalk/</literal></simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>(Virtual Disk 1) and 50&#160;GB mounted at <literal role="path">/var/lib/pgsql</literal></simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>(Virtual Disk 2).  The rest for the root partition (100&#160;GB+).</simpara></entry>
<entry align="left" valign="top"><simpara>Name:</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>example-server</simpara></entry>
<entry align="left" valign="top"><simpara>Network</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<section xml:id="sles.installation.within.kvm.media">
<title>SLES KVM Settings</title>
<simpara>This section provides guidance on installation of SUSE Manager
utilizing the full installation media with KVM and <literal role="command">virt-manager</literal>.
This section assumes you have previously setup an account with SCC and downloaded the SLES full installation media.</simpara>
<orderedlist numeration="arabic">
<title>Procedure: Preparing for SLES Installation</title>
<listitem>
<simpara>In <literal role="command">virt-manager</literal> select <menuchoice><guimenu>File</guimenu> <guimenuitem>New Virtual Machine</guimenuitem></menuchoice>.</simpara>
</listitem>
<listitem>
<simpara>Select <guibutton>Local install media (ISO image or CDROM)</guibutton>.</simpara>
</listitem>
<listitem>
<simpara>Ensure <guibutton>Use ISO Image</guibutton> is selected then click <guibutton>Browse</guibutton> and locate the full SLES image you downloaded from your SCC account.</simpara>
</listitem>
<listitem>
<simpara>Configure your machine with at least 4096 MB RAM and a minimum of 2 CPUs.</simpara>
</listitem>
<listitem>
<simpara>Create a storage device with a minimum of 234 GB storage space for the installation.
During the partitioning setup of the SLES installation this disk should be partitioned into the following disks:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="1">
<colspec colname="col_1" colwidth="100*"/>
<thead>
<row>
<entry align="left" valign="top">Disk Space Requirements</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>4&#160;GB Swap space</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>130&#160;GB XFS partition (or dedicated virtual disk) for <literal role="path">/var/spacewalk/</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>50&#160;GB XFS partition (or dedicated virtual disk) for <literal role="path">/var/lib/pgsql/</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
<listitem>
<simpara>The remaining storage space will be used by the operating system for the root partition.
Select <guibutton>Finish</guibutton> to begin the installation.</simpara>
</listitem>
</orderedlist>
<simpara>Installation of SUSE Linux Enterprise Server will begin.
For more information on completing an installation of SUSE Linux Enterprise Server, see: <link xl:href="https://www.suse.com/documentation/sles-12/book_quickstarts/data/sec_sle_installquick.html">SUSE Linux Enterprise Installation Quickstart</link>.</simpara>
</section>
</section>
<section xml:id="quickstart3.sec.suma.installation.sles.sumaext">
<title>Selecting the SUSE Manager Extension</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>During SUSE Linux Enterprise Server installation, you will be presented with the <literal role="systemitem">Extension and Module Selection</literal> screen.</simpara>
<note>
<simpara>This screen will not be shown if you have skipped the registration step at the beginning of the installation process.
Ensure you have registered with SUSE and logged in.</simpara>
</note>
</listitem>
<listitem>
<simpara>Select the SUSE Manager Extension and then click the <guibutton>Next</guibutton> button.</simpara>
</listitem>
<listitem>
<simpara>Complete the SUSE Linux Enterprise Server installation.</simpara>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="manager-extension.png" width="80%"/>
</imageobject>
<textobject><phrase>manager extension</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
<section xml:id="installation-server-setup">
<title>SUSE Manager Setup</title>
<section xml:id="suma.setup.with.yast.setup">
<title>Topics</title>
<simpara>This section covers SUSE Manager setup.
You will perform the following procedures:</simpara>
<itemizedlist>
<listitem>
<simpara>Start SUSE Manager setup via YaST or command line</simpara>
</listitem>
<listitem>
<simpara>Create the main administration account with the SUSE Manager Web UI</simpara>
</listitem>
<listitem>
<simpara>Name your base organization and add login credentials</simpara>
</listitem>
<listitem>
<simpara>Sync the SUSE Linux Enterprise product channel from SUSE Customer Center</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="suma-setup-with-yast-sumasetup">
<title>SUSE Manager Setup</title>
<warning>
<title>Third Party Software</title>
<simpara>SUSE Manager is an extension of SUSE Linux Enterprise Server and compatible with the software shipped with SUSE Linux Enterprise Server.</simpara>
<simpara>SUSE Manager is a complex system, and therefore installing third party is not allowed. Installing monitoring software provided by a third party vendor is allowed only if you do not exchange basic libraries such as SSL, cryptographic software, and similar tools.
In case of emergency, SUSE reserves the right to ask to remove any third party software (and associated configuration changes) and then to reproduce the problem on a clean system.</simpara>
</warning>
<simpara>This section will guide you through SUSE Manager setup procedures.</simpara>
<orderedlist numeration="arabic">
<title>Procedure: SUSE Manager Setup</title>
<listitem>
<simpara>Login to the SUSE Manager server desktop and perform one of the following actions to begin setup:</simpara>
<itemizedlist>
<listitem>
<simpara>Select <menuchoice><guimenu>Applications</guimenu> <guisubmenu>System Tools</guisubmenu> <guisubmenu>YaST</guisubmenu> <guimenuitem>SUSE Manager Setup</guimenuitem></menuchoice>.</simpara>
</listitem>
<listitem>
<simpara>Open a terminal as root and type <literal role="command">yast2 susemanager_setup</literal> to begin setup.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>From the introduction screen select <menuchoice><guimenu>SUSE Manager Setup</guimenu> <guimenuitem>Setup SUSE Manager from scratch</guimenuitem></menuchoice> and click <guibutton>Next</guibutton> to continue.</simpara>
</listitem>
<listitem>
<simpara>Enter an email address to receive status notifications and click <guibutton>Next</guibutton> to continue.
Note that SUSE Manager can sometimes send a large volume of notification emails.
You can disable email notifications in the Web UI after setup, if you need to.</simpara>
</listitem>
<listitem>
<simpara>Enter your certificate information and a password.
Passwords must be at at least seven characters in length, and must not contain spaces, single or double quotation marks (<literal>'</literal> or <literal>"</literal>), exclamation marks (<literal>!</literal>), or dollar signs (<literal>$</literal>).
Always store your passwords in a secure location.</simpara>
<important>
<title>Certificate Password</title>
<simpara>Without this password it will not be possible to set up a SUSE Manager Proxy Server.</simpara>
</important>
</listitem>
<listitem>
<simpara>Click <guibutton>Next</guibutton> to continue.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="quickstart-mgr-setup4.png" width="80%"/>
</imageobject>
<textobject><phrase>quickstart mgr setup4</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>From the <menuchoice><guimenu>SUSE Manager Setup</guimenu> <guimenuitem>Database Settings</guimenuitem></menuchoice> screen, enter a database user and password and click <guibutton>Next</guibutton> to continue.
Passwords must be at at least seven characters in length, and must not contain spaces, single or double quotation marks (<literal>'</literal> or <literal>"</literal>), exclamation marks (<literal>!</literal>), or dollar signs (<literal>$</literal>).
Always store your passwords in a secure location.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="quickstart-mgr-setup5.png" width="80%"/>
</imageobject>
<textobject><phrase>quickstart mgr setup5</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>Enter your SUSE Customer Center <literal role="systemitem">Organization Credentials</literal>.
Open <link xl:href="https://scc.suse.com/login">https://scc.suse.com/login</link> to register or access to your organization credentials.</simpara>
<note>
<title><guibutton>Skip</guibutton></title>
<simpara>If you are using SUSE Enterprise products, SUSE Manager requires that you connect to SUSE Customer Center for software, updates and patches.
You will not be able to synchronize or provide Enterprise channels to your clients without this information.</simpara>
<simpara>However if you would like to work with open source software channels and repositories then click the <guibutton>Skip</guibutton> button to continue.
You can setup your SUSE Customer Center credentials or configure inter-server sync at a later time.</simpara>
</note>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="quickstart-mgr-setup9.png" width="80%"/>
</imageobject>
<textobject><phrase>quickstart mgr setup9</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>Click <guibutton>Next</guibutton> to continue.</simpara>
</listitem>
<listitem>
<simpara>Click <guibutton>Yes</guibutton> to run setup when prompted.</simpara>
</listitem>
<listitem>
<simpara>Once setup has completed, click <guibutton>Next</guibutton> to continue.
You will see the address of the SUSE Manager Web UI.</simpara>
</listitem>
<listitem>
<simpara>Click <guibutton>Finish</guibutton> to complete SUSE Manager setup.</simpara>
</listitem>
</orderedlist>
<simpara>In the next section you will create the administrator&#8217;s account and synchronize with SUSE Customer Center.</simpara>
<section xml:id="suma.setup.with.yast.admin">
<title>Creating the Main Administration Account</title>
<simpara>This section will walk you through creating your organizations main administration account for SUSE Manager.</simpara>
<warning>
<title>Admin and User Accounts</title>
<simpara>The main administration account is the <emphasis>highest authority account</emphasis> within SUSE Manager and therefore account access information should be stored in a secure location.</simpara>
<simpara>For security it is recommended that the main administrator creates <emphasis>low level admin accounts</emphasis> designated for administration of organizations and individual groups.</simpara>
</warning>
<orderedlist xml:id="suma.setup.admin.account" numeration="arabic">
<title>Procedure: Setup the Main Administration Account</title>
<listitem>
<simpara>In the browser, enter the address provided after completing setup and open the SUSE Manager Web UI.</simpara>
</listitem>
<listitem>
<simpara>Add your organization name to the <menuchoice><guimenu>Create Organization</guimenu> <guimenuitem>Organization Name</guimenuitem></menuchoice> field.</simpara>
</listitem>
<listitem>
<simpara>Add your username and password to the <menuchoice><guimenu>Create Organization</guimenu> <guimenuitem>Desired Login</guimenuitem></menuchoice> and <menuchoice><guimenu>Create Organization</guimenu> <guimenuitem>Desired Password</guimenuitem></menuchoice> fields.</simpara>
</listitem>
<listitem>
<simpara>Fill in the Account Information fields including an email for system notifications.</simpara>
</listitem>
<listitem>
<simpara>Select <guimenu>Create Organization</guimenu> to finish creating your administration account.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="quickstart-mgr-setup-admin1.png" width="80%"/>
</imageobject>
<textobject><phrase>quickstart mgr setup admin1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
<simpara>You should now be presented with the SUSE Manager Front Page. In the next section you will prepare the server for connecting the first client.</simpara>
</section>
<section xml:id="quickstart.first.channel.sync">
<title>Syncing Products from SUSE Customer Center</title>
<simpara>SUSE Customer Center (SCC) maintains a collection of repositories which contain packages, software and updates for all supported enterprise client systems.
These repositories are organized into channels each of which provide software specific to a distribution, release and architecture.
After synchronizing with SCC clients may receive updates, and be organized into groups and assigned to specific product software channels.</simpara>
<simpara>This section covers synchronizing with SCC from the Web UI and adding your first client channel.</simpara>
<orderedlist xml:id="proc-quickstart-first-channel-sync" numeration="arabic">
<title>Procedure: Synchronizing with SUSE Customer Center</title>
<listitem>
<simpara>From the SUSE Manager Web UI start page select <menuchoice><guimenu>Admin</guimenu> <guimenuitem>Setup Wizard</guimenuitem></menuchoice>.</simpara>
</listitem>
<listitem>
<simpara>From the <menuchoice><guimenu>Main Menu</guimenu> <guisubmenu>Admin</guisubmenu> <guimenuitem>Setup Wizard</guimenuitem></menuchoice> page select the <guibutton>SUSE Products</guibutton> tab.
Wait a moment for the products list to populate.
If you previously registered with SUSE Customer Center a list of products will populate the table.
This table lists architecture, channels, and status information.
For more information, see:</simpara>
</listitem>
</orderedlist>
<simpara>xref:FILENAME.adoc#vle.webui.admin.wizard.products[]</simpara>
<simpara>+</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="admin_suse_products.png" width="80%"/>
</imageobject>
<textobject><phrase>admin suse products</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>+</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Since Your SUSE Linux Enterprise client is based on <literal role="systemitem">x86_64</literal> architecture scroll down the page and select the check box for this channel now.</simpara>
<itemizedlist>
<listitem>
<simpara>Add channels to SUSE Manager by selecting the check box to the left of each channel.
Click the arrow symbol to the left of the description to unfold a product and list available modules.</simpara>
</listitem>
<listitem>
<simpara>Start product synchronization by clicking the <guibutton>Add Products</guibutton> button.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<simpara>After adding the channel SUSE Manager will schedule the channel to be copied.
This can take a long time as SUSE Manager will copy channel software sources from the SUSE repositories located at SUSE Customer Center to local <literal role="path">/var/spacewalk/</literal> directory of your server.</simpara>
<tip>
<title>PostgreSQL and Transparant Huge Pages</title>
<simpara>In some environments, <emphasis>Transparent Huge Pages</emphasis> provided by the kernel may slow down PostgreSQL workloads significantly.</simpara>
<simpara>To disable <emphasis>Transparant Huge Pages</emphasis> set the <literal role="option">transparent_hugepage</literal> kernel parameter to <literal role="option">never</literal>.
This has to be changed in <literal role="path">/etc/default/grub</literal> and added to the line <literal role="option">GRUB_CMDLINE_LINUX_DEFAULT</literal>, for example:</simpara>
<screen>GRUB_CMDLINE_LINUX_DEFAULT="resume=/dev/sda1 splash=silent quiet showopts elevator=noop transparent_hugepage=never"</screen>
<simpara>To write the new configuration run <literal role="command">grub2-mkconfig -o /boot/grub2/grub.cfg</literal>.
To update the grub2 during boot run <literal role="command">grub2-install /dev/sda</literal>.</simpara>
</tip>
<simpara>Monitor channel synchronization process in real-time by viewing channel log files located in the directory <literal role="path">/var/log/rhn/reposync</literal>:</simpara>
<screen>tailf /var/log/rhn/reposync/&lt;CHANNEL_NAME&gt;.log</screen>
<simpara>After the channel sync process has completed proceed to:</simpara>
<literallayout class="monospaced">pass:c[xref:FILENAME.adoc#preparing.and.registering.clients[]]</literallayout>
</section>
</section>
</section>
</chapter>
<chapter xml:id="chapt.install.zsystems">
<title>IBM z Systems</title>
<section xml:id="installation-zsystems">
<title>SUSE Manager on IBM z Systems</title>
<section xml:id="_introduction">
<title>Introduction</title>
<simpara>This best practice guide is intended for z/VM administrators responsible for operating the IBM z Systems
Mainframe.
The goal of this guide is to lead an z/VM administrator trained on normal z Systems
operating protocols through the installation of SUSE Manager
onto an existing mainframe system.
The intent of this article is not to cover the variety of hardware configuration profiles available on z Systems
but instead to provide a foundational overview of the procedure and requirements necessary for a successful SUSE Manager
server deployment.</simpara>
</section>
<section xml:id="_base_system_requirements">
<title>Base System Requirements</title>
<simpara>The z/VM administrator should acquire and prepare the following resources for a successful SUSE Manager
installation. SUSE Manager3.2
is delivered as an extension.
These sections will provide you with the minimum and recommended system requirements for SUSE Manager
.
The base system for SUSE Manager
 3.2
is SLES 12SP3
.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Hardware</entry>
<entry align="left" valign="top">Recommended Hardware</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>IBM Systems</simpara></entry>
<entry align="left" valign="top"><simpara>* IBM zEnterprise System z196 <emphasis>(z196)</emphasis>
* IBM zEnterprise System z114 <emphasis>(z114)</emphasis>
* IBM zEnterprise EC12 <emphasis>(zEC12)</emphasis>
* IBM zEnterprise EC12 <emphasis>(zEC12)</emphasis>
* IBM zEnterprise BC12 <emphasis>(zBC12)</emphasis>
* IBM z13 <emphasis>(z13)</emphasis>
* LinuxOne Rockhopper
* LinuxOne Emperor</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RAM</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis>Split memory requirements across available RAM, VDISK and
        swap to suit your environment. On a production system the ratio of
        physical memory to VDISK will need to be re-evaluated based on the
        number of clients which will be supported.</emphasis></simpara><simpara>Minimum 5&#160;
 GB+ for test server <emphasis>(3 GB RAM + 2 GB VDISK
        Swap)</emphasis></simpara><simpara>Minimum 16&#160;
GB+ for base installation</simpara><simpara>Minimum 32&#160;
GB+ for a production server</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Free Disk Space</simpara></entry>
<entry align="left" valign="top"><simpara>Minimum 100&#160;
GB for root partition</simpara><simpara>Minimum 50&#160;
GB for <literal role="replaceable">/var/lib/pgsql</literal></simpara><simpara>Minimum 50&#160;
GB per SUSE
product + 100 GB per Red Hat product <literal role="replaceable">/var/spacewalk</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Network Connection</simpara></entry>
<entry align="left" valign="top"><simpara>* OSA Express Ethernet <emphasis>(including Fast and Gigabit Ethernet) _
* HiperSockets or Guest LAN
* 10 GBE, VSWITCH
* RoCE _(RDMA over Converged Ethernet)</emphasis></simpara><simpara>The following interfaces are still included but no longer supported:</simpara><simpara>* CTC <emphasis>(or virtual CTC)</emphasis>
* IP network interface for IUCV</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<itemizedlist>
<title>Media Requirements</title>
<listitem>
<simpara>SUSE Linux Enterprise 12SP3 Installation Media for IBM z Systems :</simpara>
<simpara><link xl:href="https://www.suse.com/products/server/download/">https://www.suse.com/products/server/download/</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_additional_requirements">
<title>Additional Requirements</title>
<simpara>There are a few additional resource requirements you will need to prepare before installing the SUSE Manager extension on your system.
This section overviews these requirements.</simpara>
<simpara>The guest z/VM should be provided with a static IP address and hostname as these cannot be easily changed after initial setup.
The hostname should contain less than 8 characters.</simpara>
<simpara>For more information on SUSE Manager additional requirements, see <link xl:href="https://www.suse.com/documentation/suse-manager-3/book_suma_best_practices/data/mgr_conceptual_overview.html">https://www.suse.com/documentation/suse-manager-3/book_suma_best_practices/data/mgr_conceptual_overview.html</link>.</simpara>
<simpara>You will need to ensure you have sufficient disk storage for SUSE Manager
before running <literal role="command">yast2 susemanagersetup</literal>.</simpara>
<simpara>This section explains these requirements in more detail.</simpara>
<warning>
<title>SUSE ManagerDefault Volume Groups and Disk Space</title>
<simpara>By default the file system of SUSE Manager, including the embedded database and patch directories, reside within the root volume.
While adjustments are possible once installation is complete, it is the administrator&#8217;s responsibility to specify and monitor these adjustments.</simpara>
<simpara>If your SUSE Manager runs out of disk space, this can have a severe impact on its database and file structure.
Preparing storage requirements in accordance with this section will aid in preventing these harmful effects.
SUSE technical services will not be able to provide support for systems suffering from low disk space conditions as this can have an effect on an entire system and therefore becomes unresolvable.
A full recovery is only possible with a previous backup or a new SUSE Manager installation.</simpara>
</warning>
<formalpara>
<title>Required Storage Devices</title>
<para>An additional disk is required for database storage.
This should be an <literal role="systemitem">zFCP</literal> or <literal role="systemitem">DASD</literal> device as these are preferred for use with <literal role="systemitem">HYPERPAV</literal>.
The disk should fulfill the following requirements:</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>At least 50&#160;GB for <literal role="path">/var/lib/pgsql</literal></simpara>
</listitem>
<listitem>
<simpara>At least 50&#160;GB for each SUSE product in <literal role="path">/var/spacewalk</literal></simpara>
</listitem>
<listitem>
<simpara>At least 100&#160;GB for each Red Hat product in <literal role="path">/var/spacewalk</literal></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Reclaiming Disk Space</title>
<para>If you need to reclaim more disk space, try these suggestions:</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>Remove custom channels (you cannot remove official SUSE channels)</simpara>
</listitem>
<listitem>
<simpara>Use the <literal role="command">spacewalk-data-fsck --help</literal> command to compare the spacewalk database to the filesystem and remove entries if either is missing.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_sles_12sp3_installation_and_the_suse_manager_extension">
<title>SLES 12SP3 Installation and the SUSE Manager Extension</title>
<simpara>This section covers the installation of SUSE Manager3.2
as an extension to SLES 12SP3
.</simpara>
<simpara>For more information on deploying SLES 12SP3
on your hardware, see <link xl:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_zseries.html">https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_zseries.html</link>.</simpara>
<simpara>During installation of SLES 12SP3
select SUSE Manager
as an extension.</simpara>
<simpara>After rebooting you will need to set up the additional storage required for <literal role="path">/var/spacewalk</literal> and <literal role="path">/var/lib/pgsql</literal> and swap space using the yast partitioner tool.
This step is <emphasis>required</emphasis> before running <literal role="command">yast2 susemanagersetup</literal>.</simpara>
<simpara>After configuring the storage requirements, having executed a YaST
online update and completed a full system reboot, run SUSE Manager
setup to finalize the SUSE Manager
installation on your z Systems
mainframe:</simpara>
<screen>{prompt.root}yast2 susemanagersetup</screen>
<simpara>This completes the installation of SUSE Manager
on your z Systems
.
For more information on beginning management with SUSE Manager
, see <link xl:href="suma-setup-with-yast-sumasetup.xml#suma-setup-with-yast-sumasetup">Setup SUSE Manager with YaST</link>.</simpara>
</section>
</section>
</chapter>
<chapter xml:id="chapt.install.proxy">
<title>Install Proxy</title>
<section xml:id="installation-proxy">
<title>SUSE Manager 3.2 Proxy</title>
<simpara>This chapter explains how to install and set up SUSE Manager 3.2 Proxy.
It also provides notes about migrating a previous proxy to version 3.2.</simpara>
<section xml:id="at.manager.proxy.concepts">
<title>Overview</title>
<simpara>SUSE Manager 3.2 Proxy is a SUSE Manager add-on that caches software packages on an internal, central server.
The proxy caches patch updates from SUSE or custom RPMs generated by third-party organizations.
A proxy allows you to use bandwidth more effectively because client systems connect to the proxy for updates, and the SUSE Manager server is no longer required to handle all client requests.
A SUSE Manager Proxy can serve both Traditional and Salt clients.
The proxy also supports transparent custom package deployment.</simpara>
<simpara>SUSE Manager Proxy is an open source (GPLv2) solution that provides the following features:</simpara>
<itemizedlist>
<listitem>
<simpara>Cache software packages within a Squid proxy.</simpara>
</listitem>
<listitem>
<simpara>Client systems see the SUSE Manager Proxy as a SUSE Manager server instance.</simpara>
</listitem>
<listitem>
<simpara>The SUSE Manager Proxy is registered as a client system with the SUSE Manager server.</simpara>
</listitem>
</itemizedlist>
<simpara>The primary goal of a SUSE Manager Proxy is to improve SUSE Manager performance by reducing bandwidth requirements and accelerating response time.</simpara>
</section>
<section xml:id="at.manager.proxy.inst-and-clients">
<title>Proxy Installation and Connecting Clients</title>
<section xml:id="at.manager.proxy.requirements">
<title>Requirements</title>
<simpara>The following section provides SUSE Manager Proxy requirements.</simpara>
<formalpara>
<title>Supported Client Systems</title>
<para>For supported clients and their requirements, see xref:FILENAME.adoc#quickstart.sect.prereq.clientos[].</para>
</formalpara>
<formalpara>
<title>Hardware Requirements</title>
<para>Hardware requirements highly depend on your usage scenario.
When planning proxy environments, consider the amount of data you want to cache on your proxy.
If your proxy should be a 1:1 mirror of your SUSE Manager, the same amount of disk space is required.
For specific hardware requirements, see the following table.</para>
</formalpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Hardware</entry>
<entry align="left" valign="top">Required</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>CPU</simpara></entry>
<entry align="left" valign="top"><simpara>Multi-core 64-bit CPU (x86_64).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RAM</simpara></entry>
<entry align="left" valign="top"><simpara>Minimum 4&#160;GB for a non-production server</simpara></entry>
</row>
<row>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara>Minimum 16&#160;GB for a production server</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Free Disk Space</simpara></entry>
<entry align="left" valign="top"><simpara>Minimum 100&#160;GB for base installation and at least 50 GB for caching per SUSE product and +100 GB per Red Hat product; a resizeable partition strongly recommended.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<tip>
<title>Storage for Proxy Data</title>
<simpara>SUSE recommends storing the squid proxy caching data on a separate disk formatted with the XFS file system.</simpara>
</tip>
<formalpara>
<title>SSL Certificate Password</title>
<para>For installing the proxy, you need the SSL certificate password entered during the initial installation of SUSE Manager.</para>
</formalpara>
<formalpara>
<title>Network Requirements</title>
<para>For additional network requirements, see xref:FILENAME.adoc#quickstart.sect.prereq.network[].</para>
</formalpara>
<formalpara>
<title>SUSE Customer Center</title>
<para>For using SUSE Manager Proxy, you need an account at SUSE Customer Center (SCC) where your purchased products and product subscriptions are registered.
Make sure you have the following subscriptions:</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>One or more subscriptions for SUSE Manager Proxy.</simpara>
</listitem>
<listitem>
<simpara>One or more subscriptions for SUSE Manager.</simpara>
</listitem>
<listitem>
<simpara>Subscriptions for the products on the client systems you want to register with SUSE Manager via SUSE Manager Proxy .</simpara>
</listitem>
<listitem>
<simpara>Subscriptions to client entitlements for the client system you want to register with SUSE Manager via SUSE Manager Proxy .</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Network Time Protocol (NTP)</title>
<para>The connection to the Web server via Secure Sockets Layer (SSL) requires correct time settings on the server, proxy and clients.
For this reason, all systems must use NTP.
For more information, see <link xl:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html">https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html</link>.</para>
</formalpara>
<formalpara>
<title>Virtual Environments</title>
<para>The following virtual environments are supported:</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><link xl:href="http://www.linux-kvm.org/page/Main_Page">http://www.linux-kvm.org/page/Main_Page</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="http://www.vmware.com/">http://www.vmware.com/</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="http://www.microsoft.com/en-us/server-cloud/solutions/virtualization.aspx">http://www.microsoft.com/en-us/server-cloud/solutions/virtualization.aspx</link></simpara>
</listitem>
</itemizedlist>
<simpara>For running SUSE Manager Proxy in virtual environments, use the following settings for the virtual machine (VM):</simpara>
<itemizedlist>
<listitem>
<simpara>At least 1 GB of RAM</simpara>
</listitem>
<listitem>
<simpara>Bridged network</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="at.manager.proxy.inst">
<title>Installation and Setup</title>
<simpara>The following section will guide you through the installation and setup procedure.</simpara>
<simpara>SUSE Manager Proxy systems are registered as traditional clients or as Salt clients using a bootstrap script.
Migrating a traditionally registered Proxy system to a Salt Proxy system is not possible.
Re-install the Proxy if you want to switch to Salt.</simpara>
<important>
<title>Downloading Channels</title>
<simpara>Before you can select the correct child channels while creating the activation key, ensure you have completely downloaded the channels for SUSE Linux Enterprise 12 SP4.</simpara>
</important>
<orderedlist xml:id="at.manager.proxy.install.prep" numeration="arabic">
<title>Procedure: Registering the Proxy</title>
<listitem>
<simpara>Create an activation key based on the SUSE Linux Enterprise 12 SP4 base channel.
For more information about activation keys, see xref:FILENAME.adoc#create.act.keys[].</simpara>
<figure>
<title>Proxy Activation Key</title>
<mediaobject>
<imageobject>
<imagedata fileref="proxy-key.png"/>
</imageobject>
<textobject><phrase>proxy key</phrase></textobject>
</mediaobject>
</figure>
</listitem>
<listitem>
<simpara>From the <literal role="guimenu">Child Channels</literal> listing select the SUSE Manager 3.2 Proxy child channel with the matching update channel (<literal role="systemitem">SUSE Manager Proxy-3.2-Pool</literal> and <literal role="systemitem">SUSE-Manager-Proxy-3.2-Updates</literal>).
These child channels are required for providing the proxy packages and updates.
For normal SLES clients, <literal role="systemitem">SLES12-SP4-Updates</literal> plus <literal role="systemitem">SLE-Manager-Tools12-Pool</literal> and <literal role="systemitem">SLE-Manager-Tools12-Updates</literal> are mandatory.</simpara>
<figure>
<title>Base and Child Proxy Channel</title>
<mediaobject>
<imageobject>
<imagedata fileref="sles12-proxy-child.png"/>
</imageobject>
<textobject><phrase>sles12 proxy child</phrase></textobject>
</mediaobject>
</figure>
</listitem>
<listitem>
<simpara>Modify a bootstrap script for the proxy if needed.
If you want to run the proxy on a traditional client (system type <literal>Management</literal>) uncheck <literal role="guimenu">Bootstrap using Salt</literal>.
Using Salt is supported since version 3.2.
For more information about bootstrap scripts, see xref:FILENAME.adoc#modify.bootstrap.script[].</simpara>
<figure>
<title>Modifying Bootstrap Script</title>
<mediaobject>
<imageobject>
<imagedata fileref="sles12-proxy-bootstrap.png"/>
</imageobject>
<textobject><phrase>sles12 proxy bootstrap</phrase></textobject>
</mediaobject>
</figure>
</listitem>
<listitem>
<simpara>Create the SUSE Manager Tools Repository for bootstrapping, see xref:FILENAME.adoc#create.tools.repository[].</simpara>
</listitem>
<listitem>
<simpara>Bootstrap the client with the bootstrap script.  For more information, see xref:FILENAME.adoc#connect.first.client[].</simpara>
</listitem>
<listitem>
<simpara>In case of a Salt client, accept the key on the <menuchoice><guimenu>Main Menu</guimenu> <guisubmenu>Salt</guisubmenu> <guimenuitem>Keys</guimenuitem></menuchoice> page by clicking the check mark and it will appear in the <menuchoice><guimenu>Main Menu</guimenu> <guisubmenu>Systems</guisubmenu> <guimenuitem>Overview</guimenuitem></menuchoice>.</simpara>
</listitem>
<listitem>
<simpara>Check via <menuchoice><guimenu>System Details</guimenu> <guisubmenu>Software</guisubmenu> <guimenuitem>Software Channels</guimenuitem></menuchoice> that the two proxy channels <literal role="systemitem">SUSE Manager Proxy-3.2-Pool</literal> and <literal role="systemitem">SUSE-Manager-Proxy-3.2-Updates</literal> are selected.</simpara>
</listitem>
</orderedlist>
<figure>
<title>Proxy Channels</title>
<mediaobject>
<imageobject>
<imagedata fileref="sles12-proxy-channels.png"/>
</imageobject>
<textobject><phrase>sles12 proxy channels</phrase></textobject>
</mediaobject>
</figure>
<simpara>A few more steps are still needed:</simpara>
<itemizedlist>
<listitem>
<simpara>Install the <literal role="path">patterns-suma_proxy</literal> pattern (see xref:FILENAME.adoc#at.manager.proxy.run.pattern[])</simpara>
</listitem>
<listitem>
<simpara>Copy the SSL certificate and key from the server (see xref:FILENAME.adoc#at.manager.proxy.run.copycert[])</simpara>
</listitem>
<listitem>
<simpara>Run <literal role="command">configure-proxy.sh</literal> (see pass:c[xref:FILENAME.adoc#at.manager.proxy.run.confproxy)</simpara>
</listitem>
</itemizedlist>
<simpara>You will then be able to register your clients against the proxy using the Web UI or a bootstrap script as if it were a SUSE Manager server.
For more information, see xref:FILENAME.adoc#at.manager.proxy.register.saltclients[].</simpara>
</section>
<section xml:id="at.manager.proxy.run.pattern">
<title>Install the <literal role="path">suma_proxy</literal> pattern</title>
<simpara>On the server select the <literal role="package">pattern_suma_proxy</literal> package for installation, or make sure the <literal role="path">suma_proxy</literal> pattern is installed using the following command on the proxy as root:</simpara>
<screen>zypper in -t pattern suma_proxy</screen>
<simpara>The new salt-broker service will be automatically started at the end of the package installation.
This service forwards the Salt interactions to the SUSE Manager server.</simpara>
<note>
<title>Proxy Chains</title>
<simpara>It is possible to arrange Salt proxies in a chain.
In such a case, the upstream proxy is named &#8220;parent&#8221;.</simpara>
</note>
<simpara>Make sure the proxie&#8217;s TCP ports <literal>4505</literal> and <literal>4506</literal> are open and that the proxy can reach the SUSE Manager server (or another upstream proxy) on these ports.</simpara>
</section>
<section xml:id="at.manager.proxy.run.copycert">
<title>Copy Server Certificate and Key</title>
<simpara>The proxy will share some SSL information with the SUSE Manager server, so the next step is to copy the certificate and its key from the SUSE Manager server or the upstream proxy.</simpara>
<simpara>As root, enter the following commands on the proxy using your SUSE Manager server or chained proxy named <literal role="replaceable">PARENT</literal>:</simpara>
<screen>mkdir /root/ssl-build
cd /root/ssl-build
scp root@PARENT:/root/ssl-build/RHN-ORG-PRIVATE-SSL-KEY .
scp root@PARENT:/root/ssl-build/RHN-ORG-TRUSTED-SSL-CERT .
scp root@PARENT:/root/ssl-build/rhn-ca-openssl.cnf .</screen>
<note>
<title>Known Limitation</title>
<simpara>The SUSE Manager Proxy functionality is only supported if the SSL certificate was signed by the same CA as the SUSE Manager Server certificate.
Using certificates signed by different CAs for Proxies and Server is not supported.</simpara>
</note>
</section>
<section xml:id="at.manager.proxy.run.confproxy">
<title>Running <literal role="command">configure-proxy.sh</literal></title>
<simpara>The <literal role="command">configure-proxy.sh</literal> script will finalize the setup of your SUSE Manager Proxy.</simpara>
<simpara>Now execute the interactive <literal role="command">configure-proxy.sh</literal> script.
Pressing <keycap>Enter</keycap> without further input will make the script use the default values provided between brackets <literal>[]</literal>.
Here is some information about the requested settings:</simpara>
<variablelist>
<varlistentry>
<term>SUSE Manager Parent</term>
<listitem>
<simpara>A SUSE Manager parent can be either another proxy server or a SUSE Manager server.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>HTTP Proxy</term>
<listitem>
<simpara>A HTTP proxy enables your SUSE Manager proxy to access the Web.
This is needed if direct access to the Web is prohibited by a firewall.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Proxy Version to Activate</term>
<listitem>
<simpara>Normally, the correct value (3.0, 3.1, or 3.2) should be offered as a default.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Traceback Email</term>
<listitem>
<simpara>An email address where to report problems.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Use SSL</term>
<listitem>
<simpara>For safety reasons, press <literal>Y</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Do You Want to Import Existing Certificates?</term>
<listitem>
<simpara>Answer <literal>N</literal>.
This ensures using the new certificates that were copied previously from the SUSE Manager server.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Organization</term>
<listitem>
<simpara>The next questions are about the characteristics to use for the SSL certificate of the proxy.
The organization might be the same organization that was used on the server, unless of course your proxy is not in the same organization as your main server.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Organization Unit</term>
<listitem>
<simpara>The default value here is the proxy&#8217;s hostname.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>City</term>
<listitem>
<simpara>Further information attached to the proxy&#8217;s certificate.
Beware the country code must be made of two upper case letters.
For further information on country codes, refer to the online <link xl:href="https://www.iso.org/obp/ui/#search">list of alpha-2 codes</link>.</simpara>
<tip>
<title>Country Code</title>
<simpara>As the country code enter the country code set during the SUSE Manager installation.
For example, if your proxy is in US and your SUSE Manager in DE, you must enter <literal>DE</literal> for the proxy.</simpara>
</tip>
</listitem>
</varlistentry>
<varlistentry>
<term>Cname Aliases (Separated by Space)</term>
<listitem>
<simpara>Use this if your proxy server can be accessed through various DNS CNAME aliases.
Otherwise it can be left empty.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>CA Password</term>
<listitem>
<simpara>Enter the password that was used for the certificate of your SUSE Manager server.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Do You Want to Use an Existing SSH Key for Proxying SSH-Push Salt Minions?</term>
<listitem>
<simpara>Use this option if you want to reuse a SSH key that was used for SSH-Push Salt minions on the server.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Create and Populate Configuration Channel rhn_proxy_config_1000010001?</term>
<listitem>
<simpara>Accept default <literal>Y</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>SUSE Manager Username</term>
<listitem>
<simpara>Use same user name and password as on the SUSE Manager server.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Activate advertising proxy via SLP?</term>
<listitem>
<simpara>SLP stands for Service Location Protocol.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>If parts are missing, such as CA key and public certificate, the script prints commands that you must execute to integrate the needed files.
When the mandatory files are copied, re-run <literal role="command">configure-proxy.sh</literal>.
Also restart the script if a HTTP error was met during script execution.</simpara>
<simpara><literal role="command">configure-proxy.sh</literal> activates services required by SUSE Manager Proxy, such as <literal role="systemitem">squid</literal>, <literal role="systemitem">apache2</literal>, <literal role="systemitem">salt-broker</literal>, and <literal role="systemitem">jabberd</literal>.</simpara>
<simpara>To check the status of the proxy system and its clients, click the proxy system&#8217;s details page on the Web UI (<menuchoice><guimenu>Main Menu</guimenu> <guisubmenu>Systems</guisubmenu> <guimenuitem>Proxy</guimenuitem></menuchoice>, then the system name). <literal role="guimenu">Connection</literal> and <literal role="guimenu">Proxy</literal> subtabs display the respective status information.</simpara>
</section>
<section xml:id="at.manager.proxy.register.saltclients">
<title>Registering Salt Clients via SUSE Manager Proxy</title>
<simpara>Proxy servers may now act as a broker and package cache for Salt minions.
These minions can be registered with a bootstrap script like the traditional clients, or from the Web UI, or the command line.</simpara>
<simpara>Registering Salt clients via SUSE Manager Proxy from the Web UI
is done almost the same way as registering clients directly with the SUSE Manager server.
The difference is that you specify the name of the proxy in the <literal role="guimenu">Proxy</literal> drop-box on the <menuchoice><guimenu>Main Menu</guimenu> <guisubmenu>Systems</guisubmenu> <guimenuitem>Bootstrapping</guimenuitem></menuchoice> page.</simpara>
<figure>
<title>Bootstrapping a Salt Client With a Proxy</title>
<mediaobject>
<imageobject>
<imagedata fileref="proxy-saltbootstrap.png" width="80%"/>
</imageobject>
<textobject><phrase>proxy saltbootstrap</phrase></textobject>
</mediaobject>
</figure>
<orderedlist numeration="arabic">
<title>Procedure: Register a Salt client through a proxy from command line</title>
<listitem>
<simpara>Instead of the Web UI, you may use the command line to register a minion through a proxy.
To do so, add the proxy FQDN as the master in the minions configuration file located at:</simpara>
<screen>/etc/salt/minion</screen>
<simpara>or alternatively:</simpara>
<screen>/etc/salt/minion.d/NAME.conf</screen>
</listitem>
<listitem>
<simpara>Add the FQDN to the minion file:</simpara>
<screen>master: proxy123.example.com</screen>
<simpara>Save and restart the salt-minion service with:</simpara>
<screen>systemctl restart salt-minion</screen>
</listitem>
<listitem>
<simpara>On the Server, accept the new minion key with:</simpara>
<screen>salt-key -a 'minion'</screen>
<simpara>The minion will now connect to the proxy exclusively for Salt operations and normal HTTP package downloads.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="at.manager.proxy.register.clients">
<title>Registering Clients via SUSE Manager Proxy with a Script</title>
<simpara>Registering clients (either traditional or Salt) via SUSE Manager Proxy with a script is done almost the same way as registering clients directly with the SUSE Manager server.
The difference is that you create the bootstrap script on the SUSE Manager Proxy with a command-line tool.
The bootstrap script then deploys all necessary information to the clients.
The bootstrap script refers some parameters (such as activation keys or GPG keys) that depend on your specific setup.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a client activation key on the SUSE Manager server using the Web UI.
See xref:FILENAME.adoc#create.act.keys[].</simpara>
</listitem>
<listitem>
<simpara>On the proxy, execute the <literal role="command">mgr-bootstrap</literal> command-line tool as root.
If needed, use the additional command-line switches to tune your bootstrap script. An important option is <literal role="command">--traditional</literal> that enables to opt for a traditional client instead of a salt minion.</simpara>
<simpara>To view available options type <literal role="command">mgr-bootstrap --help</literal> from the command line:</simpara>
<screen># mgr-bootstrap --activation-keys=key-string</screen>
</listitem>
<listitem>
<simpara>Optionally edit the resulting bootstrap script.
Execute the bootstrap script on the clients as described in xref:FILENAME.adoc#connect.first.client[].</simpara>
</listitem>
</orderedlist>
<simpara>The clients are registered with the SUSE Manager Proxy specified in the bootstrap script.</simpara>
</section>
<section xml:id="at.additional.info.about.client.registration.on.proxies">
<title>Additional Information about Client Registration on Proxies</title>
<simpara>Within the Web UI, standard proxy pages will show information about client, no matter whether minions or traditional clients.</simpara>
<simpara>A list of clients connected to a proxy can be located under <menuchoice><guimenu>Systems</guimenu> <guisubmenu>] &lt;proxy name</guisubmenu> <guimenuitem>menu:Details[</guimenuitem></menuchoice><guimenu>Proxy</guimenu>.</simpara>
<simpara>A list of chained proxies for a minion can be located under <menuchoice><guimenu>Systems</guimenu> <guisubmenu>] &lt;minion name</guisubmenu> <guimenuitem>menu:Details[</guimenuitem></menuchoice><guimenu>Connection</guimenu></simpara>
<simpara>If you decide to move any of your clients between proxies or the server you will need to repeat the registration process from scratch.</simpara>
</section>
</section>
<section xml:id="advanced.topics.proxy.pxe">
<title>Enabling PXE Boot via SUSE Manager Proxy</title>
<section xml:id="advanced.topics.proxy.pxe.sync">
<title>Synchronizing Profiles and System Information</title>
<simpara>To enable PXE boot via a proxy server, additional software must be installed and configured on both the SUSE Manager server and the SUSE Manager Proxy server.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On the SUSE Manager server install <phrase role="package">susemanager-tftpsync</phrase> :</simpara>
<screen>zypper in susemanager-tftpsync</screen>
</listitem>
<listitem>
<simpara>On the SUSE Manager Proxy server install <phrase role="package">susemanager-tftpsync-recv</phrase> :</simpara>
<screen>zypper in susemanager-tftpsync-recv</screen>
</listitem>
<listitem>
<simpara>Run the <literal role="command">configure-tftpsync.sh</literal> setup script and enter the requested information:</simpara>
<screen>configure-tftpsync.sh</screen>
<simpara>It asks for hostname and IP address of the SUSE Manager server and of the proxy itself.
Additionally, it asks for the tftpboot directory on the proxy.</simpara>
</listitem>
<listitem>
<simpara>On the SUSE Manager server, run <literal role="command">configure-tftpsync.sh</literal> to configure the upload to the SUSE Manager Proxy server:</simpara>
<screen>configure-tftpsync.sh FQDN_of_Proxy_Server</screen>
</listitem>
<listitem>
<simpara>To initiate an initial synchronization on the SUSE Manager Server run:</simpara>
<screen>cobbler sync</screen>
<simpara>Also can also be done after each a change within Cobbler that needs to be synchronized immediately.
Otherwise Cobbler synchronization will also run automatically when needed.
For more information about Cobbler, see xref:FILENAME.adoc#advanced.topics.cobbler[].</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="advanced.topics.proxy.pxe.dhcp">
<title>Configuring DHCP for PXE via SUSE Manager Proxy</title>
<simpara>SUSE Manager is using Cobbler to provide provisioning.
PXE (tftp) is installed and activated by default.
To enable systems to find the PXE boot on the SUSE Manager Proxy server add the following to the DHCP configuration for the zone containing the systems to be provisioned:</simpara>
<screen>next-server: &lt;IP_Address_of_SUSE_Manager_Proxy_Server&gt;
filename: "pxelinux.0"</screen>
</section>
</section>
<section xml:id="advanced.topics.proxy.migration3">
<title>Migrating SUSE Manager 3.1 Proxy to Version 3.2 [Management]</title>
<simpara>The recommended order for migrations is to first migrate the server and then the proxies.</simpara>
<simpara>For the migration of traditionally managed proxies there are two possible approaches:</simpara>
<itemizedlist>
<listitem>
<simpara>Existing SUSE Manager proxies may be upgraded to version 3.2 with YaST or <literal role="command">zypper</literal> migration.</simpara>
</listitem>
<listitem>
<simpara>Alternatively, the proxies may be replaced by new ones.</simpara>
</listitem>
</itemizedlist>
<simpara>This section documents both approaches.</simpara>
<note>
<title>Migrating SUSE Manager 3 Proxy and Earlier</title>
<simpara>For migrating SUSE Manager 3 Proxy and earlier, see <link xl:href="https://www.suse.com/documentation/suse-manager-3/book_suma_advanced_topics_31/data/sect1_chapter_book_suma_advanced_topics_31.html">https://www.suse.com/documentation/suse-manager-3/book_suma_advanced_topics_31/data/sect1_chapter_book_suma_advanced_topics_31.html</link>, Chapter "SUSE Manager 3.1 Proxy".</simpara>
</note>
<section xml:id="at.replacing.a.susemgrproxy">
<title>Replacing a SUSE Manager Proxy</title>
<simpara>A SUSE Manager Proxy is <literal>dumb</literal> in the sense that it does not contain any information about the clients which are connected to it.
A SUSE Manager Proxy can therefore be replaced by a new one.
Naturally, the replacement proxy must have the same name and IP address as its predecessor.</simpara>
<simpara>In order to replace a SUSE Manager Proxy and keeping the clients registered to the proxy leave the old proxy in SUSE Manager.
Create a reactivation key for this system and then register the new proxy using the reactivation key.
If you do not use the reactivation key, you will need to re-registered all the clients against the new proxy.</simpara>
<orderedlist xml:id="proc.advanced.topics.proxy.migration3.replace" numeration="arabic">
<title>Procedure: Replacing a SUSE Manager Proxy and Keeping the Clients Registered</title>
<listitem>
<simpara>Before starting the actual migration procedure, save the data from the old proxy, if needed.
Consider copying important data to a central place that can also be accessed by the new server:</simpara>
<itemizedlist>
<listitem>
<simpara>Copy the scripts that are still needed.</simpara>
</listitem>
<listitem>
<simpara>Copy the activation keys from the previous server.
Of course, it is always better to re-create the keys.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Shutdown the server.</simpara>
</listitem>
<listitem>
<simpara>Install a new SUSE Manager 3.2 Proxy, see xref:FILENAME.adoc#at.manager.proxy.inst-and-clients[].</simpara>
</listitem>
<listitem>
<simpara>In the SUSE Manager Web UI select the newly installed SUSE Manager Proxy and delete it from the systems list.</simpara>
</listitem>
<listitem>
<simpara>In the Web UI, create a reactivation key for the old proxy system: On the System Details tab of the old proxy click <literal role="guimenu">Reactivation</literal>.
Then click <literal role="guimenu">Generate New Key</literal>, and remember it (write it on a piece of paper or copy it to the clipboard).
For more information about reactivation keys, see xref:FILENAME.adoc#s5-sm-system-details-react[].</simpara>
</listitem>
<listitem>
<simpara>After the installation of the new proxy, perform the following actions (if needed):</simpara>
<itemizedlist>
<listitem>
<simpara>Copy the centrally saved data to the new proxy system.</simpara>
</listitem>
<listitem>
<simpara>Install any other needed software.</simpara>
</listitem>
<listitem>
<simpara>If the proxy is also used for autoinstallation, do not forget to setup TFTP synchronization.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<important>
<title>Proxy Installation and Client Connections</title>
<simpara>During the installation of the proxy, clients will not be able to reach the SUSE Manager server.
After a SUSE Manager Proxy system has been deleted from the systems list, all clients connected to this proxy will be (incorrectly) listed as <literal>directly connected</literal> to the SUSE Manager server.
After the first successful operation on a client <emphasis>such as execution of a remote command or installation of a package or patch</emphasis> this information will automatically be corrected.
This may take a few hours.</simpara>
</important>
</section>
<section xml:id="at.upgrade.a.susemgrproxy">
<title>Upgrading a SUSE Manager Proxy from 3.1 to 3.2</title>
<simpara>In most situations upgrading the proxy will be your preferred solution as this retains all cached packages.
Selecting this route saves time especially regarding proxies connected to SUSE Manager server via low-bandwith links.
This upgrade is similar to a standard client migration.</simpara>
<warning>
<title>Synchronizing Target Channels</title>
<simpara>Before successfully initializing the product migration, you first must make sure that the migration target channels are completely mirrored.
To upgrade to SUSE Manager 3.2 Proxy, you will require at least the <literal role="systemitem">SUSE Linux Enterprise Server 12 SP4</literal> base channel with the <literal role="systemitem">SUSE Manager Proxy 3.2</literal> child channel for your architecture.</simpara>
</warning>
<orderedlist numeration="arabic">
<title>Procedure: Migrating Proxy to 3.2</title>
<listitem>
<simpara>Direct your browser to the SUSE Manager Web UI where your proxy is registered, and login.</simpara>
</listitem>
<listitem>
<simpara>On the <menuchoice><guimenu>Main Menu</guimenu> <guisubmenu>Systems</guisubmenu> <guisubmenu>Systems</guisubmenu> <guimenuitem>Proxy</guimenuitem></menuchoice> page select your proxy server from the table.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="suma_proxy_old_details_page.png"/>
</imageobject>
<textobject><phrase>suma proxy old details page</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>On the system&#8217;s detail page select the <menuchoice><guimenu>Software</guimenu> <guimenuitem>SP Migration</guimenuitem></menuchoice> tab.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="suma_proxy_old_details_spmigration.png"/>
</imageobject>
<textobject><phrase>suma proxy old details spmigration</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>From this page you will see installed products listed on your proxy client, and the available target products.
Select the required <literal role="guimenu">Target Products</literal>.  In this case, you will require <literal role="systemitem">SUSE Linux Enterprise Server 12 SP4</literal> with <literal role="systemitem">SUSE Manager Proxy 3.2</literal>.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="suma_proxy_migration_target.png"/>
</imageobject>
<textobject><phrase>suma proxy migration target</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>Then confirm with <guibutton>Select Channels</guibutton>.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="suma_proxy_migration_channels.png"/>
</imageobject>
<textobject><phrase>suma proxy migration channels</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>From the <literal role="guimenu">Schedule Migration</literal> menu, select the time and click <guibutton>Confirm</guibutton>.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="suma_proxy_migration_schedule.png"/>
</imageobject>
<textobject><phrase>suma proxy migration schedule</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
<simpara>Check the <literal role="guimenu">System Status</literal> on the <menuchoice><guimenu>System Details</guimenu> <guimenuitem>Overview</guimenuitem></menuchoice> when the migration is done.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="suma_proxy_migrated.png"/>
</imageobject>
<textobject><phrase>suma proxy migrated</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>Finally consider scheduling a reboot.</simpara>
</section>
</section>
</section>
</chapter>
</part>
<part xml:id="book.mgr.client.cfg">
<title>SUSE Manager Client Configuration</title>
<chapter xml:id="chapt-client-cfg-overview">
<title>Client Configuration Overview</title>
<section xml:id="scc">
<title>SUSE Customer Center (SCC)</title>
<simpara>SUSE Customer Center (SCC) is the central place to manage your purchased SUSE subscriptions, helping you access your update channels and get in contact with SUSE experts.
The user-friendly interface gives you a centralized view of all your SUSE subscriptions, allowing you to easily find all subscription information you need.
The improved registration provides faster access to your patches and updates.
SUSE Customer Center is also designed to provide a common platform for your support requests and feedback.
Discover a new way of managing your SUSE account and subscriptions via one interface&#8212;&#8203;anytime, anywhere.
For more information on using SUSE Customer Center, see: <link xl:href="https://scc.suse.com/docs/userguide">https://scc.suse.com/docs/userguide</link>.</simpara>
</section>
<section xml:id="create.act.keys">
<title>Activation Keys</title>
<simpara>Activation keys are used with both traditional and Salt clients to ensure that your clients have the correct software entitlements, are connecting to the appropriate channels, and are subscribed to the relevant groups.
Each activation key is bound to an organization, which you can set when you create the key.</simpara>
<simpara>This section goes over activation key core concepts. To learn how to create activation keys for manual registration see:</simpara>
<section xml:id="bp.key.managment">
<title>Managing Activation Keys</title>
<section xml:id="_what_are_activation_keys">
<title>What are Activation Keys?</title>
<simpara>An <literal>activation key</literal> in SUSE Manager is a group of configuration settings with a label.
You can apply all configuration settings associated with an activation key by adding its label as a parameter to a bootstrap script.
Under normal operating conditions best practices suggest using an activation key label in combination with a bootstrap script.</simpara>
<simpara>An activation key can specify:</simpara>
<itemizedlist>
<listitem>
<simpara>Channel Assignment</simpara>
</listitem>
<listitem>
<simpara>System Types (Traditionally called Add-on Entitlements)</simpara>
</listitem>
<listitem>
<simpara>Contact Method</simpara>
</listitem>
<listitem>
<simpara>Configuration Files</simpara>
</listitem>
<listitem>
<simpara>Packages to be Installed</simpara>
</listitem>
<listitem>
<simpara>System Group Assignment</simpara>
</listitem>
</itemizedlist>
<simpara>Activation keys are just a collection of configuration settings which have been given a label name and then added to a bootstrap script.
When the bootstrap script is executed all configuration settings associated with the label are applied to the system the script is run on.</simpara>
</section>
<section xml:id="_provisioning_and_configuration">
<title>Provisioning and Configuration</title>
<figure>
<title>Provisioning and Configuration Overview</title>
<mediaobject>
<imageobject>
<imagedata fileref="provision-config-keys.png" width="80%"/>
</imageobject>
<textobject><phrase>provision config keys</phrase></textobject>
</mediaobject>
</figure>
</section>
<section xml:id="_activation_keys_best_practices">
<title>Activation Keys Best Practices</title>
<simpara>There are a few important concepts which should be kept in mind when creating activation keys.
The following sections provide insight when creating and naming your activation keys.</simpara>
<section xml:id="_key_label_naming">
<title>Key Label Naming</title>
<simpara>One of the most important things to consider during activation key creation is label naming.
Creating names which are associated with your organization&#8217;s infrastructure will make it easier for you when performing more complex operations.
When naming key labels keep the following in mind:</simpara>
<itemizedlist>
<listitem>
<simpara>OS naming (mandatory): Keys should always refer to the OS they provide settings for</simpara>
</listitem>
<listitem>
<simpara>Architecture naming (recommended): Unless your company is running on one architecture only, for example x86_64, then providing labels with an architecture type is a good idea.</simpara>
</listitem>
<listitem>
<simpara>Server type naming: What is, or what will this server be used for?</simpara>
</listitem>
<listitem>
<simpara>Location naming: Where is the server located? Room, building, or department?</simpara>
</listitem>
<listitem>
<simpara>Date naming: Maintenance windows, quarter, etc.</simpara>
</listitem>
<listitem>
<simpara>Custom naming: What naming scheme suits your organizations needs?</simpara>
</listitem>
</itemizedlist>
<simpara>Example activation key label names:</simpara>
<screen>sles12-sp2-web_server-room_129-x86_64</screen>
<screen>sles12-sp2-test_packages-blg_502-room_21-ppc64le</screen>
</section>
<section xml:id="_channels_which_will_be_included">
<title>Channels which will be Included</title>
<simpara>When creating activation keys you also need to keep in mind which channels (software sources) will be associated with it.</simpara>
<important>
<title>Default Base Channel</title>
<simpara>Keys should have a specific base channel assigned to it, for example <literal>SLES12-SP2-Pool-x86_64</literal>.
If this is not the case SUSE Manager cannot use specific stages.
Using the default base channel is not recommended and may cause problems.</simpara>
</important>
<itemizedlist>
<listitem>
<simpara>Channels to be included:</simpara>
<itemizedlist>
<listitem>
<simpara>suse-manager-tools</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Typical packages to be included:</simpara>
<itemizedlist>
<listitem>
<simpara>osad (pushing tasks)</simpara>
<itemizedlist>
<listitem>
<simpara>Installs <phrase role="package">python-jabberpy</phrase> and <phrase role="package">pyxml</phrase> as dependencies</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><phrase role="package">rhncfg-actions</phrase> (Remote Command, Configuration Managment)</simpara>
<itemizedlist>
<listitem>
<simpara>Installs <phrase role="package">rhncfg</phrase> and <phrase role="package">rhncfg-client</phrase> as dependencies</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="_combining_activation_keys">
<title>Combining Activation Keys</title>
<simpara>You can combine activation keys when executing the bootstrap script on your clients.
Combining keys allows for more control on what is installed on your systems and reduces duplication of keys for large complex environments.</simpara>
<figure>
<title>Combining Activation Keys</title>
<mediaobject>
<imageobject>
<imagedata fileref="combine-keys.png" width="80%"/>
</imageobject>
<textobject><phrase>combine keys</phrase></textobject>
</mediaobject>
</figure>
<figure>
<title>Combining Activation Keys 2</title>
<mediaobject>
<imageobject>
<imagedata fileref="combine-keys2.png" width="80%"/>
</imageobject>
<textobject><phrase>combine keys2</phrase></textobject>
</mediaobject>
</figure>
</section>
<section xml:id="_using_activation_keys_and_bootstrap_with_traditional_clients_non_salt">
<title>Using Activation Keys and Bootstrap with Traditional Clients (Non-Salt)</title>
<simpara>Create the initial bootstrap script template from the command line on the SUSE Manager server with:</simpara>
<screen># mgr-bootstrap</screen>
<simpara>This command will generate the bootstrap script and place them in <literal role="path">/srv/www/htdocs/pub/bootstrap</literal>.</simpara>
<simpara>Alternatively you may use the Web UI to create your bootstrap script template.
For more information, see xref:FILENAME.adoc#s3-sattools-config-bootstrap[].</simpara>
<simpara>Use the Web UI to create your keys.
From the Web UI proceed to menu:Overview[Tasks</simpara>
</section>
</section>
</section>
<section xml:id="client-cfg-traditional-vs-salt">
<title>Introduction</title>
<simpara>Coming Soon&#8230;&#8203;</simpara>
</section>
<section xml:id="client-cfg-bootstrapping">
<title>Introduction</title>
<simpara>Coming Soon&#8230;&#8203;</simpara>
</section>
</chapter>
<chapter xml:id="chapt-client-cfg-reg-manual">
<title>Manual Registration</title>
<section xml:id="client-cfg-creating-activation-keys">
<title>Creating Activation keys</title>
<simpara>This section contains information on how to create activation keys for both traditional and Salt clients, and provides some best practices for working with activation keys.</simpara>
<orderedlist xml:id="create.activation.keys" numeration="arabic">
<title>Procedure: Creating Activation Keys</title>
<listitem>
<simpara>As the administrator login to the SUSE Manager Web UI.</simpara>
</listitem>
<listitem>
<simpara>Navigate to <menuchoice><guimenu>Systems</guimenu> <guimenuitem>Activation Keys</guimenuitem></menuchoice>.</simpara>
</listitem>
<listitem>
<simpara>To open the <literal role="guimenu">Activation Key Details</literal> page click the <guibutton>Create Key</guibutton> button in the upper right corner.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="systems_create_activation_key.png" width="80%"/>
</imageobject>
<textobject><phrase>systems create activation key</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>On the <literal role="guimenu">Activation Key Details</literal> page in the <literal role="guimenu">Description</literal> field, enter a name for the activation key.</simpara>
</listitem>
<listitem>
<simpara>In the <literal role="guimenu">Key</literal> field, enter the distribution and service pack associated with the key. For example, <literal>SLES12-SP4</literal> for SUSE Linux Enterprise Server&#160;12&#160;SP4.</simpara>
<warning>
<title>Allowed Characters</title>
<simpara>Do not use commas in the <literal role="guimenu">Key</literal> field for any SUSE products.
However, you <emphasis role="strong">must</emphasis> use commas for Red Hat Products.
For more information, see xref:FILENAME.adoc#ref.webui.systems.activ-keys[].</simpara>
</warning>
</listitem>
<listitem>
<simpara>In the <literal role="guimenu">Base Channels</literal> drop-down box, select the SUSE Linux Enterprise channel that you added during
xref:FILENAME.adoc#gs-syncing-with-scc[].</simpara>
</listitem>
<listitem>
<simpara>When the base channel is selected the list of available child channels will get fetched and displayed in real time below the base channel.
Select the child channels you need (for example, the SUSE Manager tools and the updates channels that are actually mandatory).</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="systems_create_activation_key_childchannels.png" width="80%"/>
</imageobject>
<textobject><phrase>systems create activation key childchannels</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>We recommend you leave the <literal role="guimenu">Contact Method</literal> set to <literal role="guimenu">Default</literal>.</simpara>
</listitem>
<listitem>
<simpara>We recommend you leave the <literal role="guimenu">Universal Default</literal> setting unchecked.</simpara>
</listitem>
<listitem>
<simpara>Click <guibutton>Update Activation Key</guibutton> to create the activation key.</simpara>
</listitem>
<listitem>
<simpara>Check the <literal role="guimenu">Configuration File Deployment</literal> check box to enable configuration management for this key, and click <guibutton>Update Activation Key</guibutton> to save this change.</simpara>
</listitem>
</orderedlist>
<simpara>When you create activation keys, keep these best practices in mind:</simpara>
<itemizedlist>
<listitem>
<simpara>Avoid using the <literal role="systemitem">SUSE Manager Default</literal> parent channel.
This setting forces SUSE Manager to choose a parent channel that best corresponds to the installed operating system, which can sometimes lead to unexpected behavior.
Instead, we recommend you create activation keys specific to each distribution and architecture.</simpara>
</listitem>
<listitem>
<simpara>If you are using bootstrap scripts, consider creating an activation key for each script.
This will help you align channel assignments, package installation, system group memberships, and configuration channel assignments.
You will also need less manual interaction with your system after registration.</simpara>
</listitem>
<listitem>
<simpara>If you do not enter a human-readable name for your activation keys, the system will automatically generate a number string, which can make it difficult to manage your keys.
Consider a naming scheme for your activation keys to help you keep track of them.</simpara>
</listitem>
<listitem>
<simpara>Note that the <literal role="guimenu">Configuration File Deployment</literal> check box does not appear until after you have created the activation key.
Ensure you go back and check the box if you need to enable configuration management.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="create.tools.repository">
<title>Creating the SUSE Manager Tools Repository</title>
<simpara>In this section you will create a tools repository on the SUSE Manager Server for providing client tools.
The client tools repository contains packages for installing Salt on minions as well as required packages for registering traditional clients during the bootstrapping procedure.
These packages will be installed from the newly generated repository during the registration process.
In the following procedure you will create the SUSE Linux Enterprise tools repository.</simpara>
<important>
<title>Creating a Tools Repository when an SCC Channel has not been Synced</title>
<simpara>Before following the procedure to create the tools repository make sure the SUSE vendor channel you will be using with your client has been completely synced.
You can check this by running <literal role="command">tail -f /var/log/rhn/reposync/</literal>&lt;CHANNEL_NAME&gt;<literal>.log</literal> as <emphasis>root</emphasis>.
In the following example replace <literal role="replaceable">version</literal> with the actual version string:</simpara>
<screen># tail -f /var/log/rhn/reposync/sles`version`-pool-x86_64.log</screen>
<simpara>Once completed you should see the following output in your terminal:</simpara>
<screen>2017/12/12 15:20:32 +02:00 Importing packages started.
2017/12/12 15:22:02 +02:00 1.07 %
...
2017/12/12 15:34:25 +02:00 86.01 %
2017/12/12 15:35:49 +02:00 Importing packages finished.
2017/12/12 15:35:49 +02:00 Linking packages to channel.
...
2017/12/12 15:35:59 +02:00 Sync completed.</screen>
</important>
<orderedlist numeration="arabic">
<title>Procedure: Generating the Tools Repository for SUSE Linux Enterprise</title>
<listitem>
<simpara>Open a terminal on the server as root and enter the following command to list available bootstrap repositories:</simpara>
<screen>mgr-create-bootstrap-repo -l SLE-`version`-x86_64</screen>
</listitem>
<listitem>
<simpara>Then invoke the same command using the listed repository as the product label to actually create the bootstrap repository:</simpara>
<screen>mgr-create-bootstrap-repo -c SLE-`version`-x86_64</screen>
</listitem>
<listitem>
<simpara>SUSE Manager will create and add the client tools to the newly created <literal role="path">repositories</literal> directory located at <literal role="path">/srv/www/htdocs/pub/repositories/</literal>.</simpara>
</listitem>
</orderedlist>
<simpara>This repository is suitable for both Server and Desktop of SUSE Linux Enterprise.</simpara>
<note>
<title>Support for SUSE Linux Enterprise 15 Products</title>
<simpara>If you have mirrored more than one SUSE Linux Enterprise 15 Product (for example, SLES, {slda}, and SLES for SAP Application), you can specify the one you are actually interested in. First check what is avaiable:</simpara>
<screen>mgr-create-bootstrap-repo -c SLE-15-x86_64 --with-custom-channel
Multiple options for parent channel found. Please use option
--with-parent-channel &lt;label&gt; and choose one of:
- sle-product-sles15-pool-x86_64
- sle-product-sles_sap15-pool-x86_64
- sle-product-sled15-pool-x86_64</screen>
<simpara>Then specify it with <literal role="literal">--with-parent-channel</literal>:</simpara>
<screen>mgr-create-bootstrap-repo -c SLE-15-x86_64 --with-parent-channel sle-product-sled15-pool-x86_64</screen>
</note>
</section>
<section xml:id="registering.clients.traditional">
<title>Registering Traditional Clients</title>
<section xml:id="generate.bootstrap.script">
<title>Generating a Bootstrap Script</title>
<simpara>To register traditional clients, you need to create a template bootstrap script, which can be copied and modified.
The bootstrap script you create is executed on the traditional client when it is registered, and ensures all the necessary packages are deployed to the client.
There are some parameters contained in the bootstrap script which ensure the client system can be assigned to its base channel, including activation keys, and GPG keys.</simpara>
<simpara>It is important that you check the repository information carefully, to ensure it matches the base channel repository.
If the repository information does not match exactly, the bootstrap script will not be able to download the correct packages.</simpara>
<important>
<title>GPG Keys and Uyuni Client Tools</title>
<simpara>The GPG key used by Uyuni Client Tools is not trusted by default.
When you create your bootstrap script, add a path to the file containing the public key fingerprint with the <literal role="systemitem">ORG_GPG_KEY</literal> parameter.</simpara>
</important>
<note>
<title>SLES 15 and Python 3</title>
<simpara>SLES 15 uses Python 3 by default.
Bootstrap scripts based on Python 2 must be re-created for SLES 15 systems.
Attempting to register SLES 15 systems with SUSE Manager using Python 2 bootstrap scripts will fail.</simpara>
</note>
<simpara>This procedure describes how to generate a bootstrap script.</simpara>
<orderedlist xml:id="create.boot.script" numeration="arabic">
<title>Procedure: Creating a Bootstrap Script</title>
<listitem>
<simpara>From the SUSE Manager Web UI, browse to <menuchoice><guimenu>Main Menu</guimenu> <guisubmenu>Admin</guisubmenu> <guisubmenu>Manager Configuration</guisubmenu> <guimenuitem>Bootstrap Script</guimenuitem></menuchoice>. For more information, see xref:FILENAME.adoc#s3-sattools-config-bootstrap[].</simpara>
</listitem>
<listitem>
<simpara>In the <literal role="guimenu">SUSE Manager Configuration - Bootstrap</literal> dialog disable <literal role="guimenu">Bootstrap using Salt</literal>.
Use default settings and click the <guibutton>Update</guibutton> button.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="mgr_configuration_bootstrap_trad.png" width="80%"/>
</imageobject>
<textobject><phrase>mgr configuration bootstrap trad</phrase></textobject>
</mediaobject>
</informalfigure>
<warning>
<title>Using SSL</title>
<simpara>Unchecking <guimenu>Enable SSL</guimenu> in the Web UI or setting <literal>USING_SSL=0</literal> in the bootstrap script is not recommended.
If you disable SSL nevertheless you will need to manage custom CA certificates to be able to run the registration process successfully.</simpara>
</warning>
</listitem>
<listitem>
<simpara>A template bootstrap script is generated and stored on the server&#8217;s file system in the <literal role="path">/srv/www/htdocs/pub/bootstrap</literal> directory.</simpara>
<screen>cd /srv/www/htdocs/pub/bootstrap</screen>
<simpara>The bootstrap script is also available at <literal role="path"><link xl:href="https://example.com/pub/bootstrap/bootstrap.sh">https://example.com/pub/bootstrap/bootstrap.sh</link></literal>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="modify.bootstrap.script">
<title>Editing the Bootstrap Script</title>
<simpara>In this section you will copy and modify the template bootstrap script you created from xref:FILENAME.adoc#generate.bootstrap.script[].</simpara>
<simpara>A minimal requirement when modifying a bootstrap script for use with SUSE Manager is the inclusion of an activation key.
Depending on your organizations security requirements it is strongly recommended to include one or more (GPG) keys (for example, your organization key, and package signing keys).
For this tutorial you will be registering with the activation keys created in the previous section.</simpara>
<orderedlist xml:id="mod.bootstrap.script" numeration="arabic">
<title>Procedure: Modifying the Bootstrap Script</title>
<listitem>
<simpara>Login as root from the command line on your SUSE Manager server.</simpara>
</listitem>
<listitem>
<simpara>Navigate to the bootstrap directory with:</simpara>
<screen>cd /srv/www/htdocs/pub/bootstrap/</screen>
</listitem>
<listitem>
<simpara>Create and rename two copies of the template bootstrap script for use with each of your clients.</simpara>
<screen>cp bootstrap.sh bootstrap-sles11.sh
cp bootstrap.sh bootstrap-sles12.sh</screen>
</listitem>
<listitem>
<simpara>Open <literal role="path">sles12.sh</literal> for modification.
Scroll down and modify both lines marked in green.
You must comment out <literal>exit 1</literal> with a hash mark (<literal>#</literal>) to activate the script and then enter the name of the key for this script in the <literal>ACTIVATION_KEYS=</literal> field as follows:</simpara>
<screen>echo "Enable this script: comment (with #'s) this block (or, at least just"
echo "the exit below)"
echo
#exit 1

# can be edited, but probably correct (unless created during initial install):
# NOTE: ACTIVATION_KEYS *must* be used to bootstrap a client machine.
ACTIVATION_KEYS=1-sles12
ORG_GPG_KEY=</screen>
</listitem>
<listitem>
<simpara>Once you have completed your modifications save the file and repeat this procedure for the second bootstrap script.
Proceed to xref:FILENAME.adoc#connect.first.client[].</simpara>
</listitem>
</orderedlist>
<note>
<title>Finding Your Keys</title>
<simpara>To find key names you have created, navigate to <menuchoice><guimenu>Home</guimenu> <guisubmenu>Overview</guisubmenu> <guisubmenu>Manage Activation keys</guisubmenu> <guimenuitem>Key Field</guimenuitem></menuchoice> in the Web UI.
All keys created for channels are listed on this page.
You must enter the full name of the key you wish to use in the bootstrap script exactly as presented in the key field.</simpara>
</note>
</section>
<section xml:id="connect.first.client">
<title>Connecting Clients</title>
<simpara>This section covers connecting your clients to SUSE Manager with the modified bootstrap script.</simpara>
<orderedlist xml:id="run.bootstrap.script" numeration="arabic">
<title>Procedure: Running the Bootstrap Script</title>
<listitem>
<simpara>From your SUSE Manager Server command line as root navigate to the following directory:</simpara>
<screen>cd /srv/www/htdocs/pub/bootstrap/</screen>
</listitem>
<listitem>
<simpara>Run the following command to execute the bootstrap script on the client:</simpara>
<screen>cat MODIFIED-SCRIPT.SH | ssh root@example.com /bin/bash</screen>
</listitem>
<listitem>
<simpara>The script will execute and proceed to download the required dependencies located in the repositories directory you created earlier.
Once the script has finished running, log in to the Web UI and click <menuchoice><guimenu>Systems</guimenu> <guimenuitem>Overview</guimenuitem></menuchoice> to see the new client listed.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="sect.tradclient.packagelock">
<title>Package Locks</title>
<simpara>Package locks are used to prevent unauthorized installation or upgrades to software packages on traditional clients.
When a package has been locked, it will show a padlock icon, indicating that it can not be installed.
Any attempt to install a locked package will be reported as an error in the event log.</simpara>
<simpara>Locked packages can not be installed, upgraded, or removed, either through the SUSE Manager Web UI, or directly on the client machine using a package manager.
Locked packages will also indirectly lock any dependent packages.</simpara>
<note>
<simpara>Package locks can only be used on traditional clients that use the Zypper package manager.
The feature is not currently supported on Red Hat Enterprise Linux or Salt clients.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure: Using Package Locks</title>
<listitem>
<simpara>On the client machine, install the <literal role="package">zypp-plugin-spacewalk</literal> package:</simpara>
<screen># zypper in zypp-plugin-spacewalk</screen>
</listitem>
<listitem>
<simpara>Navigate to the <menuchoice><guimenu>Software</guimenu> <guisubmenu>Packages</guisubmenu> <guimenuitem>Lock</guimenuitem></menuchoice> tab on the managed system to see a list of all available packages.</simpara>
</listitem>
<listitem>
<simpara>Select the packages to lock, and click <guibutton>Request Lock</guibutton>.
You can also choose to enter a date and time for the lock to activate.
Leave the date and time blank if you want the lock to activate as soon as possible.
Note that the lock might not activate immediately.</simpara>
</listitem>
<listitem>
<simpara>To remove a package lock, select the packages to unlock and click <guibutton>Request Unlock</guibutton>.
Leave the date and time blank if you want the lock to deactivate as soon as possible.
Note that the lock might not deactivate immediately.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="preparing.and.registering.clients.salt">
<title>Registering Salt Clients</title>
<simpara>There are three possible methods for registering Salt minions.
You can use a bootstrap repository, a bootstrap script, or install using the Web UI.</simpara>
<simpara>This section describes using a bootstrap repository.
Registering Salt clients with a bootstrap client is the same as registering traditional clients, which is described at <link xl:href="quickstart3_chap_suma_keys_and_first_client.xml#registering.clients.traditional">Registering Traditional Clients</link>. When using this method, ensure you enable the <literal role="guimenu">Bootstrap using Salt</literal> and activation key options in <literal role="guimenu">Configuration File Deployment</literal>, so that highstate is applied automatically.
For information on using the Web UI, see <link xl:href="reference-webui-systems.xml#ref.webui.systems.bootstrapping">Bootstrapping Salt</link>.</simpara>
<simpara>You can also use these methods to change existing traditional clients into Salt minions.</simpara>
<important>
<title>GPG Keys and Uyuni Client Tools</title>
<simpara>The GPG key used by Uyuni Client Tools is not trusted by default.
Either update your bootstrap repository to trust the key explicitly, or use the Web UI to manually trust the key from each client.</simpara>
</important>
<simpara>To register Salt clients with a bootstrap repository, you will need to have already set up a SUSE Manager tools repository, which is described in <link xl:href="quickstart3_chap_suma_keys_and_first_client.xml#create.tools.repository">Create Tools Repository</link>.
You will also need to have fully synchronized a base channel for clients to obtain software packages (for example: <literal>SLES12-SP4-Pool_for_x86_64</literal>).</simpara>
<orderedlist numeration="arabic">
<title>Procedure: Registering Salt Minions</title>
<listitem>
<simpara>On your minion as root enter the following command:</simpara>
<screen>zypper ar http://FQDN.server.example.com/pub/repositories/sle/12/4/bootstrap/ \
   sles12-sp4</screen>
<note>
<simpara>Do not use <literal>HTTPS</literal>.
Use <literal>HTTP</literal> instead to avoid errors.</simpara>
</note>
</listitem>
<listitem>
<simpara>After adding the repository containing the necessary Salt packages execute:</simpara>
<screen>zypper in salt-minion</screen>
</listitem>
<listitem>
<simpara>Modify the minion configuration file to point to the fully qualified domain name (<literal role="replaceable">FQDN</literal>) of the SUSE Manager server (master):</simpara>
<screen>vi /etc/salt/minion</screen>
<simpara>Find and change the line:</simpara>
<screen>master: salt</screen>
<simpara>to:</simpara>
<screen>master: FQDN.server.example.com</screen>
</listitem>
<listitem>
<simpara>Restart the Salt minion with:</simpara>
<screen>systemctl restart salt-minion</screen>
</listitem>
</orderedlist>
<simpara>Your newly registered minion should now show up within the Web UI under <menuchoice><guimenu>Salt</guimenu> <guimenuitem>Keys</guimenuitem></menuchoice>.
Accept the <literal role="guimenu">pending</literal> key to begin management.</simpara>
<simpara>If you have used your hypervisor clone utility, and attempted to register the cloned Salt client, you might get this error:</simpara>
<screen>We're sorry, but the system could not be found.</screen>
<simpara>This is caused by the new, cloned, system having the same machine ID as an existing, registered, system.
You can adjust this manually to correct the error and register the cloned system successfully.</simpara>
<simpara>For more information and instructions, see xref:FILENAME.adoc#bp.chapt.suma3.troubleshooting.registering.cloned.salt.systems[].</simpara>
</section>
</chapter>
<chapter xml:id="chapt-client-cfg-automating-installation">
<title>Automating Installation</title>
<section xml:id="client-cfg-autoinstallation-methods">
<title>Introduction</title>
<note>
<title>Autoinstallation Types: AutoYaST and Kickstart</title>
<simpara>In the following section, AutoYaST and AutoYaST features apply for SUSE Linux Enterprise client systems only.</simpara>
<simpara>For RHEL systems, use Kickstart and Kickstart features.</simpara>
</note>
<important>
<title>Auto-Installing Salt Minions Currently Not Supported</title>
<simpara>This procedure will work for traditionally  managed systems (system type <literal role="systemitem">management</literal>).</simpara>
<simpara>Autoinstallation is not currently available for systems using Salt (system type <literal role="systemitem">salt</literal>).</simpara>
</important>
<simpara>AutoYaST and Kickstart configuration files allow administrators to create an environment for automating otherwise time-consuming system installations, such as multiple servers or workstations. AutoYaST files have to be uploaded to be managed with SUSE Manager.
Kickstart files can be created, modified, and managed within the SUSE Manager Web interface.</simpara>
<simpara>SUSE Manager also features the Cobbler installation server.
For more information on Cobbler, see:</simpara>
<simpara>xref:FILENAME.adoc#advanced.topics.cobbler[].</simpara>
</section>
<section xml:id="client-cfg-autoinstallation-autoyast">
<title>AutoYaST</title>
<simpara>Using AutoYaST, a system administrator can create a single file containing the answers to all the questions that would normally be asked during a typical installation of a SUSE Linux Enterprise system.</simpara>
<simpara>AutoYaST files can be kept on a single server system and read by individual computers during the installation.
This way the same AutoYaST file is used to install SUSE Linux Enterprise on multiple machines.</simpara>
<simpara>The <emphasis><phrase role="ref">SUSE Linux Enterprise Server AutoYaST Guide</phrase></emphasis> at (<link xl:href="https://www.suse.com/documentation/sles-15/">https://www.suse.com/documentation/sles-15/</link>) will contain an in-depth discussion of &#8220;Automated Installation&#8221; using AutoYaST.</simpara>
<section xml:id="s4-system-ay-intro-explain">
<title>AutoYaST Explained</title>
<simpara>When a machine is to receive a network-based AutoYaST installation, the following events must occur in this order:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>After being connected to the network and turned on, the machine&#8217;s PXE logic broadcasts its MAC address and requests to be discovered.</simpara>
</listitem>
<listitem>
<simpara>If no static IP address is used, the DHCP server recognizes the discovery request and offers network information needed for the new machine to boot. This includes an IP address, the default gateway to be used, the netmask of the network, the IP address of the TFTP or HTTP server holding the bootloader program, and the full path and file name to that program (relative to the server&#8217;s root).</simpara>
</listitem>
<listitem>
<simpara>The machine applies the networking information and initiates a session with the server to request the bootloader program.</simpara>
</listitem>
<listitem>
<simpara>The bootloader searches for its configuration file on the server from which it was loaded. This file dictates which Kernel and Kernel options, such as the initial RAM disk (initrd) image, should be executed on the booting machine. Assuming the bootloader program is SYSLINUX, this file is located in the <literal role="path">pxelinux.cfg</literal> directory on the server and named the hexadecimal equivalent of the new machine&#8217;s IP address. For example, a bootloader configuration file for SUSE Linux Enterprise Server should contain:</simpara>
<screen>port 0
prompt 0
timeout 1
default autoyast
label autoyast
  kernel vmlinuz
  append autoyast=http://`my_susemanager_server`/`path`\
    install=http://`my_susemanager_server`/`repo_tree`</screen>
</listitem>
<listitem>
<simpara>The machine accepts and uncompresses the initrd and kernel, boots the kernel, fetches the instsys from the install server and initiates the AutoYaST installation with the options supplied in the bootloader configuration file, including the server containing the AutoYaST configuration file.</simpara>
</listitem>
<listitem>
<simpara>The new machine is installed based on the parameters established within the AutoYaST configuration file.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="s4-system-ay-intro-prereq">
<title>AutoYaST Prerequisites</title>
<simpara>Some preparation is required for your infrastructure to handle AutoYaST installations.
For instance, before creating AutoYaST profiles, you may consider:</simpara>
<itemizedlist>
<listitem>
<simpara>A DHCP server is not required for AutoYaST, but it can make things easier. If you are using static IP addresses, you should select static IP while developing your AutoYaST profile.</simpara>
</listitem>
<listitem>
<simpara>Host the AutoYaST distribution trees via HTTP, properly provided by SUSE Manager.</simpara>
</listitem>
<listitem>
<simpara>If conducting a so-called bare-metal AutoYaST installation, provide the following settings:</simpara>
<itemizedlist>
<listitem>
<simpara>Configure DHCP to assign the required networking parameters and the bootloader program location.</simpara>
</listitem>
<listitem>
<simpara>In the bootloader configuration file, specify the kernel and appropriate kernel options to be used.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="s4-system-ay-iso">
<title>Building Bootable AutoYaST ISOs</title>
<simpara>While you can schedule a registered system to be installed by AutoYaST with a new operating system and package profile, you can also automatically install a system that is not registered with SUSE Manager, or does not yet have an operating system installed.
One common method of doing this is to create a bootable CD-ROM that is inserted into the target system.
When the system is rebooted or switched on, it boots from the CD-ROM, loads the AutoYaST configuration from your SUSE Manager, and proceeds to install SUSE Linux Enterprise Server according to the AutoYaST profile you have created.</simpara>
<simpara>To use the CD-ROM, boot the system and type <literal>autoyast</literal> at the prompt (assuming you left the label for the AutoYaST  boot as <literal>autoyast</literal>). When you press <keycap>Enter</keycap>, the AutoYaST  installation begins.</simpara>
<simpara>For more information about image creation, refer to KIWI at <link xl:href="http://doc.opensuse.org/projects/kiwi/doc/">http://doc.opensuse.org/projects/kiwi/doc/</link>.</simpara>
</section>
<section xml:id="s4-system-ay-pxe">
<title>Integrating AutoYaST with PXE</title>
<simpara>In addition to CD-ROM-based installations, AutoYaST installation through a Pre-Boot Execution Environment (PXE) is supported.
This is less error-prone than CDs, enables AutoYaST installation from bare metal, and integrates with existing PXE/DHCP environments.</simpara>
<simpara>To use this method, make sure your systems have network interface cards (NIC) that support PXE, install and configure a PXE server, ensure DHCP is running, and place the installation repository on an HTTP server for deployment.
Finally upload the AutoYaST profile via the Web interface to the SUSE Manager server.
Once the AutoYaST profile has been created, use the URL from the <guimenu>Autoinstallation Overview</guimenu> page, as for CD-ROM-based installations.</simpara>
<simpara>To obtain specific instructions for conducting PXE AutoYaST installation, refer to the <emphasis>Using PXE Boot</emphasis> section of the <emphasis><phrase role="ref">SUSE Linux Enterprise Deployment Guide</phrase></emphasis>.</simpara>
<simpara>Starting with xref:FILENAME.adoc#ref.webui.systems.autoinst.profiles[], AutoYaST options available from <menuchoice><guimenu>Systems</guimenu> <guimenuitem>Kickstart</guimenuitem></menuchoice> are described.</simpara>
</section>
</section>
<section xml:id="client-cfg-reg-with-bootstrap-kickstart">
<title>Kickstart</title>
<simpara>Using Kickstart, a system administrator can create a single file containing the answers to all the questions that would normally be asked during a typical installation of Red Hat Enterprise Linux.</simpara>
<simpara>Kickstart files can be kept on a single server and read by individual computers during the installation.
This method allows you to use one Kickstart file to install Red Hat Enterprise Linux on multiple machines.</simpara>
<simpara>The <emphasis><phrase role="ref">Red Hat Enterprise Linux System Administration Guide</phrase></emphasis> contains an in-depth description of Kickstart (<link xl:href="https://access.redhat.com/documentation/en/red-hat-enterprise-linux/">https://access.redhat.com/documentation/en/red-hat-enterprise-linux/</link>).</simpara>
<section xml:id="s4-system-ks-intro-explain">
<title>Kickstart Explained</title>
<simpara>When a machine is to receive a network-based {kickstart}, the following events must occur in this order:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>After being connected to the network and turned on, the machine&#8217;s PXE logic broadcasts its MAC address and requests to be discovered.</simpara>
</listitem>
<listitem>
<simpara>If no static IP address is used, the DHCP server recognizes the discovery request and offers network information needed for the new machine to boot. This information includes an IP address, the default gateway to be used, the netmask of the network, the IP address of the TFTP or HTTP server holding the bootloader program, and the full path and file name of that program (relative to the server&#8217;s root).</simpara>
</listitem>
<listitem>
<simpara>The machine applies the networking information and initiates a session with the server to request the bootloader program.</simpara>
</listitem>
<listitem>
<simpara>The bootloader searches for its configuration file on the server from which it was loaded. This file dictates which kernel and kernel options, such as the initial RAM disk (initrd) image, should be executed on the booting machine. Assuming the bootloader program is SYSLINUX, this file is located in the <literal role="path">pxelinux.cfg</literal> directory on the server and named the hexadecimal equivalent of the new machine&#8217;s IP address. For example, a bootloader configuration file for Red Hat Enterprise Linux AS 2.1 should contain:</simpara>
<screen>port 0
prompt 0
timeout 1
default My_Label
label My_Label
      kernel vmlinuz
      append ks=http://`my_susemanager_server`/`path`\
          initrd=initrd.img network apic</screen>
</listitem>
<listitem>
<simpara>The machine accepts and uncompresses the init image and kernel, boots the kernel, and initiates a Kickstart installation with the options supplied in the bootloader configuration file, including the server containing the Kickstart configuration file.</simpara>
</listitem>
<listitem>
<simpara>This {kickstart} configuration file in turn directs the machine to the location of the installation files.</simpara>
</listitem>
<listitem>
<simpara>The new machine is built based on the parameters established within the Kickstart configuration file.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="s4-system-ks-intro-prereq">
<title>Kickstart Prerequisites</title>
<simpara>Some preparation is required for your infrastructure to handle {kickstart}s.
For instance, before creating Kickstart profiles, you may consider:</simpara>
<itemizedlist>
<listitem>
<simpara>A DHCP server is not required for kickstarting, but it can make things easier. If you are using static IP addresses, select static IP while developing your Kickstart profile.</simpara>
</listitem>
<listitem>
<simpara>An FTP server can be used instead of hosting the Kickstart distribution trees via HTTP.</simpara>
</listitem>
<listitem>
<simpara>If conducting a bare metal {kickstart}, you should configure DHCP to assign required networking parameters and the bootloader program location. Also, specify within the bootloader configuration file the kernel to be used and appropriate kernel options.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="s4-system-ks-iso">
<title>Building Bootable Kickstart ISOs</title>
<simpara>While you can schedule a registered system to be kickstarted to a new operating system and package profile, you can also {kickstart} a system that is not registered with SUSE Manager or does not yet have an operating system installed.
One common method of doing this is to create a bootable CD-ROM that is inserted into the target system.
When the system is rebooted, it boots from the CD-ROM, loads the {kickstart} configuration from your SUSE Manager, and proceeds to install Red Hat Enterprise Linux according to the Kickstart profile you have created.</simpara>
<simpara>To do this, copy the contents of <literal role="path">/isolinux</literal> from the first CD-ROM of the target distribution.
Then edit the <literal role="path">isolinux.cfg</literal> file to default to 'ks'. Change the 'ks' section to the following template:</simpara>
<screen>label ks
kernel vmlinuz
  append text ks=`url`initrd=initrd.img lang= devfs=nomount \
    ramdisk_size=16438`ksdevice`</screen>
<simpara>IP address-based {kickstart} URLs will look like this:</simpara>
<screen>http://`my.manager.server`/kickstart/ks/mode/ip_range</screen>
<simpara>The {kickstart} distribution defined via the IP range should match the distribution from which you are building, or errors will occur. <literal role="replaceable">ksdevice</literal> is optional, but looks like:</simpara>
<screen>ksdevice=eth0</screen>
<simpara>It is possible to change the distribution for a Kickstart profile within a family, such as Red Hat Enterprise Linux AS 4 to Red Hat Enterprise Linux ES 4, by specifying the new distribution label.
Note that you cannot move between versions (4 to 5) or between updates (U1 to U2).</simpara>
<simpara>Next, customize <literal role="path">isolinux.cfg</literal> further for your needs by adding multiple Kickstart options, different boot messages, shorter timeout periods, etc.</simpara>
<simpara>Next, create the ISO as described in the <emphasis>Making an Installation Boot CD-ROM</emphasis> section of the <emphasis><phrase role="ref">Red Hat Enterprise Linux Installation Guide</phrase></emphasis>.
Alternatively, issue the command:</simpara>
<screen>mkisofs -o file.iso -b isolinux.bin -c boot.cat -no-emul-boot \
  -boot-load-size 4 -boot-info-table -R -J -v -T isolinux/</screen>
<simpara>Note that <literal role="path">isolinux/</literal> is the relative path to the directory containing the modified isolinux files copied from the distribution CD, while <literal role="path">file.iso</literal> is the output ISO file, which is placed into the current directory.</simpara>
<simpara>Burn the ISO to CD-ROM and insert the disc.
Boot the system and type "ks" at the prompt (assuming you left the label for the Kickstart boot as 'ks'). When you press <keycap>Enter</keycap>, Kickstart starts running.</simpara>
</section>
<section xml:id="s4-system-ks-pxe">
<title>Integrating Kickstart with PXE</title>
<simpara>In addition to CD-ROM-based installs, Kickstart supports a Pre-Boot Execution Environment (PXE). This is less error-prone than CDs, enables kickstarting from bare metal, and integrates with existing PXE/DHCP environments.</simpara>
<simpara>To use this method, make sure your systems have network interface cards (NIC) that support PXE.
Install and configure a PXE server and ensure DHCP is running.
Then place the appropriate files on an HTTP server for deployment.
Once the {kickstart} profile has been created, use the URL from the <guimenu>Kickstart Details</guimenu> page, as for CD-ROM-based installs.</simpara>
<simpara>To obtain specific instructions for conducting PXE {kickstart}s, refer to the <emphasis>PXE Network Installations</emphasis> chapter of the <emphasis><phrase role="ref">Red Hat Enterprise Linux 4 System Administration    Guide</phrase></emphasis>.</simpara>
<note>
<simpara>Running the Network Booting Tool, as described in the Red Hat Enterprise Linux 4: System Administration Guide, select "HTTP" as the protocol and include the domain name of the SUSE Manager in the Server field if you intend to use it to distribute the installation files.</simpara>
</note>
<simpara>The following sections describe the autoinstallation options available from the <menuchoice><guimenu>Systems</guimenu> <guimenuitem>Autoinstallation</guimenuitem></menuchoice> page.</simpara>
</section>
</section>
<section xml:id="client-cfg-cobbler">
<title>Cobbler</title>
<section xml:id="at.introduction.cobbler">
<title>Introduction</title>
<simpara>SUSE Manager features the Cobbler server, which allows administrators to centralize system installation and provisioning infrastructure.
Cobbler is an installation server that provides various methods of performing unattended system installations.
It can be used on server, workstation, or guest systems, in full or para-virtualized environments.</simpara>
<simpara>Cobbler offers several tools for pre-installation guidance, automated installation file management, installation environment management, and more.
This section explains some of the supported features of Cobbler, including:</simpara>
<itemizedlist>
<listitem>
<simpara>Installation environment analysis using the <literal role="command">cobbler check</literal> command</simpara>
</listitem>
<listitem>
<simpara>Multi-site installation server configuration using the <literal role="command">cobbler replicate</literal> command</simpara>
</listitem>
<listitem>
<simpara>Virtual machine guest installation automation with the <literal role="command">koan</literal> client-side tool</simpara>
</listitem>
<listitem>
<simpara>Building installation ISOs with PXE-like menus using the <literal role="command">cobbler buildiso</literal> command (for SUSE Manager systems with x86_64 architecture)</simpara>
</listitem>
</itemizedlist>
<simpara>For more detailed Cobbler documentation, see <link xl:href="http://cobbler.github.io/manuals/">http://cobbler.github.io/manuals/</link>.</simpara>
<important>
<title>Supported Cobbler Functions</title>
<simpara>SUSE only support those Cobbler functions that are either listed within our formal documentation or available via the web interface and API.</simpara>
</important>
</section>
<section xml:id="advanced.topics.cobbler.reqs">
<title>Cobbler Requirements</title>
<simpara>To use Cobbler for system installation with PXE, you will require a TFTP server. SUSE Manager installs a TFTP server by default.
To PXE boot systems, you will require a DHCP server, or have access to a network DHCP server. Edit the <literal role="path">/etc/dhcp.conf</literal> configuration file to change <literal role="option">next-server</literal> to the hostname or IP address of your Cobbler server.</simpara>
<simpara>Cobbler requires an open HTTP port to synchronize data between the Server and the Proxy.
By default, Cobbler uses port 80, but you can configure it to use port 443 instead if that suits your environment.</simpara>
<important>
<title>Correct Hostname Configuration</title>
<simpara>Cobbler uses hostnames as a unique key for each system.
If you are using the <literal role="option">pxe-default-image</literal> to onboard bare metal systems, make sure every system has a unique hostname.
Non-unique hostnames will cause all systems with the same hostname to have the configuration filess overwritten when a provisioning profile is assigned.</simpara>
</important>
<section xml:id="advanced.topics.cobbler.reqs.settings">
<title>Configuring Cobbler with /etc/cobbler/settings</title>
<simpara>Cobbler configuration is primarily managed using the <literal role="path">/etc/cobbler/settings</literal> file.
Cobbler will run with the default settings unchanged.
All configurable settings are explained in detail in the <literal role="path">/etc/cobbler/settings</literal> file, including information on each setting, and recommendations.</simpara>
<note>
<title>Supported Languages</title>
<simpara>If SUSE Manager complains that language <literal>en</literal> was not found within the list of supported languages available at <literal role="path">/usr/share/YaST2/data/languages</literal>, remove the <literal role="option">lang</literal> parameter in the <literal role="path">/etc/cobbler/settings</literal> file, or add a valid parameter such as <literal>en_US</literal>.</simpara>
<simpara>For more on this topic, see <link xl:href="https://www.suse.com/support/kb/doc?id=7018334">https://www.suse.com/support/kb/doc?id=7018334</link>.</simpara>
</note>
</section>
<section xml:id="advanced.topics.cobbler.req.dhcp">
<title>Cobbler and DHCP</title>
<simpara>Cobbler uses DHCP to automate bare metal installations from a PXE boot server.
You must have administrative access to the network&#8217;s DHCP server, or be able to configure DHCP directly on the the Cobbler server.</simpara>
<section xml:id="advanced.topics.cobbler.reqs.dhcp.notmanaged">
<title>Configuring an Existing DHCP Server</title>
<simpara>If you have existing DHCP server, you will need to edit the DHCP configuration file so that it points to the Cobbler server and PXE boot image.</simpara>
<simpara>As root on the DHCP server, edit the <literal role="path">/etc/dhcpd.conf</literal> file and append a new class with options for performing PXE boot installation.
For example:</simpara>
<informalexample>
<screen>allow booting;
allow bootp; <co xml:id="CO1-1"/>
class "PXE" <co xml:id="CO1-2"/>
{match if substring(option vendor-class-identifier, 0, 9) = "PXEClient"; <co xml:id="CO1-3"/>
next-server 192.168.2.1; <co xml:id="CO1-4"/>
filename "pxelinux.0";} <co xml:id="CO1-5"/></screen>
<calloutlist>
<callout arearefs="CO1-1">
<para>Enable network booting with the <literal role="systemitem">bootp</literal> protocol.</para>
</callout>
<callout arearefs="CO1-2">
<para>Create a class called <literal>PXE</literal>.</para>
</callout>
<callout arearefs="CO1-3">
<para>A system configured to have PXE first in its boot priority identifies itself as <literal>PXEClient</literal>.</para>
</callout>
<callout arearefs="CO1-4">
<para>As a result, the DHCP server directs the system to the Cobbler server at <literal>192.168.2.1</literal>.</para>
</callout>
<callout arearefs="CO1-5">
<para>The DHCP server retrieves the <literal role="path">pxelinux.0</literal>bootloader file.</para>
</callout>
</calloutlist>
</informalexample>
</section>
<section xml:id="advanced.topics.cobbler.reqs.dhcp.kvm">
<title>Setting up PXE Boot in KVM</title>
<simpara>It is possible to set up PXE booting in KVM, however we do not recommend you use this method for production systems.
This method can replace the <literal role="guilabel">next-server</literal> setting on a DHCP server, as described in xref:FILENAME.adoc#advanced.topics.cobbler.reqs.dhcp.notmanaged[].
Edit the network XML description with <literal role="command">virsh</literal>:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Produce an XML dump of the current description:</simpara>
<screen>virsh net-dumpxml --inactive network &gt; network.xml</screen>
</listitem>
<listitem>
<simpara>Open the XML dump file at <literal role="path">network.xml</literal> with a text editor and add a <literal role="systemitem">bootp</literal> parameter within the <literal role="systemitem">&lt;dhcp&gt;</literal>` element:</simpara>
<screen>&lt;bootp file='/pxelinux.0' server='192.168.100.153'/&gt;</screen>
</listitem>
<listitem>
<simpara>Install the updated description:</simpara>
<screen>virsh net-define network.xml</screen>
</listitem>
</orderedlist>
<simpara>Alternatively, use the <literal role="command">net-edit</literal> subcommand, which will also perform some error checking.</simpara>
<example xml:id="at.cobbler.bootp.kvm">
<title>Minimal Network XML Description for KVM</title>
<screen>&lt;network&gt;
  &lt;name&gt;default&lt;/name&gt;
  &lt;uuid&gt;1da84185-31b5-4c8b-9ee2-a7f5ba39a7ee&lt;/uuid&gt;
  &lt;forward mode='nat'&gt;
    &lt;nat&gt;
      &lt;port start='1024' end='65535'/&gt;
    &lt;/nat&gt;
  &lt;/forward&gt;
  &lt;bridge name='virbr0' stp='on' delay='0'/&gt;
  &lt;mac address='52:54:00:29:59:18'/&gt;
  &lt;domain name='default'/&gt;
  &lt;ip address='192.168.100.1' netmask='255.255.255.0'&gt;
    &lt;dhcp&gt;
      &lt;range start='192.168.100.128' end='192.168.100.254'/&gt;
      &lt;bootp file='/pxelinux.0' server='192.168.100.153'/&gt; <co xml:id="CO2-1"/>
&lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;</screen>
<calloutlist>
<callout arearefs="CO2-1">
<para><literal>bootp</literal> statement that directs to the PXE server.</para>
</callout>
</calloutlist>
</example>
</section>
</section>
<section xml:id="advanced.topics.cobbler.reqs.tftp">
<title>TFTP</title>
<simpara>SUSE Manager uses the <literal role="daemon">atftpd</literal> daemon, but it can also use TFTP.
The <literal role="daemon">atftpd</literal> daemon is the recommended method for PXE serviices, and is installed by default.
Usually, you do not have to change its configuration, but if you have to, use the YaST Services Manager.</simpara>
<simpara>Before TFTP can serve the <literal role="path">pxelinux.0</literal> boot image, you must start the tftp service.
Start YaST and use <menuchoice><guimenu>System</guimenu> <guimenuitem>Services Manager</guimenuitem></menuchoice> to configure the <literal role="daemon">tftpd</literal> daemon.</simpara>
</section>
<section xml:id="advanced.topics.cobbler.reqs.sync.tftp">
<title>Syncing TFTP Contents to SUSE Manager Proxies</title>
<simpara>It is possible to synchronize Cobbler-generated TFTP contents to SUSE Manager proxies to perform PXE booting using proxies.</simpara>
<section xml:id="_installation">
<title>Installation</title>
<simpara>On the SUSE Manager Server as the root user, install the <literal role="systemitem">susemanager-tftpsync</literal> package:</simpara>
<screen>zypper install susemanager-tftpsync</screen>
<simpara>On the SUSE Manager Proxy systems as the root user , install the <literal role="systemitem">susemanager-tftpsync-recv</literal> package:</simpara>
<screen>zypper install susemanager-tftpsync-recv</screen>
</section>
<section xml:id="_configuring_suse_manager_proxy">
<title>Configuring SUSE Manager Proxy</title>
<simpara>Execute <literal role="path">configure-tftpsync.sh</literal> on the SUSE Manager Proxy systems.</simpara>
<simpara>This setup script asks for hostnames and IP addresses of the SUSE Manager server and the proxy.
Additionally, it asks for the <literal>tftpboot</literal> directory on the proxy.
For more information, see the output of <literal role="command">configure-tftpsync.sh --help</literal>.</simpara>
</section>
<section xml:id="_configuring_suse_manager_server">
<title>Configuring SUSE Manager Server</title>
<simpara>As the root user, execute <literal role="path">configure-tftpsync.sh</literal> on SUSE Manager Server:</simpara>
<screen>configure-tftpsync.sh proxy1.example.com proxy2.example.com</screen>
<simpara>Execute <literal role="command">cobbler sync</literal> to initially push the files to the proxy systems.
This will succeed if all listed proxies are properly configured.</simpara>
<note>
<title>Changing the List of Proxy Systems</title>
<simpara>You can call <literal role="command">configure-tftpsync.sh</literal> to change the list of proxy systems.
You must always provide the full list of proxy systems.</simpara>
</note>
<note>
<title>Reinstalling a Configured Proxy</title>
<simpara>If you reinstall an already configured proxy and want to push all the files again you must remove the cache file at <literal role="path">/var/lib/cobbler/pxe_cache.json</literal> before you can call <literal role="command">cobbler sync</literal> again.</simpara>
</note>
</section>
<section xml:id="_requirements_2">
<title>Requirements</title>
<simpara>The SUSE Manager Server must be able to access the SUSE Manager Proxy systems directly.
You cannot push using a proxy.</simpara>
</section>
</section>
<section xml:id="advanced.topics.cobbler.reqs.service">
<title>Syncing and Starting the Cobbler Service</title>
<simpara>Before starting the Cobbler service, run a check to make sure that all the prerequisites are configured according to your requirements using the <literal role="command">cobbler check</literal> command.</simpara>
<simpara>If configuration is correct, start the SUSE Manager server with this command:</simpara>
<screen>/usr/sbin/spacewalk-service start</screen>
<warning>
<simpara>Do not start or stop the <literal role="command">cobblerd</literal> service independent of the SUSE Manager service.
Doing so may cause errors and other issues.</simpara>
<simpara>Always use <literal role="command">/usr/sbin/spacewalk-service</literal> to start or stop SUSE Manager.</simpara>
</warning>
</section>
</section>
<section xml:id="advanced.topics.cobbler.adddistro">
<title>Adding a Distribution to Cobbler</title>
<simpara>If all Cobbler prerequisites have been met and Cobbler is running, you can use the Cobbler server as an installation source for a distribution:</simpara>
<simpara>Make installation files such as the kernel image and the initrd image available on the Cobbler server.
Then add a distribution to Cobbler, using either the Web interface or the command line tools.</simpara>
<simpara>For information about creating and configuring AutoYaST or Kickstart distributions from the SUSE Manager interface, refer to xref:FILENAME.adoc#ref.webui.systems.autoinst.distribution[].</simpara>
<simpara>To create a distribution from the command line, use the <literal role="command">cobbler</literal> command as root:</simpara>
<screen>cobbler distro add --name=`string`--kernel=`path`--initrd=`path`</screen>
<variablelist>
<varlistentry>
<term><literal role="option">--name=</literal><literal role="replaceable">string</literal> option</term>
<listitem>
<simpara>A label used to differentiate one distribution choice from another (for example, <literal>sles12server</literal>).</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">--kernel=</literal><literal role="replaceable">path</literal> option</term>
<listitem>
<simpara>Specifies the path to the kernel image file.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">--initrd=</literal><literal role="replaceable">path</literal> option</term>
<listitem>
<simpara>specifies the path to the initial ram disk (initrd) image file.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="advanced.topics.cobbler.addprofile">
<title>Adding a Profile to Cobbler</title>
<simpara>Once you have added a distribution to Cobbler, you can add profiles.</simpara>
<simpara>Cobbler profiles associate a distribution with additional options like AutoYaST or Kickstart files.
Profiles are the core unit of provisioning and there must be at least one Cobbler profile for every distribution added.
For example, two profiles might be created for a Web server and a desktop configuration.
While both profiles use the same distribution, the profiles are for different installation types.</simpara>
<simpara>For information about creating and configuring Kickstart and AutoYaST profiles in the SUSE Manager interface, refer to xref:FILENAME.adoc#ref.webui.systems.autoinst.profiles[].</simpara>
<simpara>Use the <literal role="command">cobbler</literal> command as root to create profiles from the command line:</simpara>
<screen>cobbler profile add --name=string --distro=string [--kickstart=url] \
  [--virt-file-size=gigabytes] [--virt-ram=megabytes]</screen>
<variablelist>
<varlistentry>
<term><literal role="option">--name=</literal><literal role="replaceable">string</literal></term>
<listitem>
<simpara>A unique label for the profile, such as <literal>sles12webserver</literal> or <literal>sles12workstation</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">--distro=</literal><literal role="replaceable">string</literal></term>
<listitem>
<simpara>The distribution that will be used for this profile.
For adding distributions, see xref:FILENAME.adoc#advanced.topics.cobbler.adddistro[].</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">--kickstart=</literal><literal role="replaceable">url</literal></term>
<listitem>
<simpara>The location of the Kickstart file (if available).</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">--virt-file-size=</literal><literal role="replaceable">gigabytes</literal></term>
<listitem>
<simpara>The size of the virtual guest file image (in gigabytes).
The default is 5&#160;GB.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">--virt-ram=</literal><literal role="replaceable">megabytes</literal></term>
<listitem>
<simpara>The maximum amount of physical RAM a virtual guest can consume (in megabytes).
The default is 512&#160;MB.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="advanced.topics.cobbler.addsystem">
<title>Adding a System to Cobbler</title>
<simpara>Once the distributions and profiles for Cobbler have been created, add systems to Cobbler.
System records map a piece of hardware on a client with the Cobbler profile assigned to run on it.</simpara>
<note>
<simpara>If you are provisioning using <literal role="command">koan</literal> and PXE menus alone, it is not required to create system records.
They are useful when system-specific Kickstart templating is required or to establish that a specific system should always get specific content installed.
If a client is intended for a certain role, system records should be created for it.</simpara>
</note>
<simpara>For information about creating and configuring automated installation from the SUSE Manager interface, refer to xref:FILENAME.adoc#s4-sm-system-details-kick[].</simpara>
<simpara>Run this command as the root user to add a system to the Cobbler configuration:</simpara>
<screen>cobbler system add --name=string --profile=string \
  --mac-address=AA:BB:CC:DD:EE:FF</screen>
<variablelist>
<varlistentry>
<term><literal role="option">--name=</literal><literal role="replaceable">string</literal></term>
<listitem>
<simpara>A unique label for the system, such as <literal>engineering_server</literal> or <literal>frontoffice_workstation</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">--profile=</literal><literal role="replaceable">string</literal></term>
<listitem>
<simpara>Specifies the name of one of the profiles added in xref:FILENAME.adoc#advanced.topics.cobbler.addprofile[].</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal role="option">--mac-address=</literal><literal role="replaceable">AA:BB:CC:DD:EE:FF</literal></term>
<listitem>
<simpara>Allows systems with the specified MAC address to automatically be provisioned to the profile associated with the system record when they are being installed.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For more options, such as setting hostname or IP addresses, refer to the Cobbler manpage (<literal role="command">man cobbler</literal>).</simpara>
</section>
<section xml:id="advanced.topics.cobbler.templates">
<title>Using Cobbler Templates</title>
<simpara>The SUSE Manager web interface allows you to create variables for use with Kickstart distributions and profiles.
For more information on creating Kickstart profile variables, refer to xref:FILENAME.adoc#s4-sm-system-kick-details-variables[].</simpara>
<simpara>Kickstart variables are part of an infrastructure change in SUSE Manager to support templating in Kickstart files.
Kickstart templates are files that describe how to build Kickstart files, rather than creating specific Kickstarts.
The templates are shared by various profiles and systems that have their own variables and corresponding values.
These variables modify the templates and a template engine parses the template and variable data into a usable Kickstart file.
Cobbler uses an advanced template engine called Cheetah that provides support for templates, variables, and snippets.</simpara>
<simpara>Advantages of using templates include:</simpara>
<itemizedlist>
<listitem>
<simpara>Robust features that allow administrators to create and manage large amounts of profiles or systems without duplication of effort or manually creating Kickstarts for every unique situation.</simpara>
</listitem>
<listitem>
<simpara>While templates can become complex and involve loops, conditionals and other enhanced features and syntax, you can also create simpler Kickstart files without such complexity.</simpara>
</listitem>
</itemizedlist>
<section xml:id="advanced.topics.cobbler.templates.usage">
<title>Using Templates</title>
<simpara>Kickstart templates can have static values for certain common items such as PXE image file names, subnet addresses, and common paths such as <literal role="path">/etc/sysconfig/network-scripts/</literal>.
However, templates differ from standard Kickstart files in their use of variables.</simpara>
<simpara>For example, a standard Kickstart file may have a networking section similar to this:</simpara>
<screen>network --device=eth0 --bootproto=static --ip=192.168.100.24 \
  --netmask=255.255.255.0 --gateway=192.168.100.1 --nameserver=192.168.100.2</screen>
<simpara>In a Kickstart template file, the networking section would look like this instead:</simpara>
<screen>network --device=$net_dev --bootproto=static --ip=$ip_addr \
  --netmask=255.255.255.0 --gateway=$my_gateway --nameserver=$my_nameserver</screen>
<simpara>These variables are substituted with the values set in your Kickstart profile variables or in your system detail variables.
If the same variable is defined in both the profile and the system detail, then the system detail variable takes precedence.</simpara>
<note>
<simpara>The template for the autoinstallation has syntax rules which relies on punctuation symbols.
To avoid clashes, they need to be properly treated.</simpara>
</note>
<simpara>In case the autoinstallation scenario contains any shell script using variables like <literal>$(example)</literal>, its content should be escaped by using the backslash symbol: <literal>\$(example)</literal>.</simpara>
<simpara>If the variable named <literal>example</literal> is defined in the autoinstallation snippet, the templating engine will evaluate <literal>$example</literal> with its content.
If there is no such variable, the content will be left unchanged.
Escaping the <keycap>$</keycap> symbol will prevent the templating engine from evaluating the symbol as an internal variable.
Long scripts or strings can be escaped by wrapping them with the <literal>\#raw</literal> and <literal>\#end raw</literal> directives.
For example:</simpara>
<screen>#raw
#!/bin/bash
for i in {0..2}; do
 echo "$i - Hello World!"
done
#end raw</screen>
<simpara>Also, pay attention to scenarios like this:</simpara>
<screen>#start some section (this is a comment)
echo "Hello, world"
#end some section (this is a comment)</screen>
<simpara>Any line with a <keycap>#</keycap> symbol followed by a whitespace is treated as a comment and is therefore not evaluated.</simpara>
<simpara>For more information about Kickstart templates, refer to the Cobbler project page at:</simpara>
<simpara><link xl:href="https://fedorahosted.org/cobbler/wiki/KickstartTemplating">https://fedorahosted.org/cobbler/wiki/KickstartTemplating</link></simpara>
</section>
<section xml:id="advanced.topics.cobbler.templates.snippets">
<title>Kickstart Snippets</title>
<simpara>If you have common configurations across all Kickstart templates and profiles, you can use the Snippets feature of Cobbler to take advantage of code reuse.</simpara>
<simpara>Kickstart snippets are sections of Kickstart code that can be called by a <literal role="option">$SNIPPET()</literal> function that will be parsed by Cobbler and substituted with the contents of the snippet.</simpara>
<simpara>For example, you might have a common hard drive partition configuration for all servers, such as:</simpara>
<screen>clearpart --all
part /boot --fstype ext3 --size=150 --asprimary
part / --fstype ext3 --size=40000 --asprimary
part swap --recommended

part pv.00 --size=1 --grow

volgroup vg00 pv.00
logvol /var --name=var vgname=vg00 --fstype ext3 --size=5000</screen>
<simpara>Save this snippet of the configuration to a file like <literal role="path">my_partition</literal> and place the file in <literal role="path">/var/lib/cobbler/snippets/</literal>, where Cobbler can access it.</simpara>
<simpara>Use the snippet by calling the <literal role="option">$SNIPPET()</literal> function in your Kickstart templates.
For example:</simpara>
<screen>$SNIPPET('my_partition')</screen>
<simpara>Wherever you invoke that function, the Cheetah parser will substitute the function with the snippet of code contained in the <literal role="path">my_partition</literal> file.</simpara>
</section>
</section>
<section xml:id="advanced.topics.cobbler.koan">
<title>Using Koan</title>
<simpara>Whether you are provisioning guests on a virtual machine or reinstalling a new distribution on a running system, Koan works in conjunction with Cobbler to provision systems.</simpara>
<section xml:id="advanced.topics.cobbler.koan.virt">
<title>Using Koan to Provision Virtual Systems</title>
<simpara>If you have created a virtual machine profile as documented in xref:FILENAME.adoc#advanced.topics.cobbler.addprofile[], you can use <literal role="command">koan</literal> to initiate the installation of a virtual guest on a system.
For example, create a Cobbler profile with the following command:</simpara>
<screen>cobbler add profile --name=virtualfileserver \
  --distro=sles12-x86_64-server --virt-file-size=20 --virt-ram=1000</screen>
<simpara>This profile is for a fileserver running SUSE Linux Enterprise Server&#160;12 with a 20&#160;GB guest image size and allocated 1&#160;GB of system RAM.
To find the name of the virtual guest system profile, use the <literal role="command">koan</literal> command:</simpara>
<screen>koan --server=hostname --list-profiles</screen>
<simpara>This command lists all the available profiles created with <literal role="command">cobbler profile add</literal>.</simpara>
<simpara>Create the image file, and begin installation of the virtual guest system:</simpara>
<screen>koan --virt --server=cobbler-server.example.com \
  --profile=virtualfileserver --virtname=marketingfileserver</screen>
<simpara>This command specifies that a virtual guest system be created from the Cobbler server (hostname <literal role="server">cobbler-server.example.com</literal>) using the <literal>virtualfileserver</literal> profile.
The <literal role="option">virtname</literal> option specifies a label for the virtual guest, which by default is the system&#8217;s MAC address.</simpara>
<simpara>Once the installation of the virtual guest is complete, it can be used as any other virtual guest system.</simpara>
</section>
<section xml:id="advanced.topics.cobbler.koan.reinstall">
<title>Using Koan to Reinstall Running Systems</title>
<simpara><literal role="command">koan</literal> can replace a still running system with a new installation from the available Cobbler profiles by executing the following command <emphasis>on the system to be reinstalled</emphasis>:</simpara>
<screen>koan --replace-self --server=hostname --profile=name</screen>
<simpara>This command, running on the system to be replaced, will start the provisioning process and replace the system with the profile in <literal role="option">--profile=name</literal> on the Cobbler server specified in <literal role="option">--server=hostname</literal>.</simpara>
</section>
</section>
<section xml:id="advanced.topics.cobbler.buildiso">
<title>Building ISOs with Cobbler</title>
<simpara>Some environments might lack PXE support.
The Cobbler <literal role="command">buildiso</literal> command creates a ISO boot image containing a set of distributions and kernels, and a menu similar to PXE network installations.
Define the name and output location of the boot ISO using the <literal role="option">--iso</literal> option.</simpara>
<note>
<title>ISO Build Directory</title>
<simpara>Depending on Cobbler-related systemd settings (see <literal role="path">/usr/lib/systemd/system/cobblerd.service</literal>) writing ISO images to public <literal role="path">tmp</literal> directories will not work.</simpara>
</note>
<screen>cobbler buildiso --iso=/path/to/boot.iso</screen>
<simpara>The boot ISO includes all profiles and systems by default.
Limit these profiles and systems using the <literal role="option">--profiles</literal> and <literal role="option">--systems</literal> options.</simpara>
<screen>cobbler buildiso --systems="system1,system2,system3" \
  --profiles="profile1,profile2,profile3"</screen>
<note>
<simpara>Building ISOs with the <literal role="command">cobbler buildiso</literal> command is supported for all architectures except the z Systems architecture.</simpara>
</note>
</section>
<section xml:id="advanced.topics.cobbler.baremetal">
<title>Bare Metal Provisioning</title>
<simpara>Systems that have not yet been provisioned are called bare metal systems.
You can provision bare metal systems using Cobbler.
Once a bare metal system has been provisioned in this way, it will appear in the <literal role="guilabel">Systems</literal> list, where you can perform regular provisioning with autoinstallation, for a completely unattended installation.</simpara>
<section xml:id="advanced.topics.cobbler.baremetal.requirements">
<title>Bare Metal Provisioning System Requirements</title>
<simpara>To successfully provision a bare metal system, you will require a fully patched SUSE Manager server, version 2.1 or higher.</simpara>
<simpara>The system to be provisioned must have x86_64 architecture, with at least 2&#160;GB RAM, and be capable of PXE booting.</simpara>
<simpara>The server uses TFTP to provision the bare metal client, so the appropriate port and networks must be configured correctly in order for provisioning to be successful. In particular, ensure that you have a DHCP server, and have set the <literal role="option">next-server</literal> parameter to the SUSE Manager server IP address or hostname.</simpara>
</section>
<section xml:id="advanced.topics.cobbler.baremetal.enabling">
<title>Enabling Bare Metal Systems Management</title>
<simpara>Bare metal systems management can be enabled or disabled in the Web UI by clicking <menuchoice><guimenu>Admin</guimenu> <guisubmenu>SUSE Manager Configuration</guisubmenu> <guimenuitem>Bare-metal systems</guimenuitem></menuchoice>.</simpara>
<note>
<simpara>New systems are added to the organization of the administrator who enabled the bare metal systems management feature. To change the organization, log in as an Administrator of the required organization, and re-enable the feature.</simpara>
</note>
<simpara>Once the feature has been enabled, any bare metal system connected to the server network will be automatically added to the organization when it is powered on.
The process can take a few minutes, and the system will automatically shut down once it is complete.
After the reboot, the system will appear in the <literal role="guilabel">Systems</literal> list.
Click on the name of the system to see basic information, or go to the <literal role="guilabel">Properties</literal>, <literal role="guilabel">Notes</literal>, and <literal role="guilabel">Hardware</literal> tabs for more details.
You can migrate bare metal systems to other organizations if required, using the <literal role="guilabel">Migrate</literal> tab.</simpara>
</section>
<section xml:id="advanced.topics.cobbler.baremetal.provisioning">
<title>Provisioning Bare Metal Systems</title>
<simpara>Provisioning bare metal systems is similar to provisioning other systems, and can be done using the <literal role="guilabel">Provisioning</literal> tab.
However, you will not be able to schedule provisioning, it will happen automatically as soon as the system is configured and powered on.</simpara>
<note>
<title>Bare Metal and System Set Manager</title>
<simpara>System Set Manager can be used with bare metal systems, although not all features will be available, because bare metal systems do not have an operating system installed.
This limitation also applies to mixed sets that contain bare metal systems; all features will be re-enabled if the bare metal systems are removed from the set.</simpara>
</note>
</section>
<section xml:id="advanced.topics.cobbler.baremetal.troubleshooting">
<title>Troubleshooting Bare Metal Systems</title>
<simpara>If a bare metal system on the network is not automatically added to the <literal role="guilabel">Systems</literal> list, check these things first:</simpara>
<itemizedlist>
<listitem>
<simpara>You must have the <literal role="path">pxe-default-image</literal> package installed.</simpara>
</listitem>
<listitem>
<simpara>File paths and parameters must be configured correctly. Check that the <literal role="path">vmlinuz0</literal> and <literal role="path">initrd0.img</literal> files, which are provided by <literal role="path">pxe-default-image</literal>, are in the locations specified in the <literal role="path">rhn.conf</literal> configuration file.</simpara>
</listitem>
<listitem>
<simpara>Ensure the networking equipment connecting the bare metal system to the SUSE Manager server is working correctly, and that you can reach the SUSE Manager server IP address from the server.</simpara>
</listitem>
<listitem>
<simpara>The bare metal system to be provisioned must have PXE booting enabled in the boot sequence, and must not be attempting to boot an operating system.</simpara>
</listitem>
<listitem>
<simpara>The DHCP server must be responding to DHCP requests during boot. Check the PXE boot messages to ensure that:</simpara>
<itemizedlist>
<listitem>
<simpara>the DHCP server is assigning the expected IP address</simpara>
</listitem>
<listitem>
<simpara>the DHCP server is assigning the the SUSE Manager server IP address as <literal role="option">next-server</literal> for booting.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Ensure Cobbler is running, and that the Discovery feature is enabled.</simpara>
</listitem>
</itemizedlist>
<simpara>If you see a blue Cobbler menu shortly after booting, discovery has started.
If it does not complete successfully, temporarily disable automatic shutdown in order to help diagnose the problem. To disable automatic shutdown:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Select <literal role="option">pxe-default-profile</literal> in the Cobbler menu with the arrow keys, and press the Tab key before the timer expires.</simpara>
</listitem>
<listitem>
<simpara>Add the kernel boot parameter <literal role="option">spacewalk-finally=running</literal> using the integrated editor, and press Enter to continue booting.</simpara>
</listitem>
<listitem>
<simpara>Enter a shell with the username <literal role="option">root</literal> and password <literal role="option">linux</literal> to continue debugging.</simpara>
</listitem>
</orderedlist>
<important>
<title>Duplicate profiles</title>
<simpara>Due to a technical limitation, it is not possible to reliably distinguish a new bare metal system from a system that has previously been discovered.
Therefore, we recommended that you do not power on bare metal systems multiple times, as this will result in duplicate profiles.</simpara>
</important>
</section>
</section>
</section>
</chapter>
<chapter xml:id="chapt-client-cfg-disconnected-setup">
<title>Disconnected Setup</title>
<section xml:id="disconnect-setup">
<title>Disconnected Setup with RMT or SMT (DMZ)</title>
<simpara>If it is not possible to connect SUSE Manager
directly or via a proxy to the Internet, a disconnected setup in combination with RMT or SMT is the recommended solution.
In this scenario, RMT or SMT stays in an &#8220;external&#8221;
 network with a connection to SUSE Customer Center
 and synchronizes the software channels and repositories on a removable storage medium.
Then you separate the storage medium from RMT or SMT, and mount it locally on your SUSE Manager
 server to read the updated data.</simpara>
<note>
<title>Offline Usage Scenario</title>
<simpara>SMT and RMT are not made for server cascades.
SUSE Manager always connects to SMT or RMT in an offline or disconnected scenario.</simpara>
</note>
<formalpara>
<title>RMT</title>
<para>The successor of SMT and currently runs on the following systems:</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>SUSE Linux Enterprise 15 (when available)</simpara>
</listitem>
<listitem>
<simpara>Temporarily (for testing only): 12 SP2, and 12 SP3</simpara>
</listitem>
<listitem>
<simpara>Not officially supported: openSUSE Leap 42.2, Leap 42.3, and openSUSE Tumbleweed</simpara>
</listitem>
</itemizedlist>
<simpara>RMT allows you to provision updates for all of your devices running a product based on SUSE Linux Enterprise
 12 SPx and later as well as openSUSE Leap.</simpara>
<formalpara>
<title>SMT</title>
<para>The predecessor of RMT and is no longer actively developed.
It runs on SUSE Linux Enterprise Server
 12 SPx and allows you to provision updates for products based on SUSE Linux Enterprise
 12 SPx and earlier.
You will still need it, if you want to update SUSE Linux Enterprise
 11 clients.</para>
</formalpara>
<section xml:id="rmtool">
<title>Repository Management Tool (RMT) and Disconnected Setup (DMZ)</title>
<simpara>The following procedure will guide you through using RMT.
It will work best with a dedicated RMT instance per SUSE Manager
.</simpara>
<orderedlist numeration="arabic">
<title>Procedure: RMT: Fetching Repository Data from SUSE Customer Center</title>
<listitem>
<simpara>Configure RMT in the external network with SCC. For details about configuring RMT, see the official guide (when available).</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Preparation work:</simpara>
<simpara>Run <literal role="command">rmt-cli sync</literal> to download available products and repositories data for your organization from SCC.</simpara>
<simpara>Run <literal role="command">rmt-cli products list --all</literal> to see the list of products that are available for your organization.</simpara>
<simpara>Run <literal role="command">rmt-cli repos list --all</literal> to see the list of all repositories available.</simpara>
</listitem>
<listitem>
<simpara>With <literal role="command">rmt-cli repos enable</literal> enable repositories you want to mirror.</simpara>
</listitem>
<listitem>
<simpara>With <literal role="command">rmt-cli products enable</literal>enable products. For example, to enable SLES _15:</simpara>
<screen>rmt-cli product enable sles/15/x86_64</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Using RMT, mirror all required repositories.</simpara>
</listitem>
<listitem>
<simpara>Get the required JSON responses from SCC and save them as files at the specified path (for example, <literal role="path">/mnt/usb</literal> ).</simpara>
<important>
<title>Write Permissions for RMT User</title>
<simpara>The directory being written to must be writeable for the same user as the rmt service.
The rmt user setting is defined in the <literal>cli</literal> section of <literal role="path">/etc/rmt.conf</literal>
.</simpara>
</important>
<simpara>Enter:</simpara>
<screen>{prompt.root}rmt-cli export data /mnt/usb</screen>
</listitem>
<listitem>
<simpara>Export settings about repositories to mirror to the specified path (in this case, <literal role="path">/mnt/usb</literal> ); this command will create a <literal role="path">repos.json</literal> file there:</simpara>
<screen>{prompt.root}rmt-cli export settings /mnt/usb</screen>
</listitem>
<listitem>
<simpara>Mirror the repositories according to the settings in the <literal role="path">repos.json</literal> file to the specified path (in this case, <literal role="path">/mnt/usb</literal> ).</simpara>
<screen>{prompt.root}rmt-cli export repos /mnt/usb</screen>
</listitem>
<listitem>
<simpara>Unmount the storage medium and carry it securely to your SUSE Manager server.</simpara>
</listitem>
</orderedlist>
<simpara>On the SUSE Manager
server, continue with <xref linkend="disconnect.mgr.update-repos"/>.</simpara>
</section>
<section xml:id="sub.mgr.tool">
<title>Repository Management Tool (SMT) and Disconnected Setup (DMZ)</title>
<simpara>The following procedure will guide you through using SMT.</simpara>
<orderedlist numeration="arabic">
<title>Procedure: SMT: Fetching Repository Data from SUSE Customer Center</title>
<listitem>
<simpara>Configure SMT in the external network with SCC. For details about configuring SMT with SUSE Linux Enterprise 12, see <link xl:href="https://www.suse.com/documentation/sles-12/book_smt/data/book_smt.html">https://www.suse.com/documentation/sles-12/book_smt/data/book_smt.html</link>.</simpara>
</listitem>
<listitem>
<simpara>Using SMT, mirror all required repositories.</simpara>
</listitem>
<listitem>
<simpara>Create a &#8220;database replacement file&#8221; (for example, <literal role="path">/tmp/dbrepl.xml</literal> ).</simpara>
<screen>{prompt.root}smt-sync --createdbreplacementfile /tmp/dbrepl.xml</screen>
</listitem>
</orderedlist>
<orderedlist xml:id="pro.mgr.tool.mount.storage" numeration="arabic">
<listitem>
<simpara>Mount a removable storage medium such as an external hard disk or USB flash drive.</simpara>
</listitem>
<listitem>
<simpara>Export the data to the mounted medium:</simpara>
<screen>smt-sync --todir /media/disk/
smt-mirror --dbreplfile /tmp/dbrepl.xml --directory /media/disk \
           --fromlocalsmt -L /var/log/smt/smt-mirror-export.log</screen>
<important>
<title>Write Permissions for SMT User</title>
<simpara>The directory being written to must be writeable for the same user as the smt daemon (user=smt). The smt user setting is defined in <literal role="path">/etc/smt.conf</literal>
.
You can check if the correct user is specified via the following command:</simpara>
</important>
</listitem>
</orderedlist>
<screen>{prompt.root}egrep '^smtUser' /etc/smt.conf</screen>
<simpara>+</simpara>
<simpara>+
.Keeping the Disconnected Server Up-to-date
NOTE: <literal role="command">smt-sync</literal> also exports your subscription data.
To keep SUSE Manager
 up-to-date with your subscriptions, you must frequently import and export this data.</simpara>
<simpara>+</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Unmount the storage medium and carry it securely to your SUSE Manager server.</simpara>
</listitem>
</orderedlist>
<simpara>On the SUSE Manager
server, continue with <xref linkend="disconnect.mgr.update-repos"/>.</simpara>
</section>
<section xml:id="disconnect.mgr.update-repos">
<title>Updating Repositories on SUSE Manager From Storage Media</title>
<simpara>This procedure will show you how to update the repositories on the SUSE Manager
server from the storage media.</simpara>
<orderedlist numeration="arabic">
<title>Procedure: Updating the SUSE ManagerServer from the Storage Medium</title>
<listitem>
<simpara>Mount the storage medium on your SUSE Manager server (for example, at <literal role="path">/media/disk</literal> ).</simpara>
</listitem>
<listitem>
<simpara>Specify the local path on the SUSE Manager server in <literal>/etc/rhn/rhn.conf</literal>:</simpara>
<screen>server.susemanager.fromdir = /media/disk</screen>
<simpara>This setting is mandatory for SUSE Customer Center
and <literal role="command">mgr-sync</literal>.</simpara>
</listitem>
<listitem>
<simpara>Restart Tomcat:</simpara>
<screen>systemctl restart tomcat</screen>
</listitem>
</orderedlist>
<orderedlist xml:id="pro.mgr.tool.sync" numeration="arabic">
<listitem>
<simpara>Before performing another operation on the server execute a full sync:</simpara>
<screen>mgr-sync refresh   # SCC (fromdir in rhn.conf required!)</screen>
</listitem>
<listitem>
<simpara><literal role="command">mgr-sync</literal> can now be executed normally:</simpara>
<screen>mgr-sync list channels
mgr-sync add channel channel-label</screen>
<warning>
<title>Data Corruption</title>
<simpara>The disk must always be available at the same mount point.
To avoid data corruption, do not trigger a sync, if the storage medium is not mounted.
If you have already added a channel from a local repository path, you will not be able to change its URL to point to a different path afterwards.</simpara>
</warning>
</listitem>
</orderedlist>
<simpara>Up-to-date data is now available on your SUSE Manager
server and is ready for updating client systems.
According to your maintenance windows or update schedule refresh the data on the storage medium with RMT or SMT.</simpara>
</section>
<section xml:id="_refreshing_data_on_the_storage_medium">
<title>Refreshing Data on the Storage Medium</title>
<orderedlist numeration="arabic">
<title>Procedure: Refreshing Data on the Storage Medium from RMT or SMT</title>
<listitem>
<simpara>On your SUSE Manager server, unmount the storage medium and carry it to your RMT or SMT.</simpara>
</listitem>
<listitem>
<simpara>On your RMT or SMT system, continue with the synchronization step.</simpara>
<warning>
<title>Data Corruption</title>
<simpara>The storage medium must always be available at the same mount point.
To avoid data corruption, do not trigger a sync if the storage medium is not mounted.</simpara>
</warning>
</listitem>
</orderedlist>
<simpara>This concludes using RMT or SMT with SUSE Manager
.</simpara>
</section>
</section>
</chapter>
<chapter xml:id="chapt-client-cfg-subscription-mgmt">
<title>Subscription Management</title>
<section xml:id="client-cfg-subscription-management">
<title>Introduction</title>
<simpara>Coming Soon&#8230;&#8203;</simpara>
</section>
</chapter>
</part>
</book>